[["index.html", "Statistical Reasoning through Computation and R Preface", " Statistical Reasoning through Computation and R Brandon LeBeau and Andrew S. Zieffler January 12, 2022 Preface This book provides a modern statistical reasoning and introduction to statistics text. Computation, using the R programming language, are used instead of relying on traditional statistical theory. These analyses can often be categorized into two broad categories, Descriptive Statistics Inferential Statistics Descriptive Statistics help to describe the data and are particularly useful to give a single numeric summary for a single variable. We will explore this idea more fully in this section. Inferential Statistics help us to make broader statements from the data we have to the larger group of interest, commonly referred to as the population. More details on these steps later in the course. "],["introduction.html", "Chapter 1 Introduction 1.1 Statistics vs Data Science 1.2 Experiments vs Observations 1.3 Data Structure", " Chapter 1 Introduction Here is an intro. And more. 1.1 Statistics vs Data Science 1.2 Experiments vs Observations The type of analysis that is relevant for a given problem depends on the experimental design. More generally, experimental design refers to how the data were collected. How the data were collected has implications for the type of conclusions that can be drawn from the data (ie., you may have heard the phrase, “correlation does not imply causation”) and the subsequent analysis. A extremely sophisticated analysis does not overcome limitations in how the data were collected. Given that how the data were collected greatly impacts the types of conclusions that can be drawn from the analysis, what are design features are important to consider? This topic is large, nuanced, and an entire series of courses have been created on this topic. The goal here is to think about some concepts that are particularly helpful to consider for any analysis. The discussion will be framed around articulating whether the data collected were part of an experiment or were simply observed. The simplest definition for observational data are those that were collected without strong consideration about who is or how the data are collected. One example of observational data could be collected information about the shoes that people wear as they are walking in a busy part of town. Given this type of data collected, what could be some limitations about this type of data? To contrast observational data, experimental data are those in which care was taken to how the data were collected. Within experimental data, it is common for there to be two or more conditions that are being explored. For example, in a clinical trial that is testing whether a vaccine or drug is effective and safe, the new vaccine or drug is often tested against a placebo. The placebo is a harmless substance that does not produce any change to the body, for example, the placebo could be a sugar pill that looks just like a new drug to be tested in every other way. The placebo would simply not contain the active ingredients of the new drug. When conducting these experiments with two or more groups that are of interest to be compared, those who are participating in the study are randomly assigned or selected to be in one of the groups. Using the placebo vs new drug example, this would mean that each participant is randomly assigned to either receive the placebo or the new drug, but the participant does not know which one they are receiving. Often, those administering the treatment also do not know whether the placebo or active drug is being given as well. 1.2.1 Explore Random Assignment How does the random assignment of individuals to treatment conditions change the design from an observational to an experimental design? The random process is really the primary differentiator between being an observational vs an experiment. The random assignment to each condition in the study has the ability to, on average, even out differences across the two groups. Since the inclusion of being in one of the two groups is random, if the study has enough participants, it is more likely for the two groups to be as equal as possible across all characteristics for the study participants. What could random assignment look like in practice? More to come … . 1.2.2 Example: Natural Experiments 1.3 Data Structure Data are often stored in tabular form for ease of use with common statistical programs, however data need not be in this structure. Data can come from anywhere and could consist of text, numbers representing some quantity, text labels representing groups, or many other formats. This section aims to give an introduction to the form and format of data, both common and uncommon. 1.3.1 Tabular Data Tabular data are those that are most commonly used in statistics courses. Tabular data are such that rows indicate unique cases of data and the columns represent different attributes for those cases. Table 1.1 shows the an example of tabular data. Table 1.1: Example of tabular data. name human fictional height He-Man Yes Yes 83 She-Ra Yes Yes 96 Voltron No Yes 3936 In this table, each row represents a unique person and the first column would represent the identifying attribute for each unique row. For example, the first row represents characteristics for He-Man and each subsequent column represents specific attributes about them. For example, the second and third columns are Yes/No attributes indicating if the person is human or fictional or not. What are the primary differences between the second and third columns? Notice that the third column, the fictional attribute, is one in which all elements in the current tabular data are the same value. This would be an attribute that does not vary across the different rows in the data. These attributes are not helpful from a statistical perspective in this small data set, but if more data were added in which some elements were not fictional, then this attribute would contain useful information. In statistics terminology, this type of attribute would be called a constant attribute. In statistics, we are interested in attributes that vary across our units in the data. Attributes that vary are often referred to as variables in statistics terminology. Throughout this textbook, the term attribute will be used instead of variable. An attribute will refer to a column of data that carries information about the units in the data. The final column of data in Table 1.1 is one that represents the approximate height of each character, in inches.1 This attribute is different from the rest of the attributes in that it is a numeric quantity for each unit. Numeric quantities usually carry more information about magnitude differences compared to the text attributes described earlier. More explicitly, differences in the height attributes can be quantified and in this case, with the attribute being represented as the height in inches, each unit represents the same distance across the entire scale. That is, a one inch difference across the entire height scale is the same no matter where that inch occurs on the scale. These types of attributes will be used extensively in this text, most commonly these types of attributes will be used as the attribute of most interest in our analyses. For example, from the data above, it could be asked if there are differences in height of the characters based on if they are human or not. For this question, differences in height for those that are human (ie., human attribute = “Yes”) compared to those that are not human (ie., human attribute = “No”) would be used as the primary comparison. This text will explore questions like this from a descriptive and inferential framework later. For these data, there are only three rows, but you could imagine adding more rows to these elements for other characters. For example, imagine adding a new row to this data for a rabbit. Take a few minutes to think about what a new row with the rabbit would look like for each column of Table 1.1. Would any of the columns in the table change from a constant to now being a variable? 1.3.2 Non-tabular Data Data can come in many different formats, this book will not extensively cover data that are not in a tabular format. However, non-tabular data are very common in practical problems. In these situations, these data are often wrangled into a more structured format to conduct a statistical analysis. Some common non-tabular data formats include data coming from text, video, audio, graphics, images, sensors, and even more. This is based on a search for the character name plus height, for example, “He-Man height.”↩︎ "],["visualization.html", "Chapter 2 Visualization 2.1 Exploring Attributes 2.2 Plot Customization 2.3 Density plots", " Chapter 2 Visualization Data scientists and statisticians visualize data to explore and understand data. Visualization can help analysts identify features in the data such as typical or extreme observations. Visualizations can also help describe variation. Because it is so powerful, data visualization is often the first step in any statistical analysis. The topic of data visualization can be extremely large and entire textbooks are written about the topic.2 Given the powerful nature of data visualization, only an introduction of data visualization will be covered in this book, with the goal of using them for interpreting statistical results. As such, only a handful of data visualizations will be explored, described, and used. Data visualization can be broken down into many different components and in many different ways. In this book, we aim to differentiate data visualization into two different types. First, in this chapter, the goal will be to explore a single attribute of interest, often referred to as univariate visualization. The term, univariate, can be split into two pieces, one, “uni-”, meaning one, and second, “variate”, referring to the attribute or variable explored. Often, univariate data visualization is used for the attribute that is deemed the outcome or the attribute of interest, but this does not need be the only attribute explored in such a way. The goals of the univariate data visualization is to understand more about the single attribute of interest, what are key features, are there extreme values, does the attribute spread over a lot of possible values or are the data more homogeneous (i.e., similar). The second type of data visualization discussed in this book is that of multivariate data visualization. The phrase “multivariate” can again be broken into two components, “multi-” referring to more than one, and “variate”, referring to the attribute of variables. This will be discussed more in chapter 4 of this book, but multivariate visualization is used primarily to understand how more than one attribute in the data may be related to one another. Multivariate data visualization can also be useful for understanding if there are differences across smaller subgroups in the data. This chapter will focus on univariate or single attribute data visualization. The goals for this chapter are to introduce common univariate data visualizations, how to interpret these data visualizations, and explore cases where each type of data visualization is useful to use. 2.0.1 College Scorecard Data The U.S. Department of Education publishes data on institutions of higher education in their College Scorecard (https://collegescorecard.ed.gov/) to facilitate transparency and provide information for interested stakeholders (e.g., parents, students, educators). A subset of this data is provided in the file College-scorecard-clean.csv. To illustrate some of the common methods statisticians use to visualize data, we will examine admissions rates for 2,019 institutions of higher education. Before we begin the analysis, we will load two packages, the tidyverse package and the ggformula package. These packages include many useful functions that we will use in this chapter. library(tidyverse) library(ggformula) There are many functions in R to import data. We will use the function read_csv() since the data file we are importing (College-scorecard-clean.csv) is a comma separated value (CSV) file..3 CSV files are a common format for storing data. Since they are encoded as text files they generally do not take up a lot of space nor computer memory. They get their name from the fact that in the text file, each data attribute (i.e. column in the data) is separated by a comma within each row. Each row represents a unique case or observation from the data. The syntax to import the college scorecard data is as follows: colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) In this syntax we have passed two arguments to the read_csv() function. The first argument, file=, indicates the path to the data file. The data file here is stored on GitHub, so the path is specified as a URL. The second argument, guess_max=, helps ensure that the data are read in appropriately. This argument will be described in more detail later. The syntax to the left of the read_csv() function, namely colleges &lt;-, takes the output of the function and stores it, or in the language of R, assigns it to an object named colleges. In data analysis, it is often useful to use results in later computations, so rather than continually re-running syntax to obtain these results, we can instead store those results in an object and then compute on the object. Here for example, we would like to use the data that was read by the read_csv() function to explore it. When we want to assign computational results to an object, we use the assignment operator, &lt;- . (Note that the assignment operator looks like a left-pointing arrow; it is taking the computational result produced on the right side and storing it in the object to the left side.) 2.0.2 View the Data Once we have imported and assigned the data to an object, it is quite useful to ensure that it was read in appropriately. The head() function will give us a quick snapshot of the data by printing the first six rows of data. head(colleges) ## # A tibble: 6 × 17 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama … Normal AL Bachel… South… City:… 0.903 18 4824 22886 ## 2 Universi… Birmi… AL Bachel… South… City:… 0.918 25 12866 24129 ## 3 Universi… Hunts… AL Bachel… South… City:… 0.812 28 6917 22108 ## 4 Alabama … Montg… AL Bachel… South… City:… 0.979 18 4189 19413 ## 5 The Univ… Tusca… AL Bachel… South… City:… 0.533 28 32387 28836 ## 6 Auburn U… Montg… AL Bachel… South… City:… 0.825 22 4211 19892 ## # … with 7 more variables: costt4_p &lt;dbl&gt;, tuitionfee_in &lt;dbl&gt;, ## # tuitionfee_out &lt;dbl&gt;, debt_mdn &lt;dbl&gt;, grad_debt_mdn &lt;dbl&gt;, female &lt;dbl&gt;, ## # bachelor_degree &lt;dbl&gt; We can also include an interactive version for viewing the book on the web using the DT package. DT::datatable(colleges) 2.1 Exploring Attributes Data scientists and statisticians often start analyses by exploring attributes (i.e., variables) that are of interest to them. For example, suppose we are interested in exploring the admission rates of the institutions in the college scorecard data to determine how selective the different institutions are. We will begin our exploration of admission rates by examining different visualizations of the admissions rate attribute. There is not one perfect visualization for exploring the data. Each visualization has pros and cons; it may highlight some features of the attribute and mask others. It is often necessary to look at many different visualizations of the data in the exploratory phase. One of the primary goals of any data visualization, especially those in this chapter, are to summarize (think, simplify) the data so that it can be more easily processed to understand key components of the attribute being explored. To be more explicit, it would be possible to explore all 2019 of the raw data to see the exact admission rate for each institution. However, if the goal is to know overall trends for the admission rates of institutions, knowing the exact values for each institution from the table would be too unwieldy. Instead, the goal of the data visualization to simplify the attribute to understand better the key components of the admission rate attribute. This is a trade-off, as there is a loss of information, but this loss of information is useful in this context as it allows for the summarization of the attribute. 2.1.1 Histograms The first visualization we will examine is a histogram. We can create a histogram of the admission rates using the gf_histogram() function. (This function is part of the ggformula package which needs to be loaded prior to using the gf_histogram() function.) This function requires two arguments. The first argument is a formula that identifies the variables to be plotted and the second argument, data =, specifies the data object that was assigned on data import. For example, earlier we used the read_csv() function to import the college scorecard data and we assigned this to the name, colleges. The syntax used to create a histogram of the admission rates is: gf_histogram(~ adm_rate, data = colleges) Figure 2.1: Histogram of college admission rates The formula we provide in the first argument is based on the following general structure: ~ attribute name where the attribute name identified to the right of the ~ is the exact name of one of the columns in the colleges data object. 2.1.2 Interpretting Histograms Histograms are created by collapsing the data into bins and then counting the number of observations that fall into each bin. To show this more clearly in the figure created previously, we can color the bin lines to highlight the different bins. To do this we include an additional argument, color =, in the gf_histogram() function. We can also set the color for the bins themselves using the fill = argument. Here we color the bin lines black and set the bin color to yellow.4 gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;) Figure 2.2: Histogram changing the color and fill of the bars. When looking at a single bar, for example the bar that is at 0.50, shows the number of institutions with admissions rates between about 0.48 and 0.52. In this case, there are about 80 institutions that have admissions rates between 0.48 and 0.52 (i.e., 48% to 52% admission rates). Similar interpretations are found for all of the other bars as well. One common assumption made with a histogram is for the width of the bars be the same width on the attribute of interest. For example, the single bar interpreted in the preceding paragraph, the width of the bar was about 4% on the admission rate scale. Therefore, with the assumption that all bars have the same width, this would mean that all of the 30 bars in the histogram would each range by about 4%. Rather than focusing on any one bin, we typically want to describe the distribution as a whole. For example, it appears as though most institutions admit a high proportion of applicants since the bins to the right of 0.5 have higher counts than the bins that are below 0.5. There are, however, some institutions that are quite selective, only admitting fewer than 25% of the students who apply. 2.1.2.1 Adjusting Number of Bins Interpretation of the distribution is sometimes influenced by the width or number of bins. It is often useful to change the number of bins to explore the impact this may have on your interpretation. This can be accomplished by either (1) changing the width of the bins via thebinwidth = argument in the gf_histogram() function, or (2) changing the number of bins using the bins = argument. The binwidth in the histogram refers to the range or width of each bin. A larger binwidth would mean there are fewer bins as it would take fewer bins to span the entire range of the attribute of interest. In contrast, a smaller binwidth would require more bins to span the entire range of the attribute of interest. In contrast, the number of bins can be specified directly, for example 10 or 20 bins. The default within the R graphics package used is 30. Within this framework, each bin will have the same width or binwidth. The relationship between the number of bins and binwidth could be shown with the following equation: \\[ binwidth = \\frac{attribute\\ range}{\\#\\ of\\ bins} \\] To be more explicit, suppose that we wanted there to be 25 bins, using algebra we could compute the new binwidth given that we know we want 25 bins and knowing the range of the original attribute. The admission rates attribute have values as small as 0 and as large as 1. Therefore, the total range would be 1 (1 - 0 = 1). The binwidth could then be computed as: \\[ bindwidth = \\frac{1}{25} = .04 \\] In contrast, if we wanted to specify the binwidth instead of the number of bins, we could do a little bit of algebra in the equation above to compute the number of bins needed to span the range of the attribute given the specified binwidth. For example, if we wanted the binwidth to be .025, 2.5%, we could compute this as follows: \\[ \\#\\ of\\ bins = \\frac{1}{.025} = 40 \\] We will take the approach of letting the software compute these, but the equations above shows the general process that is used by the software in selecting the binwidth. More bins/smaller binwidth can give a slightly more nuanced interpretation of the attribute of interest, whereas fewer bins/large binwidth will do more summarization. Having too few bins or too many bins can make the figure more difficult to interpret by missing key features of the attribute or including too many unique features of the attribute. For this reason, it is often of interest to adjust the binwidth or number of bins to explore the impact on the interpretation. The code below changes the binwidth to specify it as .01 via the binwidth = .01 argument with the figure shown in Figure 2.3. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, binwidth = .01) Figure 2.3: Histogram modifying the binwidth. The code below specifies 10 bins via the bins = 10 argument with the figure shown in Figure ??. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 10) Figure 2.4: Histogram modifying the number of bins. In general, our interpretation remains the same across all the different binwidth/bins combinations, namely that most institutions admit a high proportion of applicants. When we used a bin width of 0.01, however, we were able to see that several institutions admit 100% of applicants. This was obscured in the other histograms we examined. As a data scientist these institutions might warrant a more nuanced examination. 2.2 Plot Customization There are many ways to further customize the plot we produced to make it more appealing. For example, you would likely want to change the label on the x-axis from adm_rate to something more informative. Or, you may want to add a descriptive title to your plot. These customizations can be specified using the gf_labs() function. Specific examples are given below. 2.2.1 Axes labels To change the labels on the x- and y-axes, we can use the arguments x = and y = in the gf_labs() function. These arguments take the text for the label you want to add to each axis, respectively. Here we change the text on the x-axis to “Admission Rate” and the text on the y-axis to “Frequency”. The gf_labs() function is connected to the histogram by linking the gf_histogram() and gf_labs() functions with the pipe operator (%&gt;%).5 gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39; ) Figure 2.5: Adding custom x and y-axis labels. 2.2.2 Plot title and subtitle We can also add a title and subtitle to our plot. Similar to changing the axis labels, these are added using gf_labs(), but using the title = and subtitle = arguments. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) Figure 2.6: Adding plot title and subtitle. Plot titles and subtitles are helpful to used to provide context to the figure and describe the overall purpose for the figure. For example, the subtitle in Figure 2.6 describes the source for the data plotted. 2.2.3 Plot theme By default, the plot has a grey background and white grid lines. This can be modified to using the gf_theme() function. For example, in the syntax below we change the plot theme to a white background with no grid lines using theme_classic(). Again, the gf_theme() is linked to the histogram with the pipe operator. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) %&gt;% gf_theme(theme_classic()) Figure 2.7: Varying figure theme. We have created a custom theme to use in the gf_theme() function that we will use for most of the plots in the book. The theme, theme_statthinking() is included in the statthink package, a supplemental package to the text that can be installed and loaded with the following commands: remotes::install_github(&#39;lebebr01/statthink&#39;) library(statthink) ## ## Attaching package: &#39;statthink&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## colleges We can then change the theme in a similar manner to how we changed the theme before. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,000 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) %&gt;% gf_theme(theme_statthinking()) Figure 2.8: Histogram showing distribution of admission rates for institutions of higher education. 2.2.3.1 Setting the default plot theme Since we will be using this theme for all of our plots, it is useful to make it the default theme (rather than the grey background with white gridlines). To set a different theme as the default, we will use the theme_set() function and specify the theme_statthinking() theme as the argument within this function. theme_set(theme_statthinking(base_size = 14)) Now when we create a plot, it will automatically use the statthinking theme without having to specify this in the gf_theme() function. Often, the plot theme would be one of the first lines of code for any analysis so that all the figures created in subsequent areas would all have a similar theme/style. In the code chunk above, the size of the text is increased slightly to make it a bit easier to read throughout the book. This was done with the base_size = argument, where the font size was specified to be size 14. Figure 2.9 shows that the theme is now applied automatically without needing to use the gf_theme() function to specify. In addition, the font size is increased as well compared to previous figures created in this chapter. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,000 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) Figure 2.9: Histogram showing the distribution of admission rates for institutions of higher education. 2.3 Density plots Another plot that is useful for exploring attributes is the density plot. This plot usually highlights similar distributional features as the histogram, but the visualization does not have the same dependency on the specification of bins. Density plots can be created with the gf_density() function which takes similar arguments as gf_histogram(), namely a formula identifying the attribute to be plotted and the data object.6 If you compare the code specified for the very first histogram, notice that only the function name changed. gf_density(~ adm_rate, data = colleges) Figure 2.10: Density curve of admission rates for institutions of higher education. 2.3.1 Interpreting Density Plots Density plots are interpreted similarly to a histogram in that areas of the density curve that are higher indicate more data in those areas of the attribute of interest. Places where the density curve are lower indicate areas where data occur infrequently. The density metric on the y-axis is not the same as the histogram, but the relative magnitude can be interpreted similarly. That is, higher indicates more data in that region of the attribute. Just like the histogram, the attribute being depicted in the density curve is on the x-axis. Therefore, important features for the attribute of interest can be found by looking at the y-axis, but then the place where high or low prevalence occurs are depicted by looking back to the x-axis. For example, when looking at the density curve in Figure 2.11, the density curve has a peak on the y-axis density scale of just under 2.0, the peak of this density curve occurs around a 0.75 as shown on the x-axis. Our interpretation remains that most institutions admit a high proportion of applicants. In fact, colleges that admit around 75% of their applicants have the highest probability density, indicating this is where most of the institutions are found in the distribution. Additionally, there are just a few institutions that are have an admission rate 25% or less. The axis labels, title, subtitle can be customized with gf_labs() in the same manner as with the histogram. The color = and fill = arguments in gf_density() will color the density curve and area under the density curve, respectively. gf_density(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Probability density&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) Figure 2.11: Density curve showing the distribution of admission rates for institutions of higher education. Clause Wilke has a great book on the Fundamentals of Data Visualization and Kieran Healy has another great book data visualization called Data Visualization: A practical introduction. Both of these books are also freely available online with print versions able to be purchased as well.↩︎ This function is a part of the tidyverse package, so you need to be sure to run library(tidyverse) prior to using read_csv().↩︎ R knows the names of 657 colors. To see these names type colors() at the command prompt.↩︎ The pipe operator, %&gt;%, can be read as “then” and the code can be thought of as logical processes. For example, first the histogram is created, then (%&gt;%), the labels for the x- and y- axes are changed.↩︎ The default kernel used in gf_density() is the normal kernel. This can be changed if desired, but the default works well in many situations.↩︎ "],["descriptive-statistics-numerically-describing-the-sample-data.html", "Chapter 3 Descriptive Statistics: Numerically Describing the Sample Data 3.1 Summarizing Attributes 3.2 Understanding the Median and Mean 3.3 Numerically Summarizing Variation 3.4 Summarizing Categorical Attributes 3.5 Advanced Extension: Computing Your Own Measure of Variation", " Chapter 3 Descriptive Statistics: Numerically Describing the Sample Data Data visualization is often the first step on the statistical journey to explore a research question. However, this is usually not where the journey stops, instead additional analyses are often performed to learn more about the trends and structure in the data. In this chapter we will learn about methods that useful for numerically summarizing a sample of data. These methods are commonly referred to as descriptive statistics. We will again use the data provided in the file College-scorecard-clean.csv to examine admissions rates for 2,019 institutions of higher education. As in the previous chapter, before we begin the analysis, we will load several packages that include functions we will use in the chapter. We also import the College Scorecard data using the read_csv() function. # Load packages library(tidyverse) library(ggformula) library(mosaic) library(statthink) # Set theme for plots theme_set(theme_statthinking()) # Import the data colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) # View first six cases DT::datatable(colleges) 3.1 Summarizing Attributes Data are often stored in a tabular format where the rows of the data are the cases and the columns are attributes. For example, in the college scorecard data (displayed above) the rows each represent a specific institution of higher education (cases) and the columns represent various attributes measured on those higher education institutions. This type of tabular representation is a common structure for storing and analyzing data. In the previous chapter, we visualized different attributes by referencing those attributes in the function we used to create a plot of the distribution. For example, when we wanted to plot a histogram of the distribution of admission rates, we referenced the adm_rate attribute in the gf_histogram() function. In a similar vein, we will obtain numerical summaries of an attribute by referencing that attribute in the df_stats() function. Below, we obtain numerical summaries for the admissions rate attribute: df_stats(~ adm_rate, data = colleges, median) ## response median ## 1 adm_rate 0.7077 The df_stats() function takes a formula syntax that is the same as the formula syntax we introduced in the previous chapter. In particular, the variable that we wish to compute a statistic on is specified after the ~. We also specify the data object with the data = argument. Finally, we include additional arguments indicating the name of the particular numerical summary (or summaries) that we want to compute.7 In the syntax above, we compute the median admission rate. The median is also referred to as the 50th percentile, and is the value at which half of the admission rates in the data are above and half are below. In our data, the median admission rate is 70.8%. In our data 1,009 institutions have an admission rate below 70.8% and 1,009 have an admission rate above 70.8%. In the histogram shown in Figure 3.2, we add a vertical line at the median admission rate to help you visualize where this statistic would fall in the distribution of admission rates. Another common numerical summary that is often used to describe a distribution is the mean. To compute the mean admission rate we again use the df_stats() function, but include mean as our additional argument. This replaces the median function from the previous code using the df_stats() function. df_stats(~ adm_rate, data = colleges, mean) ## response mean ## 1 adm_rate 0.6827355 The mean (or average) admission rate for the 2,019 institutions of higher education is 68.3%. 3.2 Understanding the Median and Mean In your previous educational experiences with the mean and median, you may have learned the formulas or algorithms that produce these values. For example: Mean: Add up all the values of the attribute and divide by the number of values; Median: Order all the values of the attribute from smallest to largest and find the one in the middle. If there is an even number of observations, find the mean of the middle two values. To better understand these summaries, we will visualize them on the distribution of admission rates. Figure 3.1: Distribution of admission rates for thw 2,019 institutions of higher education. The mean admission rate is displayed as a red, dashed line. The mean (displayed as a red, dashed line in Figure 3.1) represents the “balance point” of the distribution. If the distribution were a physical entity, it is the location where you would put your finger underneath the distribution to balance it. In a statistical sense, we balance the distribution by “balancing” the deviations. To explain this, let’s examine a toy data set of five observations: \\[ Y = \\begin{pmatrix}10\\\\ 10\\\\ 20\\\\ 30\\\\ 50\\end{pmatrix} \\] The mean of these five values is 24. Each of these values has a deviation which is computed as the difference between the observed value and the mean value. For the toy data, \\[ Y = \\begin{pmatrix}10 - 24\\\\ 10-24\\\\ 20-24\\\\ 30-24\\\\ 50-24\\end{pmatrix} = \\begin{pmatrix}-14\\\\ -14\\\\ -4\\\\ 6\\\\ 26\\end{pmatrix} \\] Notice that some of the deviations are negative (the observation was below the mean) and some are positive (the observation was above the mean). The mean “balances” these deviations since the sum of the deviations is 0. What if we had instead looked at the deviations from the median, which is 20? \\[ Y = \\begin{pmatrix}10 - 20\\\\ 10-20\\\\ 20-20\\\\ 30-20\\\\ 50-20\\end{pmatrix} = \\begin{pmatrix}-10\\\\ -10\\\\ 0\\\\ 10\\\\ 30\\end{pmatrix} \\] The median does not balance the deviations; the sum is not zero (it is \\(+20\\)). The mean is the only value we can use to “balance” the deviations. In this sense, the mean is optimal and this is another reason the mean is used much more frequently than the median in many statistical applications. What about the median? Figure 3.2: Histogram depicting the median of the distribution. In Figure 3.2, half of the observations in the histogram have an admission rate below the blue line and half have an admission rate above the blue line. The median splits the distribution into two equal areas; as such, the median is equivalent to the 50th percentile. Note that the median is not necessarily in the middle of the values represented on the \\(x\\)-axis; that would be 0.50 rather than 0.708. It is the area under the curve (or embodied by the histogram) that is halved. We will come back to this idea later when we explore a type of figure called the empirical cumulative density to show another way to summarize the distribution and discuss percentiles. 3.2.1 Summarize with the Mean or Median? The goal of summarizing the distribution numerically is to provide a value that typifies or represents the observed values of the attribute. In our example, we need a value that summarizes the 2,019 admission rates. Since the mean balances the deviations, it is representative because it is the value that is “closest” (at least on average) to all of the observations. (It is the value that produces the smallest average deviation—since the sum of deviations is 0, the average deviation is also 0.) The median is representative because half of the distribution is smaller than that value and the other half is larger. But, does one represent the distribution better than the other? Figure 3.3: Distribution of the admission rates for 2,019 institutions of higher education. The mean admission rate is displayed as a red, dashed line. The median admission rate is displayed as a blue, solid line. Figure 3.3 shows both the mean and median on the same plot, the red dashed line represents the mean and the blue dashed line is the median. In this example, both values are quite similar, so either would send a similar message about the distribution of admission rates, namely that a typical admission rate for these 2,019 institutions of higher education is around 70%. Looking at the plot, we see that the mean admission rate is lower than the median admission rate. In a left-skewed distribution this will often be the case. The mean is “pulled toward the tail” of the distribution. This is because the mean is influenced by extreme values (which in a skewed distribution are in the tail). The median is not influenced by extreme values; we say it is robust to these values. This is because in calculating the median, only the middle score (or middle two scores) are used, so its value is not informed by the extreme values in the distribution.8 In practice, it is a good idea to compute both the mean and the median and explore whether one is more representative than the other (perhaps by plotting them on the distribution). The choice of one over the other should also be guided by substantive knowledge. For example, in highly skewed distributions such as personal income or house prices, the median is often favored or commonly reported. The median is used due to its robust property discussed earlier. If you think about what a distribution of personal income may look like, the majority of individuals make low to moderate incomes, with a few individuals making much larger incomes.9 As such, the median is often more representative of typical household income than the mean. The strong positive/right skewed (right sided tail) income distribution would result in the mean being pulled much larger than the median due to the balance property discussed earlier. 3.3 Numerically Summarizing Variation In the distribution of admission rates, both the mean and median seem to offer a representative admission rate since both are close to the modal clump of the distribution. (There are several colleges that have an admission rate close to 70%.) But, you will also notice that an admission rate of 70% does not do a great job representing all of the institutions’ admissions rates. This is true for any single statistic we pick to summarize the distribution. To more fully summarize the distribution we need to summarize the variability in the distribution in addition to a “typical” or representative value. There are several summaries that statisticians and data scientists use to describe the variation in a distribution. And, like the representative summary measures, each of the summaries of variation provide slightly different information by highlighting different aspects of the variability. We will explore some of these measures below. 3.3.1 Range One measure of variation that you have almost surely encountered before is the range. This numerical measure is the difference between the maximum and minimum values in the data. To compute this we provide the df_stats() function with two additional arguments, min and max. Then we can compute the difference between these values. # Obtain minimum and maximum admission rate df_stats(~ adm_rate, data = colleges, min, max) ## response min max ## 1 adm_rate 0 1 # Compute range 1 - 0 ## [1] 1 The range of the admission rates is 1. When people colloquially describe the range, they typically provide the limits of the data rather than actually providing the range. For example, they may describe the range of the admission rates as: “the admission rates range from 0 to 1”. While this is technically not the range (which is a single number), it is probably more descriptive as it also gives a sense of the lower- and upper-limits for the observations. One problem with the range is that if there are extreme values, the range will not give an accurate picture of the variation encompassing most observations. For example, consider the following five test scores: \\[ Y = \\begin{pmatrix}30\\\\ 35\\\\ 36\\\\ 37\\\\ 100\\end{pmatrix} \\] The range of these data is 70, indicating that the variation between the lowest and highest score is 70 points, suggesting a lot of variation in the scores. The range, however, is clearly influenced by the score of 100. Were it not for that score, we would have a much different take on the score variability; the other scores are between 30 and 37 (a range of 7), suggesting that there is not a lot of differences in the test scores.10 While the range is not the best measure of variation, it is quite useful as a validity check on the data to ensure that the attribute’s values are theoretically possible. In this case the values are all between 0 and 1, which are values that are theoretically plausible for admission rate. If for example, we had found a data value that was 1.1, this would represent a data error. We would want to confirm that case was entered correctly, or if no adjustment could be made, remove this case from further analysis as a proportion can not be larger than 1. 3.3.2 Percentile Range One way to deal with extreme values in the sample is simply to not include them when we calculate the range. For example, instead of computing the difference between the maximum and minimum value in the data (which includes extreme values), truncate the bottom 10% and upper 10% of the data and calculate the range between the remaining maximum and minimum values. This is essentially the range of the middle 80% of the data. To compute the endpoint after truncating the lower- and upper 10% we will use the quantile() function. This function finds the data value for an associated percentile provided to the function. If we want to truncate the lower- and upper 10% of a distribution we are interested in finding the values associated with the 10th and 90th percentiles. The syntax below shows two manners for obtaining these values for the admissions rate attribute. # Provide both percentiles separately colleges %&gt;% df_stats(~ adm_rate, quantile(0.10), quantile(.90)) ## response 10% 90% ## 1 adm_rate 0.39284 0.94706 # Provide both percentiles in a single quantile() call colleges %&gt;% df_stats(~ adm_rate, quantile(c(0.1, 0.9))) ## response 10% 90% ## 1 adm_rate 0.39284 0.94706 # Compute the range ofthe middle 80% of the data 0.94706 - 0.39284 ## [1] 0.55422 The range of admissions rates for 80% of the 2,019 institutions of higher education is 0.554. We can visualize this by adding the percentiles on the plot of the distribution of admission rates, shown in Figure 3.4. These values seem to visually correspond to where most of the data are concentrated and may provide a better indication for the variability for most of the data or the typical cases in the data. Figure 3.4: Distribution of the admission rates for 2,019 institutions of higher education. The solid, red lines are placed at the 10th and 90th percentiles, respectively. 3.3.3 Interquartile Range (IQR) One percentile range that statisticians and data scientists use a great deal is the interquartile range (IQR). This range demarcates the middle 50% of the distribution; it truncates the lower and upper 25% of the values. In other words it is based on finding the range between the 25th- and the 75th-percentiles. # Obtain values for the 25th- and 75th percentiles colleges %&gt;% df_stats(~ adm_rate, quantile(c(0.25, 0.75))) ## response 25% 75% ## 1 adm_rate 0.5524 0.83815 # Compute the IQR 0.83815 - 0.5524 ## [1] 0.28575 The range of admission rates for the middle 50% of the distribution is 28.5%. Since it is based on only 50% of the observations, the IQR no longer gives the range for “most” of the data, but, as shown in Figure 3.5, this range encompasses the modal clump of institutions’ admission rates and can be useful for describing the variation. Figure 3.5: Distribution of the admission rates for 2,019 institutions of higher education. The solid, red lines are placed at the 25th and 75th percentiles, respectively. Since the IQR describes the range for half of the observations, it can also be useful to compare this range with the entire range of the data. Below we compute these values and visualize them on a histogram of the distribution, see Figure 3.6. # Obtain values for the 25th- and 75th percentiles colleges %&gt;% df_stats(~ adm_rate, min, quantile(c(0.25, 0.75)), max) ## response min 25% 75% max ## 1 adm_rate 0 0.5524 0.83815 1 # Compute the IQR 0.83815 - 0.5524 ## [1] 0.28575 # Compute the range 1 - 0 ## [1] 1 Figure 3.6: Distribution of the admission rates for 2,019 institutions of higher education. The solid, red lines are placed at the 25th and 75th percentiles, respectively. The dashed, blue lines are placed at the minimum and maximum values, respectively. Although our sample of 2,019 institutions of higher education have wildly varying admissions rates (from 0% to 100%), the middle half of those institutions have admissions rates between 55% and 84%. We also note that the 25% of institutions with the lowest admissions rate range from 0% to 55%, while the 25% of institutions with the highest admissions rate range from only 84% to 100%. This means that there is more variation in the admissions rates in the institutions with the lowest admissions rate than in the institutions with the highest admissions rates. Understanding how similar the range of variation is in these areas of the distribution can give us information about the shape of the distribution. For example, the bigger range in the lowest 25% of the data suggests that the distribution has a tail on the left side. Seventy-five percent of the institutions have admissions rates higher than 50%. These two features suggest that the distribution is left-skewed (which we also see in the histogram). When we describe the shape of a distribution, we are actually describing the variability in the data! Examining the lowest 25%, highest 25%, and middle 50% of the data is so common that a statistician named John Tukey invented a visualization technique called the box-and-whiskers plot to show these ranges. To create a box-and-whiskers plot we use the gf_boxploth() function.11 This function takes a formula that is slightly different than we have been using, namely 0 ~ attribute name. (Note that the 0 in the formula is where the box-and-whiskers plot is centered on the y-axis.) gf_boxploth(0 ~ adm_rate, data = colleges, fill = &quot;skyblue&quot;) %&gt;% gf_labs(x = &quot;Admission rate&quot;) The “box”, extending from 0.55 to 0.84, depicts the interquartile range; the middle 50% of the distribution. The line near the middle of the box is the median value. The “whiskers” extend to either the end of the range, or the next closest observation that is not an extreme value.12 There are several extreme values on the left-hand side of the distribution representing institutions with extremely low admission rates. The length of the whisker denotes the range of the lowest 25% of the distribution and the highest 25% of the distribution. We can also display both the histogram and the box-and-whiskers plot. In the syntax below, we center the box-and-whiskers plot at \\(y=170\\). We also make the box slightly wider to display better in the plot. The resulting figure is shown in Figure 3.7. gf_histogram(~ adm_rate, data = colleges, bins = 30) %&gt;% gf_boxploth(170 ~ adm_rate, data = colleges, fill = &quot;skyblue&quot;, width = 10) %&gt;% gf_labs(x = &quot;Admission rate&quot;, y = &quot;Frequency&quot;) Figure 3.7: Histogram and boxplot shown together. 3.3.4 Empirical Cumulative Density The percentile range plots and the boxplot indicated the values of the distribution that demarcated a particular proportion of the distribution. For example, the boxplot visually showed the admission rates that were at the 25th, 50th, and 75th percentiles. Another plot that can be useful for understanding how much of a distribution is at or below a particular value is a plot of the empirical cumulative density. To create this plot we use the gf_ecdf() function from the ggformula package. gf_ecdf(~ adm_rate, data = colleges) %&gt;% gf_labs(x = &quot;Admission rate&quot;, y = &#39;Cumulative proportion&#39;) To read this plot, we can map admission rates to their associated cumulative proportion. For example, one-quarter of the admission rates in the distribution are at or below 0.55; that is the admission rate of 0.55 has an associated cumulative proportion of 0.25. Similarly, an admission rate of 0.71 is associated with a cumulative proportion of 0.50; one-half of the admission rates in the distribution are at or below the value of 0.71. The cumulative proportion depicted on the y-axis are the same as the percentiles for the distribution. For example, the admission rate of 0.55 has an associated cumulative proportion of 0.25, is also the 25th percentile. As such, the cumulative proportion represents the amount of data that fall below the associated score on the x-axis. 3.3.5 Variance and Standard Deviation Two measures of variation that are commonly used by statisticians and data scientists are the variance and the standard deviation. These can be obtained by including var and sd, respectively, in the df_stats() function. # Compute variance and standard deviation colleges %&gt;% df_stats(~ adm_rate, var, sd) ## response var sd ## 1 adm_rate 0.04467182 0.2113571 The variance and standard deviation are related to each other in that if we square the value of the standard deviation we obtain the variance. # Square the standard deviation 0.2113571 ^ 2 ## [1] 0.04467182 In general, the standard deviation is more useful for describing the variation in a sample because it is in the same metric as the data. In our example, the metric of the data is proportion of students admitted, and the standard deviation is also in this metric. The variance, as the square of the standard deviation, is in the squared metric—in our example, proportion of students admitted squared. While this is not a useful metric in description, it does have some nice mathematical properties, so it is also a useful measure of the variation. 3.3.5.1 Understanding the Standard Deviation To understand how we interpret the standard deviation, it is useful to see how it is calculated. To do so, we will return to our toy data set: \\[ Y = \\begin{pmatrix}10 \\\\ 10\\\\ 20\\\\ 30\\\\ 50\\end{pmatrix} \\] Recall that earlier we computed the deviation from the mean for each of these observations, and that these deviations was a measure of how far above or below the mean each observation was. \\[ Y = \\begin{pmatrix}10 - 24\\\\ 10-24\\\\ 20-24\\\\ 30-24\\\\ 50-24\\end{pmatrix} = \\begin{pmatrix}-14\\\\ -14\\\\ -4\\\\ 6\\\\ 26\\end{pmatrix} \\] A useful measure of the variation in the data would be the average of these deviations. This would tell us, on average, how far from the mean the data are. Unfortunately, if we were to compute the mean we would get zero because the sum of the deviations is zero. (That was a property of the mean discussed earlier in the chapter!) To alleviate this, we square the deviations before summing them. \\[ \\begin{pmatrix}-14^2\\\\ -14^2\\\\ -4^2\\\\ 6^2\\\\ 26^2\\end{pmatrix} = \\begin{pmatrix}196\\\\ 196\\\\ 16\\\\ 36\\\\ 676\\end{pmatrix} \\] The sum of these squared deviations is 1,120. And the average squared deviation is then \\(1120/5 = 224\\); which would represent the population variance for these values. If we take the square root of 224, which is 14.97, we now have the average deviation for the five observations. On average, the observations in the distribution are 14.97 units from the mean value of 24. The standard deviation is interpreted as the average deviation from the mean.13 3.3.5.2 Using the Standard Deviation In our example, the mean admission rate for the 2,019 institutions of higher education was 68.2%, and the standard deviation was 21.1%. We can combine these two pieces of information to make a statement about the admission rates for most of the institutions in our sample. In general, most observations in a distribution fall within one standard deviation of the mean. So, for our example, most institutions have an admission rate that is between 47.1% and 89.3%.14 # 1 SD below the mean 0.682 - 0.211 ## [1] 0.471 # 1 SD above the mean 0.682 + 0.211 ## [1] 0.893 3.4 Summarizing Categorical Attributes Categorical attributes are attributes that have values that represent categories. For example, the attribute region indicates the region in the United States where the institution is located (e.g., Midwest). The attribute bachelor_degree is a categorical value indicating whether or not the institution offers a Bachelor’s degree. Sometimes statisticians and data scientists use the terms dichotomous (two categories) and polytomous (more than two categories) to further define categorical variables. Using this nomenclature, region is a polytomous categorical variable and bachelor_degree is a dichotomous categorical variable. Sometimes analysts use numeric values to encode the categories of a categorical attribute. For example, the attribute bachelor_degree is encoded using the values of 0 and 1. It is important to note that these values just indicate whether the institution offers a Bachelor’s degree (1) or not (0). The values are not necessarily ordinal in the sense that a 1 means more of the attribute than a 0. Since the values just refer to categories, an analyst might have reversed the coding and used 0 to encode institutions that offer a Bachelor’s degree and 1 to encode those institutions that do not. Similarly, the values of 0 and 1 are not sacrosanct; any two numbers could have been used to represent the categories.15 Most of the time, the numerical summaries we computed earlier in the chapter do not work so well for categorical attributes. For example, it would not make sense to compute the mean region for the institutions. In general, it suffices to compute counts and proportions for the categories included in these attributes. To compute the category counts we use the tally() function. This function takes a formula indicating the name of the categorical attribute and the name of the data object. To find the category counts for the region attribute: # Compute category counts tally(~region, data = colleges) ## region ## Far West Great Lakes Mid East New England ## 221 297 458 167 ## Outlying Areas Plains Rocky Mountains Southeast ## 35 200 50 454 ## Southwest US Service Schools ## 133 4 To find the proportion of institutions in each region, we can divide each of the counts by 2,019. # Compute category proportions tally(~region, data = colleges) / 2019 ## region ## Far West Great Lakes Mid East New England ## 0.109460129 0.147102526 0.226844973 0.082714215 ## Outlying Areas Plains Rocky Mountains Southeast ## 0.017335315 0.099058940 0.024764735 0.224863794 ## Southwest US Service Schools ## 0.065874195 0.001981179 You could also compute the proportions directly with the tally() function by specifying the argument format = \"proportion\". # Compute category proportions tally(~region, data = colleges, format = &quot;proportion&quot;) ## region ## Far West Great Lakes Mid East New England ## 0.109460129 0.147102526 0.226844973 0.082714215 ## Outlying Areas Plains Rocky Mountains Southeast ## 0.017335315 0.099058940 0.024764735 0.224863794 ## Southwest US Service Schools ## 0.065874195 0.001981179 Often, descriptively exploring both the raw counts and the proportions can be informative when exploring the data. This can be true for more complicated tables when more than one attribute are being explored at the same time, this will be explored in more detail in subsequent chapters. 3.5 Advanced Extension: Computing Your Own Measure of Variation If you have another non-standard measure of variation that you want to compute, you can always write your own function to compute it. For example, say you wanted to compute the mean absolute error (the mean of the absolute values of the deviations). To compute the mean absolute error, we first need to define a new function. mae &lt;- function(x, na.rm = TRUE, ...) { avg &lt;- mean(x, na.rm = na.rm, ...) abs_avg &lt;- abs(x - avg) mean(abs_avg) } We can now use this new function by employing it as an argument in the df_stats() function. colleges %&gt;% df_stats(~ adm_rate, mae) ## response mae ## 1 adm_rate 0.1692953 This statistic would then be interpreted as the average absolute deviation from the mean admission rate is about 16.9%. Note that the names of the summaries we include in the df_stats() function need to be the actual names of functions that R recognizes.↩︎ The downside of using the median is that it is only informed by one or two observations in the data. The mean is informed by all of the observations. This property of the mean makes it a more useful than the median in some mathematical and theoretical applications.↩︎ To be more specific, in 2014, 50% of households made less than $53,700 and 90% of households made less than $157,500 according to the US census bureau↩︎ A second issue with range is that it is a biased statistic. If we use it as an estimate of the population range, it will almost inevitably be too small. The population range will almost always be larger since the sampling process will often not select extreme population values.↩︎ The gf_boxplot() function creates vertical a box-and-whiskers plot, and the gf_boxploth() function creates a horizontal box-and-whiskers plot.↩︎ Extreme values are typically defined as ranged more than 1.5 times the IQR from either the lower or upper portion of the box. Recall, the lower and upper portion of the box reflect the 25th and 75th percentiles respectively.↩︎ Technically, after summing the squared deviations, we divide this sum by \\(n-1\\) rather than \\(n\\). But, when the sample size is even somewhat large, this difference is trivial.↩︎ If we know the exact shape of the distribution we can be more specific about the proportion of the distribution that fall within one standard deviation of the mean.↩︎ Using 0’s and 1’s for the encoding does have some advantages over other coding schemes which we will explore later when fitting statistical models.↩︎ "],["multivariate-visualization.html", "Chapter 4 Multivariate Visualization 4.1 Multivariate Distributions 4.2 Faceting 4.3 Multivariate Descriptive Statistics", " Chapter 4 Multivariate Visualization library(tidyverse) library(ggformula) library(statthink) # Add plot theme theme_set(theme_statthinking()) Real world data are never as simple as exploring a distribution of a single variable, particularly when trying to understand individual variation. In most cases attributes interact with one another, move in tandem, and many phenomena help to explain the attribute of interest. For example, when thinking about admission rates of higher education institutions, what may be some important attributes that would explain some of the reasons why higher education institutions differ in their admission rates? When thinking about the high temperature for the given day, what attributes would be helpful to understand why the high temperature on a day is different? Take a few minutes to brainstorm some ideas. Building off of the high temperature example, we will use weather data from two seasons of cooler months of the year, October through April of 2018-2019 and 2019-2020, of various locations around the United States. This data was downloaded from the National Centers for Environmental Information (NCEI) Climate Data Online portal and is part of the companion package, statthink. The locations extracted in these data are found in the northern part of the United States and include: c(“Buffalo, NY”, “Iowa City, IA”, “Chicago, IL”, “Boston, MA”, “Portland, ME”, “Minneapolis, MN”, “Duluth, MN”, “Detroit, MI”). The first few rows of the data are shown below. DT::datatable(us_weather) 4.1 Multivariate Distributions Before moving to multivariate distributions, first, we will explore a univariate distribution of the average daily temperature. The average daily temperature takes the recorded temperatures from a single day and averages those, therefore this value would fall somewhere between the high and low temperature for the day. What are some key characteristics of the average daily temperature distribution? Take a few minutes to summarize the key characteristics. gf_histogram(~ drybulbtemp_avg, data = us_weather, bins = 30) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Univariate distribution of avg daily temperature&quot;) From the univariate figure of the average daily temperature, you may have noticed that there is variation in these values, with a fairly wide range considering the values. For example, there are some average daily temperatures that are below zero and some of 60 degree Fahrenheit. However, most values are between 20 and 45 or so, which is reasonable given these are cooler months of the year and mostly northern location in the United States. Finally, the distribution is unimodal and somewhat symmetric, at least not skewed enough to be too concerned. Earlier the question was asked what other attributes may help to explain why there are variations in high temperatures. If you did not think about that question then, we encourage you to think about this question now. What attributes could help to understand why there is such a wide range of values in the average daily temperature? There are many answers that could be informative to this discussion, the one we will explore first is whether it snowed on a given day. This could influence the average temperature for a few reasons, first when it snows it must be cold enough for the precipitation to stay in frozen form rather than melting as it falls being at or below freezing (32 degrees Fahrenheit);16 also, if it snowed during the day it would also be less likely for the sun to be out further making the temperature less variable and likely lower during the winter months. 4.1.1 Histograms Below is the multivariate distribution of the average daily temperature by whether it snowed or not at some point during that day. Whether it snowed or not is depicted by color in the figure, the blue color is showing the distribution of average daily temperature for days where it snowed and red is otherwise. Before we interpret the figure in more detail, the code for making this change was done by adding the snow attribute to the fill aesthetic using the formula syntax: fill = ~ snow. The fill aesthetic is telling the histogram bars to be colored by the different categories of the attribute of interest, here snow. The fill attribute for histrograms is best used when the attribute only has a few categories. gf_histogram(~ drybulbtemp_avg, data = us_weather, bins = 30, fill = ~ snow) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, fill = &quot;Snow?&quot;) If you were to compare the distribution for the average daily temperature for days when it snowed compared to when it did not snow, what similarities and differences do you notice? Take a few minutes to try and interpret this figure, focusing on characteristics that are similar or different across days when it snowed or not. You may notice first that the bars are higher for the days that did not snow compared to days that it did snow. Why might this be occurring? If you said the reason is that it tends to snow less frequently, you would be correct. The bars are higher for days when it did not snow due to more data (i.e., days) where it did not snow. This characteristic of the histogram in this example makes the histogram more difficult to interpret and can even mislead when making comparisons across the two groups. We will explore a solution to this problem soon. Another observation you may have noticed is that it appears the temperature tends to be lower for days in which it snows compared to those that it did not snow. On average, if you estimated the location of the mean for the two groups, the average daily temperature for days where it snowed would likely be just under 30 degrees where as it would be around 45 degrees Fahrenheit for days it did not snow. You may have also noticed that the variability for days when it snowed was also lower compared to days where it did snow. Why might this occur? This is likely due to the temperature ranges where snow forms readily. You may wonder, why are there days where the average daily temperature was about 45 degrees Fahrenheit when it generally needs to be lower than 32 degrees Fahrenheit for it to snow? There could be many explanations for this, but since the distribution is showing the average daily temperature over a 24 hour period, the weather can change quite drastically over this period. For example, it could snow at the beginning or end of the day and the rest of the day could be quite different and much warmer than when it snowed. Finally, you may also notice that the distribution for days when it snowed is somewhat more left skewed compared to days in which it does not snow. As snow readily forms when it is cooler, typically less than 32 degrees Fahrenheit, the temperature would commonly fall below this value when it snows. Therefore, the average temperature would be restricted by those colder values, pulling the average lower, even if it was warmer earlier or later in the day. This restriction is not found for days when it does not snow, therefore the temperatures are able to be found over a wider range of temperature values. 4.1.2 Density Curves Often density plots are easier to visualize when there are more than one group. The other benefit of moving to a density plot is that any sample size differences across the groups are normalized automatically, which does not occur by default with the histogram. To start, we will explore the average daily temperature based on whether it snowed that day or not. For the first density curve, we will use almost the same code as the multivariate histogram. The only change in the code is to use gf_density() instead of gf_histogram(). Take a few minutes to summarize important differences in this figure compared to the multivariate histogram. gf_density(~ drybulbtemp_avg, data = us_weather, size = 1, fill = ~ snow) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, fill = &quot;Snow?&quot;) Notice that there are two density curves, each shaded a different color, light blue for days in which it snowed, and light red for days where it did not snow. One primary difference, between the density and histogram is that the curves are normalized for sample size differences, therefore differences in heights for the two density curves can be interpreted as there being more data at that location. For example, the curve for days it snowed has a higher peak, around 30 degrees Fahrenheit, compared to the peak for days it did not snow, around 45 degrees. The higher peak here means there is more data clustered around 30 degrees for the snowy days compared to the amount of data clustered around 45 degrees for the days it did not snow. The normalization of sample size across the groups helps to accentuate the key differences. Namely, there are differences in where the center and amount of variation between the two groups. When plotting two groups with a histogram, the groups are plotted over top of each other which can further mask differences in the group that is plotted first. The density curve with some transparency, as depicted above, does not suffer from these problems. There is another plotting aesthetic that is useful to know about when using density curves. This is the color aesthetic which changes the colors of the line of the density curve. Similar to the fill aesthetic, it is possible to have the lines change color based on the snow attribute as such: color = ~ snow. The code below does this and also makes the lines slightly larger to view easier with the size = 1 global aesthetic. gf_density(~ drybulbtemp_avg, data = us_weather, color = ~ snow, size = 1) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, color = &quot;Snow?&quot;) Without specifying the fill aesthetic, both groups have the same fill color below the lines, but notice that now the lines are colored instead of the entire curve being filled in. Areas that are darker gray are areas where the two groups overlap. The rest of the density curves are the same besides the appearance differences. It is also possible to combine the color and fill aesthetics, even with the same attribute. For example, the code below using the snow attribute for both the color and fill aesthetics. The primary difference here is that the lines are a bit larger compared to the first density figure shown. gf_density(~ drybulbtemp_avg, data = us_weather, color = ~ snow, size = 1, fill = ~ snow) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, fill = &quot;Snow?&quot;, color = &quot;Snow?&quot;) It is also possible to use both aesthetics, but set one to be constant rather than adding the attribute to it. When there are many groups, this can be a good way to visualize more groups and not have too much color happening. In the code below, the snow attribute is specified to the color aesthetic, but now the fill aesthetic is set to a specific color, this time a shade of gray. The grayscale ranges from 0 to 100, where 0 is black and 100 is white, therefore in this case when setting fill = 'gray75' this would be a lighter gray as the number is closer to 100. gf_density(~ drybulbtemp_avg, data = us_weather, color = ~ snow, size = 1, fill = &#39;gray75&#39;) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, color = &quot;Snow?&quot;) This results in a figure that is a bit lighter than the default gray color used in density plots. This lighter gray makes it a bit easier to see the density curves that are plotted behind. When there are more than two groups, this can be an important consideration to take into account to ensure the visualization is easier to interpret. Take the example below which instead of showing whether it snowed it shows the different locations. gf_density(~ drybulbtemp_avg, data = us_weather, color = ~ location, size = 1, fill = &#39;gray85&#39;) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, color = &quot;&quot;) Figure 4.1: Multivariate distribution of avg daily temperature by whether it snowed In this figure, the grayscale was changed from 75 to 85 as well, but you’ll notice that it is still difficult to view all of the locations. This is made more difficult here due to the locations having similar distributional centers. The next section will show an alternative visualization that can help when density curves are difficult to interpret. 4.1.3 Violin Plots Violin plots are another way to make comparisons of distributions across groups. Violin plots are also easier to interpret when there are more than two groups on a single graph. Violin plots are density plots that are mirrored to be fully enclosed. Procedurally, the density curve that was shown above is mirrored or flipped across the x-axis. Let’s explore an example that creates a violin plot with the gf_violin() function and a two-sided formula, drybulbtemp_avg ~ snow which will visualize the average daily temperature by whether it has snowed or not. gf_violin(drybulbtemp_avg ~ snow, data = us_weather) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, title = &#39;Multivariate distribution of avg daily temperature by whether it snowed&#39;, x = &quot;Snow?&quot;) By default, the violin plots are oriented vertically, where the temperature is on the y-axis and whether it snowed or not is on the x-axis. This is opposite of what we have been using with histograms and density curves. Fortunately, we can change this behavior by adding a single line of code, gf_refine(coord_flip()). This command flips the x- and y- axes to place the average daily temperature on the x-axis. Throughout the rest of the book, any violin plots shown will have the default axes flipped for consistency with the histograms and density plots. gf_violin(drybulbtemp_avg ~ snow, data = us_weather) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, title = &#39;Multivariate distribution of avg daily temperature by whether it snowed&#39;, x = &quot;Snow?&quot;) %&gt;% gf_refine(coord_flip()) Violin plots are depicted for each group separately. This means that there will be the same number of violin plots as the number of groups that are in the attribute on the right hand side of the equation specified inside gf_violin(). In the example above, there is a violin plot for days it snowed and second one for days it did not snow and these violin plots are shown separately and never stacked on top of one another. This is the feature that makes these figures able to handle a larger number of groups more efficiently compared to density of histograms. From the figure, similar findings discussed earlier can be articulated. For example, there is a higher center and more variation on days that it did not snow. There is also evidence that the distribution for days that did snow is left-skewed whereas the distribution for days it did not snow is more symmetric. Aesthetically, these figures are a bit more pleasing to look at if they include a light fill color. This is done similar to the density plots shown above with the fill = argument, specified as fill = 'gray85'. gf_violin(drybulbtemp_avg ~ snow, data = us_weather, fill = &#39;gray85&#39;) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, title = &#39;Multivariate distribution of avg daily temperature by whether it snowed&#39;, x = &quot;Snow?&quot;) %&gt;% gf_refine(coord_flip()) Percentiles are another useful feature to aid in the comparison across groups with violin plots. These can be added with the draw_quantiles argument. In the below code, three percentiles are shown, the 10th, 50th, and 90th percentiles are added with the code, draw_quantiles = c(0.1, 0.5, 0.9). Notice that the percentiles are represented as a proportion instead of as a percentage. gf_violin(drybulbtemp_avg ~ snow, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, x = &quot;Snow?&quot;) %&gt;% gf_refine(coord_flip()) Figure 4.2: Multivariate distribution of average daily temperature by whether it snowed When the percentiles are added, it is now easier to compare the center of the two distributions by using the middle vertical line in each violin plot which represents the 50th percentile or median. In this violin plot, the 50th percentile is lower for days it did snow compared to days it did not snow. Furthermore, the 90th percentile is also lower than the 50th percentile for days that it did snow. This helps to provide evidence that days that it did snow tend to be much colder than typical days, as shown by the 50th percentile, when it did not snow. Comparing the distance between the 10th and 90th percentiles for each group can also give a sense of the range of typical values. Although this isn’t the interquartile range,17 the difference between the 10th and 90th percentiles reflects a similar quantity. For days that it snowed, the 10th percentile is around 10 to 12 degrees Fahrenheit and the 90th percentile is around 37 to 40 degrees Fahrenheit. Therefore the difference would be about 25 to 30 degrees Fahrenheit. Doing the same process for days it did not snow, the 10th percentile would be around 24 degrees Fahrenheit and the 90th percentile would be around 55 degrees Fahrenheit. The difference would then be around 31 degrees Fahrenheit. This suggests that the spread of the middle 80% of the data in each group are similar, with days in which it snowed showing some evidence of being slightly less spread out over this middle 80%. 4.1.3.1 Violin Plots with many groups As discussed earlier, visualizing many groups can be done more easily using violin plots compared to density or histograms. This is shown below where the average daily temperature is shown for each of the 8 locations in the US weather data. The 10th, 50th, and 90th percentiles are shown as well for comparison. Take a minute to compare this figure to the one with overlapping density curves in Figure 4.1. gf_violin(drybulbtemp_avg ~ location, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, x = &quot;Location&quot;) %&gt;% gf_refine(coord_flip()) Figure 4.3: Average daily temperature by the location of weather measurement. 4.2 Faceting Faceting is a graphical procedure to create multiple figures side by side. This is helpful when there are more than 2 attributes to explore. For example, we have explored the distribution of average daily temperature by whether it snowed or not and also the location separately. Perhaps, there are differences in the distribution of average daily temperatures for whether it snowed or not based on the location. This multivariate visualization now considers two attributes that may help to understand why the average daily temperature may differ. Below is an example of adding the faceting to a violin plot. The violin plot will explore the average daily temperature by the location. This figure was explored initially in Figure 4.3. This new figure (Figure 4.4) adds one line of code to do the faceting, gf_facet_wrap(~ snow). You may notice that inside the gf_facet_wrap() function, a similar formula notation used by other plotting functions is used. Within this function, a one-sided formula is used where the attribute name is specified after the ~. gf_violin(drybulbtemp_avg ~ location, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, x = &quot;Location&quot;) %&gt;% gf_refine(coord_flip()) %&gt;% gf_facet_wrap(~ snow) Figure 4.4: Average daily temperature by the location of weather measurement and whether it snowed on a given day. From the figure, the primary difference between Figure 4.3 is the creation of two side-by-side plots, one representing observations when it did not snow (left-side plot) and the other representing observations when it did snow that day (right-side plot). If the two figures were combined, the same figure as in Figure 4.3 happen. The faceting is splitting the observations based on this new attribute, therefore each panel of the figure has less data compared to the overall that did not split the figure by whether it snowed or not. There are a few differences shown in the figure, primarily related to variation differences. For instance, the variation of average daily temperatures is much smaller for Detroit when it snows compared to when it does not snow. These variation differences are not as pronounced for a city like Duluth or Minneapolis. Similar to previous discussions, there does not seem to be large differences in average daily temperature across the locations. As a contrast, one instance that may have a larger impact than location, would be month of the year. Below is a faceted figure that instead of location, adds month of year. gf_violin(drybulbtemp_avg ~ month, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, x = &quot;Month&quot;) %&gt;% gf_refine(coord_flip()) %&gt;% gf_facet_wrap(~ snow) Figure 4.5: Average daily temperature by the month of weather measurement and whether it snowed on a given day. 4.3 Multivariate Descriptive Statistics We’ve spent a lot of time trying to reason about other attributes that may be important in explaining variation in our attribute of interest. We started this chapter by exploring visually other attributes that help to understand or explain variation in the primary outcome. For example, we visually explored attributes that may help us understand why average daily temperatures may differ. We found that if it snows on a given day has an impact and month has an impact, but location does not seem to have as large of an effect. In this section, we will revisit descriptive statistics, or single number summaries of key elements of a distribution. The idea of descriptive statistics will now be generalized to be multivariate in nature. Practically, this means that we will no longer use a single numeric quantity as was done in Chapter 3, instead, we will consider multiple numeric quantities for a single distribution. More accurately, we are now calculating conditional descriptive statistics for conditional distributions. For example, if you look back at Figure 4.5, we conditioned the distribution on month and whether it snowed or not. The conditioning means that the distribution of the average daily temperature was split into separate distributions for each unique combination of the month and snow attributes. In the case of descriptive statistics, we will now compute single number summaries for those conditional or separate distributions. Let’s jump in with a concrete example that computes the multivariate (conditional) descriptive statistics that we explored in Figure 4.2. Recall from Chapter 3, to compute descriptive statistics the df_stats() function is used. The primary arguments to this function are a formula indicating the attributes to consider followed by the statistics to be calculated. In Chapter 3, the formula used for the df_stats() function were all one-sided as these were univariate descriptive statistics. The formula when calculating multivariate (conditional) descriptive statistics will be two-sided of the following form primary-attribute ~ conditional-attributes. In this formula specification, the primary attribute would represent the attribute the descriptive statistics will be computed for or the attribute of interest. The conditional attribute(s) represent the attribute that the data will be conditioned or split on. Looking at the next code chunk, notice that the formula specification is drybulbtemp_avg ~ snow which means that the descriptive statistics will be computed for the average daily temperature or the primary attribute and this computation will be done for each unique value of the snow attribute. As we know, the snow attribute takes on two unique values, either it snowed or it did not snow. Therefore, the resulting output will contain two rows, one for days in which it did snow and one for when it did not snow. us_weather %&gt;% df_stats(drybulbtemp_avg ~ snow, median) ## response snow median ## 1 drybulbtemp_avg No 41 ## 2 drybulbtemp_avg Yes 29 Presented above are the conditional medians for the average daily temperature for days in which it snowed (Yes) compared to days it did not snow (No). These are the middle lines shown in Figure 4.2, but now explicitly calculated. The data were split into subgroups and the median was computed for each of those subgroups instead of pooling all observations. One thing that is useful to add in when computing conditional statistics, is how many data points are in each group. This is particularly useful when the groups are different sizes, which commonly occurs. To do this, we can add another function, length, to the df_stats() function. us_weather %&gt;% df_stats(drybulbtemp_avg ~ snow, median, length) ## response snow median length ## 1 drybulbtemp_avg No 41 2392 ## 2 drybulbtemp_avg Yes 29 1008 This adds another columns which represents the number of observations that went into the median calculation for each group and that there are many more days for these locations where it did not snow. The syntax above also shows that you can add additional functions separated by a comma in the df_stats() function and are not limited to a single function. We will take advantage of this feature later on. 4.3.1 Adding additional groups What if we thought more than one attribute was important in explaining variation in the outcome? These can also be added to the df_stats() function for additional conditional statistics. The key is to add another attribute to the right-hand side of the formula argument. Each attribute on the right-hand side of the ~ are separated with a + symbol. The following example computes the median and number of observations for the average daily temperature based on the unique values of whether it snowed by each month. us_weather %&gt;% df_stats(drybulbtemp_avg ~ snow + month, median, length) ## response snow month median length ## 1 drybulbtemp_avg No Oct 50.0 466 ## 2 drybulbtemp_avg Yes Oct 37.0 30 ## 3 drybulbtemp_avg No Nov 38.0 309 ## 4 drybulbtemp_avg Yes Nov 30.0 171 ## 5 drybulbtemp_avg No Dec 33.0 335 ## 6 drybulbtemp_avg Yes Dec 29.0 161 ## 7 drybulbtemp_avg No Jan 31.0 238 ## 8 drybulbtemp_avg Yes Jan 24.0 258 ## 9 drybulbtemp_avg No Feb 30.0 256 ## 10 drybulbtemp_avg Yes Feb 23.5 200 ## 11 drybulbtemp_avg No Mar 40.0 381 ## 12 drybulbtemp_avg Yes Mar 32.0 115 ## 13 drybulbtemp_avg No Apr 47.0 407 ## 14 drybulbtemp_avg Yes Apr 36.0 73 From the output, there are 14 rows which coincide with the number of months times 2. The 2 here represents the number of categories for the snow attribute, being if it snowed or not. In this case each month by snow combination has data, but if a combination of the attributes on the right-hand side of the equation do not have any data, those rows would not show up in the descriptive statistic summary table. These multivariate descriptive statistics coincide with Figure 4.5, specifically the middle line in these violin plots. 4.3.2 Adding more descriptive statistics Chapter 3 discussed how to add more descriptive statistics to the df_stats() function. This section serves as a reminder for how to do that. In this chapter, we have only computed the median and number of observations, these are useful statistics, but it is often useful to compute descriptive statistics of variation or other location based statistics such as percentiles or means. To do this, we can continue adding function separated by commas directly at the end of the df_stats() function. For example, we can add the mean and standard deviation to the previous multivariate descriptive statistics tables by adding , mean, sd to the end of the previous df_stats() function. us_weather %&gt;% df_stats(drybulbtemp_avg ~ snow + month, median, length, mean, sd) ## response snow month median length mean sd ## 1 drybulbtemp_avg No Oct 50.0 466 51.25751 8.739614 ## 2 drybulbtemp_avg Yes Oct 37.0 30 37.03333 4.278925 ## 3 drybulbtemp_avg No Nov 38.0 309 37.52427 9.009927 ## 4 drybulbtemp_avg Yes Nov 30.0 171 28.77778 7.512952 ## 5 drybulbtemp_avg No Dec 33.0 335 32.60000 9.318451 ## 6 drybulbtemp_avg Yes Dec 29.0 161 27.20497 8.892495 ## 7 drybulbtemp_avg No Jan 31.0 238 28.51681 13.316250 ## 8 drybulbtemp_avg Yes Jan 24.0 258 20.96899 12.657990 ## 9 drybulbtemp_avg No Feb 30.0 256 28.01172 12.269676 ## 10 drybulbtemp_avg Yes Feb 23.5 200 22.74500 9.545447 ## 11 drybulbtemp_avg No Mar 40.0 381 38.30446 10.005878 ## 12 drybulbtemp_avg Yes Mar 32.0 115 29.31304 9.812600 ## 13 drybulbtemp_avg No Apr 47.0 407 47.91646 7.817367 ## 14 drybulbtemp_avg Yes Apr 36.0 73 36.31507 4.918437 One thing to note when adding more statistics to the df_stats() function is the order with which these show up. The statistics computed show up in the order specified, therefore the statistics from the following command will be median, number of observations (length), mean, and then standard deviation. For more information on why it snows instead of rains, this is an informative description from the National Snow &amp; Ice Data Center↩︎ The interquartile range is the difference between the 25th and 75th percentiles↩︎ "],["classification.html", "Chapter 5 Classification 5.1 Topic: Decision Trees 5.2 Adding Categorical Attributes 5.3 Comparison to Baseline 5.4 Training/Test Data 5.5 Introduction to Resampling/Bootstrap", " Chapter 5 Classification Classification is a task that tries to predict group membership using the data attributes available. For example, one could try to predict if an individual is left or right handed based on their preferences to music or hobbies. In this situation, the classification technique would look for patterns in the music or hobby preferences to help differentiate between those that are left or right handed. Perhaps the data would show that left handed individuals would be more likely to be artistic, therefore those that rated more highly artistic tasks would be more likely to be classified as left handed. To perform the classification tasks in this chapter, we are going to consider a group of statistical models called decision trees, or more specifically in this case, classification trees. Statistical models are used to help us as humans understand patterns in the data and estimate uncertainty. Uncertainty comes from the variation in the data. For example, those that are left handed are likely not all interested or like artistic hobbies or tasks, but on average maybe they are more likely to enjoy these tasks compared to right handed individuals. Statistical models help us to understand if the differences shown in our sample of data are due to signal (true differences) or noise (uncertainty). In the remaining sections of this chapter, we will build off of this idea of statistical models to understanding how these work with classification trees to classify. Furthermore, we will aim to develop heuristics to understand if our statistical model is practically useful. That is, does our model help us to do our classification task above just randomly guessing. We will use a few additional packages to perform the classification tasks, including rpart, rpart.plot, and rsample. The following code chunk loads all the packages that will be used in the current chapter. library(tidyverse) library(ggformula) library(statthink) library(rpart) library(rpart.plot) library(rsample) # Add plot theme theme_set(theme_statthinking()) us_weather &lt;- mutate(us_weather, snow_factor = factor(snow), snow_numeric = ifelse(snow == &#39;Yes&#39;, 1, 0)) The book will also make use of a package on GitHub that helps to visualize the decision trees in more detail. This can be installed with the following code assuming the remotes package is already installed. remotes::install_github(&quot;grantmcdermott/parttree&quot;) The package can then be loaded with the following single line of code. library(parttree) 5.1 Topic: Decision Trees We will continue to use the United States weather data introduced in Chapter 4. Given that this data was for the winter months in the United States, the classification task we will attempt to perform is to correct predict if it will snow on a particular day that precipitation occurs. To get a sense of how often it rains vs snows in these data, we can use the count() function to do this. For the count() function, the first argument is the name of the data, followed by the attributes we wish to count the number of observations in the unique values of those attributes. count(us_weather, rain, snow) ## # A tibble: 4 × 3 ## rain snow n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 No No 1571 ## 2 No Yes 728 ## 3 Yes No 821 ## 4 Yes Yes 280 The following table counts the number of times that it rains or snows in the data. You may notice that there are days in which it does not rain or snow as shown by the row with No for both the rain and snow columns. There are also days in which it both rains and snows as shown in the row with Yes in both the rain and snow columns. Not surprisingly, a majority of the days it does not rain or snow, occurring about 46% of the time (\\(1571 / (1571 + 728 + 821 + 280) = 46.2%\\)). Using similar logic, about 8% of the days in the data have both snow and rain. 5.1.1 Fitting a Classification Tree Let’s class_tree our first classification tree to predict whether it snowed on a particular day. For this, we will use the rpart() function from the rpart package. The first argument to the rpart() function is a formula where the outcome of interest is specified to the left of the ~ and the attributes that are predictive of the outcome are specified to the right of the ~ separated with + signs. The second argument specifies the method for which we want to run the analysis, in this case we want to classify days based on the values in the data, therefore we specify method = 'class'. The final argument is the data element, in this case us_weather. Before we fit the model, what attributes do you think would be predictive of whether it will rain or snow on a particular day during the winter months? Take a few minutes to brainstorm some ideas. In this example, a handful of attributes to explore, including the average, minimum, and maximum temperature for the day. These happen to be all continuous attributes, meaning that these attributes can take many data values. The model is not limited to those types of data attributes, but that is where we will start the classification journey. Notice that the fitted model is saved to the object, class_tree. This will allow for easier interaction with the model results later. Then after fitting the model, the model is visualized using the rpart.plot() function. The primary argument to the rpart.plot() function is the fitted model object from the rpart() function, here that would be class_tree. The additional arguments passed below adjust the appearance of the visualization. class_tree &lt;- rpart(snow_factor ~ drybulbtemp_min + drybulbtemp_max, method = &#39;class&#39;, data = us_weather) rpart.plot(class_tree, roundint = FALSE, type = 3, branch = .3) Figure 5.1: Classification tree predicting whether it will snow or rain The visualization shown in Figure 5.1 produces the decision rules for the classification tree. The decision rules start from the top of the tree and proceed down the branches to the leaf nodes at the bottom that highlight the predictions. By default, the rpart() algorithm assumes that each split should go in two directions. For example, the first split occurs with the maximum temperature is less than 42 degrees Fahrenheit or greater than or equal to 42 degrees Fahrenheit. If the maximum temperature for the day is greater than or equal to 42 degrees Fahrenheit, the first split in the decision tree follows the left-most branch and proceeds to the left-most leaf node. This results in the prediction for those days as being days in which it does not snow (i.e., a category prediction of “No”). The numbers below the “No” label indicate that the probability of it snowing on a day where the maximum temperature was greater than or equal to 42 degrees Fahrenheit is 0.09 or about 9%. Furthermore, this category represents about 53% of the total number of data cases inputted. Following the right-hand split of the first decision, which occurs for days when the maximum temperature is less than 42 degrees, we come to another split. This split is again for the maximum temperature, but now the split comes at 36 degrees Fahrenheit. In this case, if the temperature is greater than or equal to 36 degrees Fahrenheit, the decision leads to the next leaf node and a prediction that it will not snow that day. For this leaf node, there is more uncertainty in the prediction, where on average the probability of it snowing would be 0.42 or about 42%. This value is less than 50%, therefore the “No” category is chosen. This occurs for about 16% of the data. For days in which the maximum temperature is less than 36 degrees Fahrenheit, the decision tree moves to the right further and comes to another split. The third split in the decision tree is for the minimum daily temperature and occurs at 23 degrees Fahrenheit. For days where the minimum temperature is greater than 23 degrees Fahrenheit (but also had a maximum temperature less than 36 degree Fahrenheit), the right-most leaf node is predicted. For these data cases, about 8% of the total data, the prediction is that it will snow (i.e., “Yes” category) and the probability of it snowing in those conditions is about 71%. Finally, if the minimum temperature is less than 23 degrees Fahrenheit (but also had a maximum temperature less than 36 degree Fahrenheit), then one last split occurs on the maximum temperature at 29 degrees Fahrenheit. This leads to the last two leaf node in the middle of Figure 5.1. One prediction states it will snow, for maximum temperature less than 29 degrees and one predicting it will not snow, for those greater than or equal to 29 degrees. Both of these leaf nodes have more uncertainty in the predictions, being close to 50% probability. Note, that the average daily temperature was included in the model fitting procedure, but was not included in the results shown in Figure 5.1. Why do you think this happened? The model results show the attributes that were helpful in making the prediction of whether it snowed or not. For this task, the model found that the maximum and minimum temperature attributes were more useful and adding the average daily temperature did not appreciably improve the predictions. For this reason, it did not show up in the decision tree. Furthermore, the attributes that are most informative in making the prediction are at the top of the decision tree. In the results shown in Figure 5.1, the maximum daily temperature was the most helpful attribute in making the snow or not prediction. The decision tree rules can also be requested in text form using the rpart.rules() function and are shown below. The rows in the output are the leaf nodes from 5.1 and the columns represent the probability of it snowing, the decision rules that are applicable, and the percentage of data found in each row. For example, for the first row, it is predicted to snow about 9% of the time when the maximum temperature for the day is greater than 42 and this occurred in 53% of the original data. Since the probability is less than 50%, the prediction would be that it would not snow on days with those characteristics. In rows where there are &amp; symbols, these separate different data attributes that are useful in the classification model. rpart.rules(class_tree, cover = TRUE) ## snow_factor cover ## 0.09 when drybulbtemp_max &gt;= 42 53% ## 0.42 when drybulbtemp_max is 36 to 42 16% ## 0.45 when drybulbtemp_max is 29 to 36 &amp; drybulbtemp_min &lt; 23 9% ## 0.63 when drybulbtemp_max &lt; 29 &amp; drybulbtemp_min &lt; 23 14% ## 0.71 when drybulbtemp_max &lt; 36 &amp; drybulbtemp_min &gt;= 23 8% 5.1.1.1 Visualizing Results To get another view of what the classification model is doing in this scenario, we will visualize the study results. First, the gf_point() function is used to create a scatterplot where the maximum temperature is shown on the x-axis and the minimum temperature is shown on the y-axis, shown in Figure 5.2. There is a positive relationship between maximum and minimum temperatures and on days with lower maximum temperatures are where it tends to snow. However, there is not perfect separation, meaning that there are days that have similar minimum and maximum temperatures where it does snow and other where it does not snow. temperature_scatter &lt;- gf_point(drybulbtemp_min ~ drybulbtemp_max, color = ~ snow_factor, alpha = .75, data = us_weather) %&gt;% gf_labs(x = &quot;Maximum Temperature (in F)&quot;, y = &quot;Minimum Temperature (in F)&quot;, color = &quot;Snow?&quot;) temperature_scatter Figure 5.2: Scatterplot of the minimum and maximum daily temperatures and if it snows or not The next figure will make use of the parttree R package to visualize what the classification model is doing. The geom_parttree() function is used where the primary argument is the saved classification model object that was save earlier, named class_tree. The other two arguments to add are the fill aesthetic that is the outcome of the classification tree and to control how transparent the backgroud fill color is. In this example, this is set using alpha = .25 where the transparency is set at 75% (i.e., 1 - 0.25 = 0.75). Setting a higher alpha value would reduce the amount of transparency, whereas setting a smaller value would increase the transparency. Figure 5.3 gives a sense as to what the classification model is doing to the data. The classification model breaks the data into quadrants and makes a single uniform prediction for those quadrants. For example, the areas of the figure that are shaded as red are days in which the model predicts it will not snow whereas the blue/green color are days in which the model predicts it will snow. The data points are the real data cases, therefore there are instances inside each of the quadrants in which the model did not correctly predict or classify the case. Each of the quadrants in the figure represent different leaf nodes shown in 5.1 and each represent a different likelihood of it snowing. temperature_scatter + geom_parttree(data = class_tree, aes(fill = snow_factor), alpha = .25) + scale_fill_discrete(&quot;Snow?&quot;) Figure 5.3: Showing the predictions based on the classification tree with the raw data 5.1.2 Accuracy Evaluating the model accuracy helps to understand how well the model performed the classification. If you recall, the classification model is making a prediction about whether it is going to snow on a given day based on the observed data where it was recorded if it snowed that day or not. Therefore, the data has for each day if it snowed or not. With this information, how could we evaluate how well the model performed in classifying whether it snows on a given day? To do this, the observation of whether it snowed or not can be compared to the model prediction of whether it snowed or not. Better classification accuracy would occur when the observed snow or no snow attribute is the same as the model prediction of snow or not. That is, when the same category is predicted as what is observed, this would result in better classification accuracy, a good thing. If there are cases where different categories between the observed and predicted categories or classes, this would be an example of poor classification accuracy. In the data so far, there is the observed data value on whether it snowed or not, this is the attribute that was used to fit the classification model, named snow_factor. To add the predicted classes based on the classification model shown in Figure 5.1, the predict() function can be used. To use the predict() function, the primary argument is a model object, in this case the classification model object named class_tree. To get the predicted classes, that is the leaf nodes at the bottom of Figure 5.1, a second argument is needed, type = 'class' which tells the predict function to report the top line of the leaf nodes in Figure 5.1. These predicted classes are saved into a new attribute named snow_predict. Another element is also added that represent the probability of a particular day not snowing or snowing, these are reported in the columns No and Yes in the resulting output. us_weather_predict &lt;- us_weather %&gt;% mutate(snow_predict = predict(class_tree, type = &#39;class&#39;)) %&gt;% cbind(predict(class_tree, type = &#39;prob&#39;)) head(us_weather_predict, n = 20) ## station date dewpoint_avg drybulbtemp_avg ## 1 72528014733 2018-10-01 23:59:00 51 52 ## 2 72528014733 2018-10-02 23:59:00 59 60 ## 3 72528014733 2018-10-03 23:59:00 55 62 ## 4 72528014733 2018-10-04 23:59:00 56 60 ## 5 72528014733 2018-10-05 23:59:00 43 51 ## 6 72528014733 2018-10-06 23:59:00 62 63 ## 7 72528014733 2018-10-07 23:59:00 58 60 ## 8 72528014733 2018-10-08 23:59:00 61 68 ## 9 72528014733 2018-10-09 23:59:00 66 77 ## 10 72528014733 2018-10-10 23:59:00 64 74 ## 11 72528014733 2018-10-11 23:59:00 56 62 ## 12 72528014733 2018-10-12 23:59:00 36 47 ## 13 72528014733 2018-10-13 23:59:00 36 46 ## 14 72528014733 2018-10-14 23:59:00 39 51 ## 15 72528014733 2018-10-15 23:59:00 43 49 ## 16 72528014733 2018-10-16 23:59:00 32 45 ## 17 72528014733 2018-10-17 23:59:00 34 45 ## 18 72528014733 2018-10-18 23:59:00 30 40 ## 19 72528014733 2018-10-19 23:59:00 38 50 ## 20 72528014733 2018-10-20 23:59:00 42 48 ## relativehumidity_avg sealevelpressure_avg stationpressure_avg ## 1 95 30.26 29.50 ## 2 96 30.01 29.26 ## 3 86 30.05 29.31 ## 4 77 29.97 29.18 ## 5 75 30.17 29.41 ## 6 90 30.03 29.28 ## 7 97 30.24 29.44 ## 8 84 30.23 29.49 ## 9 72 30.13 29.39 ## 10 70 29.89 29.18 ## 11 77 29.66 28.91 ## 12 66 29.82 29.05 ## 13 74 29.95 29.15 ## 14 69 30.12 29.34 ## 15 79 29.94 29.16 ## 16 61 30.06 29.31 ## 17 66 30.02 29.21 ## 18 68 30.37 29.59 ## 19 63 30.00 29.28 ## 20 86 29.68 28.90 ## wetbulbtemp_avg windspeed_avg cooling_degree_days ## 1 51 10.9 0 ## 2 60 8.5 0 ## 3 57 5.5 0 ## 4 59 12.5 0 ## 5 47 9.6 0 ## 6 63 8.1 0 ## 7 58 9.4 0 ## 8 63 7.9 3 ## 9 69 11.4 12 ## 10 68 10.6 9 ## 11 59 15.7 0 ## 12 42 12.5 0 ## 13 41 8.4 0 ## 14 45 6.5 0 ## 15 47 12.8 0 ## 16 39 15.8 0 ## 17 40 15.3 0 ## 18 36 11.2 0 ## 19 45 18.0 0 ## 20 45 12.3 0 ## departure_from_normal_temperature heating_degree_days drybulbtemp_max ## 1 -4.6 13 54 ## 2 3.8 5 69 ## 3 6.2 3 70 ## 4 4.6 5 74 ## 5 -4.0 14 58 ## 6 8.4 2 74 ## 7 5.7 5 67 ## 8 14.1 0 82 ## 9 23.5 0 83 ## 10 20.9 0 81 ## 11 9.2 3 74 ## 12 -5.4 18 51 ## 13 -6.1 19 51 ## 14 -0.7 14 60 ## 15 -2.4 16 58 ## 16 -6.0 20 52 ## 17 -5.7 20 53 ## 18 -10.4 25 47 ## 19 0.0 15 57 ## 20 -1.7 17 55 ## drybulbtemp_min peak_wind_direction peak_wind_speed precipitation snow_depth ## 1 50 50 24 0.090 0 ## 2 51 320 24 1.000 0 ## 3 53 210 25 0.005 0 ## 4 46 220 39 0.450 0 ## 5 44 100 21 0.000 0 ## 6 51 250 26 0.730 0 ## 7 53 50 21 0.020 0 ## 8 53 70 20 0.010 0 ## 9 70 210 30 0.000 0 ## 10 67 190 25 0.005 0 ## 11 50 220 39 0.010 0 ## 12 42 280 27 0.010 0 ## 13 40 250 24 0.140 0 ## 14 41 250 17 0.000 0 ## 15 40 220 37 0.090 0 ## 16 38 210 40 0.005 0 ## 17 36 290 36 0.050 0 ## 18 33 250 28 0.030 0 ## 19 43 210 48 0.005 0 ## 20 40 220 49 0.470 0 ## snowfall wind_direction wind_speed weather_occurances sunrise sunset month ## 1 0.000 60 20 RA DZ BR 612 1757 Oct ## 2 0.000 320 21 RA DZ BR 613 1755 Oct ## 3 0.000 200 20 DZ BR 614 1753 Oct ## 4 0.000 220 32 TS RA BR 615 1751 Oct ## 5 0.000 70 16 &lt;NA&gt; 616 1750 Oct ## 6 0.000 200 20 TS RA BR 618 1748 Oct ## 7 0.000 60 16 RA DZ BR 619 1746 Oct ## 8 0.000 70 16 RA 620 1744 Oct ## 9 0.000 210 23 &lt;NA&gt; 621 1743 Oct ## 10 0.000 190 21 &lt;NA&gt; 622 1741 Oct ## 11 0.000 240 29 RA 623 1739 Oct ## 12 0.000 260 21 RA 625 1738 Oct ## 13 0.000 240 18 RA BR 626 1736 Oct ## 14 0.000 250 14 &lt;NA&gt; 627 1734 Oct ## 15 0.000 290 28 RA BR 628 1733 Oct ## 16 0.000 220 30 RA 629 1731 Oct ## 17 0.005 290 28 GR RA SN 631 1729 Oct ## 18 0.005 240 21 RA SN HZ 632 1728 Oct ## 19 0.000 240 35 RA 633 1726 Oct ## 20 0.100 240 36 TS GR RA BR PL 634 1725 Oct ## month_numeric year day winter_group location fog mist drizzle rain snow ## 1 10 2018 1 18_19 Buffalo, NY No Yes Yes Yes No ## 2 10 2018 2 18_19 Buffalo, NY No Yes Yes Yes No ## 3 10 2018 3 18_19 Buffalo, NY No Yes Yes No No ## 4 10 2018 4 18_19 Buffalo, NY No Yes No Yes No ## 5 10 2018 5 18_19 Buffalo, NY No No No No No ## 6 10 2018 6 18_19 Buffalo, NY No Yes No Yes No ## 7 10 2018 7 18_19 Buffalo, NY No Yes Yes Yes No ## 8 10 2018 8 18_19 Buffalo, NY No No No Yes No ## 9 10 2018 9 18_19 Buffalo, NY No No No No No ## 10 10 2018 10 18_19 Buffalo, NY No No No No No ## 11 10 2018 11 18_19 Buffalo, NY No No No Yes No ## 12 10 2018 12 18_19 Buffalo, NY No No No Yes No ## 13 10 2018 13 18_19 Buffalo, NY No Yes No Yes No ## 14 10 2018 14 18_19 Buffalo, NY No No No No No ## 15 10 2018 15 18_19 Buffalo, NY No Yes No Yes No ## 16 10 2018 16 18_19 Buffalo, NY No No No Yes No ## 17 10 2018 17 18_19 Buffalo, NY No No No Yes Yes ## 18 10 2018 18 18_19 Buffalo, NY No No No Yes Yes ## 19 10 2018 19 18_19 Buffalo, NY No No No Yes No ## 20 10 2018 20 18_19 Buffalo, NY No Yes No Yes No ## snow_factor snow_numeric snow_predict No Yes ## 1 No 0 No 0.9130676 0.08693245 ## 2 No 0 No 0.9130676 0.08693245 ## 3 No 0 No 0.9130676 0.08693245 ## 4 No 0 No 0.9130676 0.08693245 ## 5 No 0 No 0.9130676 0.08693245 ## 6 No 0 No 0.9130676 0.08693245 ## 7 No 0 No 0.9130676 0.08693245 ## 8 No 0 No 0.9130676 0.08693245 ## 9 No 0 No 0.9130676 0.08693245 ## 10 No 0 No 0.9130676 0.08693245 ## 11 No 0 No 0.9130676 0.08693245 ## 12 No 0 No 0.9130676 0.08693245 ## 13 No 0 No 0.9130676 0.08693245 ## 14 No 0 No 0.9130676 0.08693245 ## 15 No 0 No 0.9130676 0.08693245 ## 16 No 0 No 0.9130676 0.08693245 ## 17 Yes 1 No 0.9130676 0.08693245 ## 18 Yes 1 No 0.9130676 0.08693245 ## 19 No 0 No 0.9130676 0.08693245 ## 20 No 0 No 0.9130676 0.08693245 The first 20 rows of the resulting data are shown. Notice that for all of these 20 rows, the predicted class, shown in the attribute, snow_predict, are represented as “No” indicating that these days it was not predicted to snow. Notice toward the bottom however, that there were two days in which it did in fact snow, shown in the column named, snow_factor. These would represent two cases of misclassification as the observed data is not the same as the model predicted class. Finally, the probabilities shown in the last two attribute columns are all the same here. These are all the same as the maximum dry bulb temperature was greater than 42 degrees Fahrenheit in all of these days. Therefore, all 20 of the cases shown in the data here represent the left-most leaf node shown in Figure 5.1. Now that the observed data and the model predicted classes are in the data, it is possible to produce a table that shows how many observations were correctly predicted (indicating better model accuracy). To do this, the count() function can be used where the observed and predicted class attributes are passed as arguments. us_weather_predict %&gt;% count(snow_factor, snow_predict) ## snow_factor snow_predict n ## 1 No No 2147 ## 2 No Yes 245 ## 3 Yes No 529 ## 4 Yes Yes 479 The resulting table shows the observed data values in the left-most column (snow_factor) followed by the predicted class (snow_predict) in the middle column. The final column represents the number of rows or observations that were in each combination of the first two columns. For example, the first row shows that 2,147 observations were counted that had the combination where it was observed and predicted to have not snowed that day. These 2,147 observations would be instances of correct classification. The second row shows that 245 observations occurred where it was observed to not have snowed, but the model predicted it would snow that day. All of the 245 observations were misclassified based on the classification model. From this table, the overall model accuracy can be calculated by summing up the cases that matched and dividing by the total number of observations. This computation would look like: \\[ accuracy = \\frac{\\textrm{matching predictions}}{\\textrm{total observations}} = \\frac{(2147 + 479)}{(2147 + 245 + 529 + 479)} = .772 = 77.2% \\] This means that the overall classification accuracy for this example was just over 77%, meaning that about 77% of days the model was able to correctly classify whether it snowed or not. This computation can also be done programmatically. To do this, a new attribute named, same_class, can be added to the data that is given a value of 1 if the observed data matches the predicted class and a value of 0 otherwise. Descriptive statistics, such as the mean and sum, can be computed on this new vector to represent the accuracy as a proportion and the number of matching predictions (the numerator shown in the equation above). us_weather_predict %&gt;% mutate(same_class = ifelse(snow_factor == snow_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) ## response mean sum ## 1 same_class 0.7723529 2626 Notice that the same model accuracy was found, about 77.2%, and the number of observations (i.e., days) that the correct classification was found was 2,626 days. Is correctly predicting 77.2% of the days good? That is, would you say this model is doing a good job at accurately predicting if it will snow or not on that day? 5.1.2.1 Conditional Accuracy One potential misleading element of simply computing the overall model accuracy as done above, is that the accuracy will likely differ based on the which class. This could occur for a few reasons, one it could be more difficult to predict one of the classes due to similarities in data across the two classes. The two classes are also often unbalanced, therefore exploring the overall model accuracy will give more weight to the group/class that has more data. In addition, this group has more data so it could make it a bit easier for the model to predict, these issues could be exacerbated in small sample conditions. Therefore, similar to earlier discussion in the book about multivariate distributions, it is often important to consider conditional or multivariate accuracy instead of the overall model accuracy. Let’s explore this a different way than simply computing a percentage, instead we could use a bar graph to explore the model accuracy. Figure 5.4 shows the number of correct classifications for the two observed data classes (i.e., snow or did not snow) on the x-axis by the predicted classes shown with the fill color in the bars. The fill color are red for days that the model predicts it will not show and green/blue for days in which it will not snow. Therefore, accuracy would be represented in the left-bar by the red portion of the bar and the right-bar by the green/blue portion of the bar. gf_bar(~ snow_factor, fill = ~snow_predict, data = us_weather_predict) %&gt;% gf_labs(x = &quot;Observed Snow Status&quot;, fill = &quot;Predicted Snow Status&quot;) Figure 5.4: A bar graph showing the conditional prediction accuracy represented as counts. Figure 5.4 is not a very good picture to depict accuracy as the two groups have different numbers of observations so comparisons between the bars is difficult. Secondly, the count metric makes it difficult to estimate how many are in each group, for example, it is difficult from the figure alone to know how many were incorrectly classified in the left-most bar represented by the blue/green color. These issues can be fixed by adding an additional argument, position = 'fill' which will scale each bar as a proportion, ranging from 0 to 1. The bar graph is now scaling each bar based on the sample size to normalize sample size differences. gf_bar(~ snow_factor, fill = ~snow_predict, data = us_weather_predict, position = &quot;fill&quot;) %&gt;% gf_labs(x = &quot;Observed Snow Status&quot;, fill = &quot;Predicted Snow Status&quot;, y = &#39;Proportion&#39;) %&gt;% gf_refine(scale_y_continuous(breaks = seq(0, 1, .1))) Figure 5.5: A bar graph showing the conditional prediction accuracy represented as a proportion. From this new figure (Figure 5.5), it is much easier to estimate the prediction accuracy from the figure. For example, the green/blue portion of the left-most bar is at about 0.10, meaning that about 10% of the cases are misclassified and 90% would be correctly classified. Therefore the classification accuracy for days in which it did not snow would be about 90%. Compare this to days in which it did not snow (the right bar), where the prediction accuracy represented by the green/blue color is about 48%, meaning that the misclassification rate is about 52%. Let’s recalibrate how we think the model is doing? If you were just given the overall classification rate of about 77%, how did you feel about the model? Now that we know the model accurate predicts it won’t snow about 90% of the time, but can only identify that it will snow about 48% of the time, how well do you feel the model is performing now? Would you feel comfortable using this model in the real world? One last note, we can also compute the conditional model accuracy more directly using the df_stats() function as was done for the overall model accuracy. The primary difference in the code is to specify the same_class attribute to the left of the ~. This represent the attribute to compute the statistics of interest with. Another attribute is added to the right of the ~ to represent the attribute to condition on, in this case the observed data point of whether it snowed or not. us_weather_predict %&gt;% mutate(same_class = ifelse(snow_factor == snow_predict, 1, 0)) %&gt;% df_stats(same_class ~ snow_factor, mean, sum, length) ## response snow_factor mean sum length ## 1 same_class No 0.8975753 2147 2392 ## 2 same_class Yes 0.4751984 479 1008 The output returns the conditional model accuracy as a proportion, the number of correct classifications for each class/group, and the total number of observations (both correct and incorrect classifications) for each class/group. The estimated values we had from the figure were very close to the actual calculated values, but we find the figure to be more engaging than just the statistics. 5.2 Adding Categorical Attributes Up to now, the classification models have only used continuous attributes, that is, those that take on many different data values rather than a few specific values that represent categories or groups. Classification models do not need to be limited to only continuous attributes to predict the binary outcome. The model may also add some additional predictive power and accuracy with the inclusion of more attributes. For example, one attribute that may be helpful in this context to predict whether it will snow on a given day could be the month of year. The data used here are from the fall, winter, and spring seasons, therefore it is likely true that it would be more likely to snow in the winter months rather than fall or spring. Prior to including in the model, this can be visualized. Figure 5.6 shows a bar chart that represents the proportion of days in which it tends to snow the most. You may notice that it does snow on some days in every month, however, the frequency is much higher in January and February. As the proportion of days that it snows during different months of the year, this attribute could be helpful in predicting days in which it snows. gf_bar(~ month, fill = ~ snow, data = us_weather, position = &#39;fill&#39;) %&gt;% gf_labs(x = &quot;&quot;, y = &quot;Proportion of days with snow&quot;, fill = &quot;Snow?&quot;) Figure 5.6: Bar chart showing the proportion of days that is snows across months To add this attribute to the classification model, the model formula is extended to the right of the ~ to add + month. Note, this model is saved to the model object, class_tree_month, and the remaining code is the same as the previous model fitted with the two continuous attributes. class_tree_month &lt;- rpart(snow_factor ~ drybulbtemp_min + drybulbtemp_max + month, method = &#39;class&#39;, data = us_weather) rpart.plot(class_tree_month, roundint = FALSE, type = 3, branch = .3) Figure 5.7: The classification model with the month attribute added as a categorical attribute. The fitted model classification results are shown in Figure 5.7. Notice that month does not show up in the figure of the classification tree and Figure 5.7 is identical to Figure 5.1. Why is this happening and what does this mean? To understand what is happening, we need to think about temperature and how that may be related to the month of the year. In the United States, particularly in northern locations as those with the data used here, the temperature varies quite substantially by the month of the year. Figure 5.8 shows the maximum daily temperature by the months of the year. Notice there is overlap in adjacent months, but the median for many of the months of the year are quite different from other months. This suggests that temperature can serve as a proxy for month and contain overlapping information. Not shown here, but a similar trend would likely be shown for the minimum daily temperature. gf_violin(drybulbtemp_max ~ month, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&gt;% gf_labs(x = &quot;&quot;, y = &quot;Maximum daily temperature (in Fahrenheit)&quot;) %&gt;% gf_refine(coord_flip()) Figure 5.8: Violin plots of the maximum temperature by month of the year For this reason, month does not show up in the model because the classification model determined that the maximum and minimum daily temperatures were more useful at predicting whether it will snow on a particular day. This highlights an important point with these classification models, only the attributes that are the most helpful in predicting the outcome will show up in the final classification model. If an attribute is not helpful, it will not show up in the classification results. As such, the model that included month had the same classification tree and will result in the same predictions as the model fitted without month in it. With the same predictions, the same model accuracy will also be obtained. 5.2.1 Exploring Location Another categorical attribute that is in the data is the location of the weather observation. These locations are across a variety of geographic locations within the northern part of the United States, including areas close to the Canadian border, others are in the northeast portion of the United States as well. Thus, location could be important, and maybe this would include additional information over and above that of just temperature. Figure 5.9 shows a bar chart that explores if some locations are more likely to have snow. The figure shows that Buffalo, Duluth, and Minneapolis all tend to have more days where it snows. gf_bar(~ location, fill = ~ snow, data = us_weather, position = &#39;fill&#39;) %&gt;% gf_labs(x = &quot;&quot;, y = &quot;Proportion of days with snow&quot;, fill = &quot;Snow?&quot;) Figure 5.9: Bar chart showing the proportion of days that is snows across locations We can also explore if the temperature is located to the location to determine if there is overlapping information in these. Recall, Buffalo, Duluth, and Minneapolis had evidence of higher days when it snowed, but notice from Figure 5.10, that Duluth has a lower temperature than the rest, but Minneapolis and Buffalo have similar maximum daily temperatures to the other locations. The location may carry some additional information that would be informative to the classification model. gf_violin(drybulbtemp_max ~ location, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&gt;% gf_labs(x = &quot;&quot;, y = &quot;Maximum daily temperature (in Fahrenheit)&quot;) %&gt;% gf_refine(coord_flip()) Figure 5.10: Violin plots of the maximum temperature by location The location attribute is added similarly to the model as month was. Note, the month attribute was removed as it was not deemed to be a useful attribute to help understand if it snows on a given day. The model results are shown in Figure 5.11. Notice that now the maximum daily temperature is still included at two separate locations, but now instead of the minimum daily temperature, location is now more important. As such, the minimum daily temperature is not located in the classification tree. class_tree_location &lt;- rpart(snow_factor ~ drybulbtemp_min + drybulbtemp_max + location, method = &#39;class&#39;, data = us_weather) rpart.plot(class_tree_location, roundint = FALSE, type = 3, branch = .3) Figure 5.11: The classification model with the location attribute added as a categorical attribute. When there is a split for a categorical attribute, the category split does not occur at a value like with continuous attributes, rather the split occurs for one or more categories. For example, for the second split in the latest results, if the location is Buffalo, Detroit, or Duluth, then the path progresses to the right to the “Yes” category representing a prediction of it will snow. The other locations continue to the left of that split, then maximum daily temperature is useful again, and then finally the final split is the location again. Now, the location attribute does not contain Buffalo, Detroit, or Duluth, but contains the other groups. In this case, those locations of Chicago and Minneapolis result in predictions that it will snow and other locations that it will not snow. 5.2.1.1 Accuracy As the attributes used in the model differ, the accuracy could be different. Therefore, it is important to explore model accuracy again. Figure 5.12 shows the classification results. These results can be compared to Figure 5.5. There are two major differences, one, the prediction accuracy for days in which it did not snow decreased slightly (left bar), but the prediction accuracy for days where it did snow the accuracy increased (right bar). us_weather_predict &lt;- us_weather_predict %&gt;% mutate(snow_predict_location = predict(class_tree_location, type = &#39;class&#39;)) gf_bar(~ snow_factor, fill = ~snow_predict_location, data = us_weather_predict, position = &#39;fill&#39;) %&gt;% gf_labs(x = &quot;Observed Snow Status&quot;, fill = &quot;Predicted Snow Status&quot;) %&gt;% gf_refine(scale_y_continuous(breaks = seq(0, 1, .1))) Figure 5.12: Bar chart showing model accuracy with location attribute included. The prediction accuracy can also be computed analytically with the df_stats() function. us_weather_predict %&gt;% mutate(same_class = ifelse(snow_factor == snow_predict, 1, 0), same_class_location = ifelse(snow_factor == snow_predict_location, 1, 0)) %&gt;% df_stats(same_class_location ~ snow_factor, mean, sum, length) ## response snow_factor mean sum length ## 1 same_class_location No 0.8599498 2057 2392 ## 2 same_class_location Yes 0.6220238 627 1008 5.3 Comparison to Baseline Overall model accuracy or conditional model accuracy as shown above, can be an important first step to evaluate how well the classification model is performing. However, it is useful to compare the model accuracy to a baseline. This is important to consider as it is often not the case that the event the model is predicting occurs equally likely, more specifically has a probability of occurring equal to 50%. For example, in the US weather data, the observed number of days in which it snows was 0.3, meaning that on average for the days and locations in the data, it snowed on 3 out of 10 days. This would mean that a naive model that only predicted it did not snow would be correct 70% of the time. Therefore, the overall accuracy of the classification models can use this information to see if the model outperforms a simple analysis that simply predicts the predominate category (in this case that it does not snow). The first model fitted to the US weather data used the drybulb maximum and minimum temperature and had an overall model accuracy of around 77%. Therefore, using the two temperature attributes increased the overall model accuracy about 7%, \\(77% - 70% = 7%\\), compared to the naive/baseline prediction of not snowing for every day. Is this 7% improved accuracy useful in this situation? In general, likely yes, but this answer is more complicated after looking at the conditional accuracy presented for the model using the two temperature attributes (see Figure 5.5). For example, the model does a good job of accurately predicting days in which it won’t snow (about 90% accurate), but not days in which it does snow (less than 50% accurate). When thinking about this situation compared to the baseline model that would predict that all observations would not snow, this model would correctly predict all of the days in which it would not snow (left-bar in Figure 5.5), but would not predict any days correctly in which it did snow (right-bar in Figure 5.5). Arguably, accurately predicting the days in which it snows is the more interesting/useful characteristic, therefore this model going from a 0% prediction accuracy for the baseline model to almost 50% is a sizable increase in prediction accuracy. This same idea could be taken with the updated model that includes location in addition to the two temperature attributes (see Figure ??). The prediction accuracy is somewhat worse for days in which it does not snow, but about a 12% increase in accuracy compared to the model with just temperature. Compared to the baseline model there would be a 62% increase in accuracy for days it does snows. This shows that the idea of the baseline comparison can come from the naive model, that is predicting a single category for all attributes, but the baseline comparison can be a previous model to compare how the model improves with the inclusion of even more attributes. 5.3.0.1 Further conditional accuracy Not explored earlier, but the model uses location in the prediction, therefore it would also be possible to condition the accuracy based on the location to understand if the model is performing better for certain locations than others. Figure ?? shows a bar chart on model accuracy for each location separately. This could be compared directly with Figure ?? to see if there are differences in the prediction accuracy by different locations. gf_bar(~ snow_factor, fill = ~snow_predict_location, data = us_weather_predict, position = &#39;fill&#39;) %&gt;% gf_labs(x = &quot;Observed Snow Status&quot;, fill = &quot;Predicted Snow Status&quot;) %&gt;% gf_refine(scale_y_continuous(breaks = seq(0, 1, .2))) %&gt;% gf_facet_wrap(~ location) Figure 5.13: Bar chart showing model accuracy with location attribute included and facetted by the different locations. You can really see a lot of variation in the prediction accuracy across locations from this model. For example, Boston, Iowa City, and Portland always predict that it won’t snow (i.e., a 0% prediction accuracy for days it does snow). In contrast, for Buffalo, Detroit, and Duluth, the prediction accuracy for days in which it snows is above 80%. Furthermore, Duluth has the worst prediction accuracy for days in which it does not snow, slightly larger than 50%. If Boston, Iowa City, and Portland were removed from the data to make the predictions, the prediction accuracy for days in which it snows would increase, but the prediction accuracy for days in which it does not snow would likely decrease. 5.3.1 Absolute vs Relative Comparison To come … 5.4 Training/Test Data So far we have used the entire data to make our classification. This is not best practice and we will explore this is a bit more detail. First, take a minute to hypothesize why using the entire data to make our classification prediction may not be the best? It is common to split the data prior to fitting a classification/prediction model into a training data set in which the model makes a series of predictions on the data, learns which data attributes are the most important, etc. Then, upon successfully identifying a useful model with the training data, test these model predictions on data that the model has not seen before. This is particularly important as the algorithms to make the predictions are very good at understanding and exploiting small differences in the data used to fit the model. Therefore, exploring the extent to which the model does a good job on data the model has not seen is a better test to the utility of the model. We will explore in more detail the impact of not using the training/test data split later, but first, let’s refit the classification tree to the weather data by splitting the data into 80% training and 20% test data. A 70/30 split is also common in practice, so why choose an 80/20 training/test data split. This choice can be broken down into two primary arguments to consider when making this decision. The main idea behind the making the test data smaller is so that the model has more data to train on initially to understand the attributes from the data. However, as discussed above, it is helpful to evaluate the model on data the model has not seen before. This helps to ensure the process is similar to that of what may happen in the real world where the model could be used to help predict/classify cases that happen in the future. Secondly, the test data does not need to be quite as large, but we would like it to be representative of the population of interest. In larger samples, the splitting may not make big differences, but in small samples more care choosing the appropriate split percentages is a helpful step. Here, the data are not small, but also not extremely large, about 3400 weather instances. When using a 80/20 training/test data split, there would be 2720 in the training data and 680 in the testing data. 5.4.1 Splitting the data into training/test This is done with the rsample package utilizing three functions, initial_split(), training(), and test(). The initial_split() function helps to take the initial random sample and the proportion of data to use for the training data is initially identified. The random sample is done without replacement meaning that the data are randomly selected, but can not show up in the data more than once. Then, after using the initial_split() function, the training() and test() functions are used to obtain the training and test data respectively. It is good practice to use the set.seed() function to save the seed that was used as this is a random process. Without using the set.seed() function, the same split of data would likely not be able to be recreated in the code was ran again. Let’s do the data splitting. set.seed(2021) us_weather_split &lt;- initial_split(us_weather, prop = .8) us_weather_train &lt;- training(us_weather_split) us_weather_test &lt;- testing(us_weather_split) class_tree &lt;- rpart(snow_factor ~ drybulbtemp_min + drybulbtemp_max + location, method = &#39;class&#39;, data = us_weather_train) rpart.plot(class_tree, roundint = FALSE, type = 3, branch = .3) This seems like a reasonable model. Let’s check the model accuracy. us_weather_predict &lt;- us_weather_train %&gt;% mutate(snow_predict = predict(class_tree, type = &#39;class&#39;)) us_weather_predict %&gt;% mutate(same_class = ifelse(snow_factor == snow_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) ## response mean sum ## 1 same_class 0.79375 2159 The model accuracy on the training data, the same data that was used to fit the model was about 80%. We will now use the testing data to compare to the 80% accuracy obtained from the training data. Make a prediction as to what you think the prediction accuracy may be for the testing data, will it be higher, the same, or lower compared to the accuracy for the training data? 5.4.2 Evaluate accuracy on the testing data To evaluate the accuracy on the test data, the primary difference in the code is to pass a data object to the argument, newdata. The new data object is the testing data that was not used to fit the data and was withheld for the sole purpose of testing the performance of the classification model. In this example, the code to input the new data is, newdata = us_weather_test. us_weather_predict_test &lt;- us_weather_test %&gt;% mutate(snow_predict = predict(class_tree, newdata = us_weather_test, type = &#39;class&#39;)) us_weather_predict_test %&gt;% mutate(same_class = ifelse(snow_factor == snow_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) ## response mean sum ## 1 same_class 0.7911765 538 For the test data, prediction accuracy was quite a bit lower, about 76.5%. The reason the accuracy tends to be lower when evaluating accuracy on the testing data is that the model has not seen these data, therefore, there could be new patterns in these data that the model has not seen before. Although the accuracy is lower, it would be more realistic to how these models would be used in practice. Commonly, the model is fitted on data on hand and is used to predict future cases. The training/test data setup is more realistic to this scenario in practice where the testing data mimic those future cases that the model would ultimately be used to predict. 5.5 Introduction to Resampling/Bootstrap To explore these ideas in more detail, it will be helpful to use a statistical technique called resampling or the bootstrap. These ideas will be used a lot going forward to explore inference and understand sampling variation. In very simple terminology, resampling or the bootstrap can help to understand uncertainty in model estimates and also allow more flexibility in the statistics that are able to be used. The main drawback of resampling and bootstrap methods is that they can be computationally heavy, therefore depending on the situation, more time is needed to come to the conclusion desired. Resampling and bootstrap methods use the sample data that are already collected and perform the sampling procedure again. This makes the assumption that the sample data are representative of the population, an assumption that is also made for classical statistical inference. Generating the new samples is done with replacement (more on this later) and is done many times (5000, 10,000, etc.) with more in general being better. As an example with the US weather data, assume this is the population of interest, and resample from this population 10,000 times (with replacement). For each resampled data, calculate the proportion of days that it snows. Before this computation is ran, think about the following questions. Would you expect the proportion of days that it snowed to be the same in each new sample? Why or why not? Sampling with replacement keeps coming up, what do you think this means? Hypothesize why sampling with replacement would be a good idea? Let’s try the resampling with the calculation of the proportion of days that it snowed. These 10,000 calculations will be saved and visualized. To summarize what the resampling procedure does here, Resample the data, with replacement (assuming the population is representative) Compute and save a statistic of interest (e.g., mean) Repeat steps 1 and 2 many times (i.e., 5000, 10,000, or more) Explore the distribution of the statistic of interest resample_weather &lt;- function(...) { us_weather %&gt;% sample_n(nrow(us_weather), replace = TRUE) %&gt;% df_stats(~ snow_numeric, mean) } snow_prop &lt;- map_dfr(1:10000, resample_weather) gf_density(~ mean, data = snow_prop) %&gt;% gf_labs(x = &#39;Proportion of snow days&#39;) Figure 5.14: Density figure of the distribution of the proportion of days in which is snows from the resampled data. One can see from the figure that there is variation in the proportion of days that have snowfall. It is centered just below 0.3, shown by the peak of the density curve. This means that on average, there are just under 30% of days in which snow fell across all of the resampled data. The variation in the proportion of days in which it snowed ranged from just under 0.28 to just over 0.32, meaning that in the resampled data, there were some data cases where it snowed 28% of the time and others it snowed 32% of the time. Proportions of snow days in these extremes are more unlikely to occur as the density curve is much lower for these ranges, compared to the peak around 29% to 30%. 5.5.1 Bootstrap variation in prediction accuracy The same methods can be used to evaluate the prediction accuracy based on the classification model above. When using the bootstrap, we can get an estimate for how much variation there is in the classification accuracy based on the sample that we have. In addition, we can explore how different the prediction accuracy would be for many samples when using all the data and by splitting the data into training and test sets. The steps to do this resampling/bootstrap are similar to before, but one important step is added. Before the relevant statistic of interest is saved, the statistical model needs to be fitted on the resampled data. Resample the data, with replacement (assuming the population is representative) Fit statistical model to resampled data Compute and save a statistic of interest (e.g., mean accuracy) Repeat steps 1 through 3 many times (i.e., 5000, 10,000, or more) Explore the distribution of the statistic of interest When fitting a classification model, it can often be of interest to split the resampled data into the training and testing data as well as part of this process. This mimics the way in which the model would be used in practice and will help to ensure that the prediction accuracy obtained is not overstated. Therefore, a step 1a could be added such that the steps look like: Resample the data, with replacement (assuming the population is representative) Split resampled data into training/test data. Fit statistical model to resampled data If data are split into training/test, fit to training data Compute and save a statistic of interest (e.g., mean accuracy) If data are split into training/test, evaluate performance on testing data Repeat steps 1 through 3 many times (i.e., 5000, 10,000, or more) Explore the distribution of the statistic of interest This process is shown below with the function, calc_predict_acc(). Here the sample_n() function is used to sample with replacement, then fit the classification model to each of these samples, then calculate the prediction accuracy. First, I’m going to write a function to do all of these steps one time. calc_predict_acc &lt;- function(data) { rsamp &lt;- us_weather %&gt;% sample_n(nrow(us_weather), replace = TRUE) rsamp_split &lt;- initial_split(rsamp, prop = .8) rsamp_train &lt;- training(rsamp_split) rsamp_test &lt;- testing(rsamp_split) class_model &lt;- rpart(snow_factor ~ drybulbtemp_min + drybulbtemp_max + location, method = &#39;class&#39;, data = rsamp_train, cp = .02) rsamp_predict &lt;- rsamp_test %&gt;% mutate(tree_predict = predict(class_model, newdata = rsamp_test, type = &#39;class&#39;)) rsamp_predict %&gt;% mutate(same_class = ifelse(snow_factor == tree_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) } calc_predict_acc() ## response mean sum ## 1 same_class 0.7985294 543 The function will then be replicated 10,000 times to generate the prediction accuracy. predict_accuracy_fulldata &lt;- map_dfr(1:10000, calc_predict_acc) gf_density(~ mean, data = predict_accuracy_fulldata) %&gt;% gf_labs(x = &quot;Classification Accuracy&quot;) Figure 5.15: Prediction accuracy of classification model on resampled data with training/testing split. "],["regression-trees.html", "Chapter 6 Regression Trees 6.1 Predicting ACT Score 6.2 Continuous Association 6.3 First Regression Tree 6.4 Evaluating accuracy 6.5 Adding more attributes", " Chapter 6 Regression Trees Regression trees are an extension of classification trees where the outcome no longer represents categories (e.g., it snowed or did not snow), but instead the outcome is a continuous or interval type. For example, earlier in this book, distributions of college admission rates were explored, this represented a continuous or interval type attribute as the data took on many different values. Regression trees differ from classification trees in that the outcome of the model is a prediction of the continuous quantity rather than predicting if an observation belongs to a specific category. For example, instead of predicting whether it snows or not, a regression tree would predict how much snow fell, such as 5.5 inches. The difference in the predicted value has a few implications for the model, first, the accuracy measure used to evaluate the model needs to differ as it is unlikely that the prediction will be the same for any of the data in a regression tree, secondly, the prediction task is now much more difficult. These two observations need to be kept in mind when working with a regression tree. Much of the machinery is similar between regression and classification trees, however. Most notably, the same tree type structure will be used to make the prediction. Also, to parallel the classification trees explored so far, the regression trees used in this text will assume that only two splits can occur at any point along the tree. There are also differences between the classification and regression trees. Already discussed is the outcome attribute is continuous for a regression tree, therefore the predicted value will represent one that is continuous as well instead of representing a category. Furthermore, since the predictions are done on a continuous scale, a different measure of overall model fit or accuracy will have to be used. This chapter will explore those details in turn. But first, an example of a regression tree. 6.1 Predicting ACT Score The first example of using a regression tree will attempt to predict the median ACT score for a college institution using other attributes that describe the type of college. Before getting into the model, the packages and loading of the data needs to be done. 6.1.1 Loading R packages The following code will load the packages to use for the fitting of the regression tree and loading of the college scorecard data that was used earlier in the book (see Chapters 2 and 3). library(tidyverse) library(ggformula) library(mosaic) library(rpart) library(rsample) library(rpart.plot) library(statthink) library(parttree) # Set theme for plots theme_set(theme_statthinking()) # Load in data colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) 6.1.2 Visualize distributions Exploring the distribution of the variable of interest is often the first step in an analysis. A density figure is used to understand gf_density(~ actcmmid, data = colleges) %&gt;% gf_labs(x = &quot;Median college ACT score&quot;) ## Warning: Removed 730 rows containing non-finite values (stat_density). Figure 6.1: Density curve of median college ACT score. What are the primary features of the distribution for median ACT score of the college? Could there be concerns about specific features of this distribution if we are looking to do an analysis on this? 6.2 Continuous Association It is often interesting to estimate whether two continuous/interval attributes are associated or related to one another. This can be done with a statistic called the correlation. The correlation measures the degree to which the two attributes move together or not. For example, if one attribute increases does it give information about whether the second attribute increases (ie., a positive correlation) or decreases (ie., a negative correlation)? Before getting to the computation, exploring a sample can be helpful. First, to get a sense to what the correlation represents, it is useful to explore a scatterplot. The scatterplot depicts two attributes and places a point where the two attributes intersect. For this first example, a scatterplot with median college ACT score and admission rate are explored. gf_point(actcmmid ~ adm_rate, data = colleges, size = 3) %&gt;% gf_labs(x = &#39;Admission Rate&#39;, y = &#39;Median College ACT Score&#39;) ## Warning: Removed 730 rows containing missing values (geom_point). Figure 6.2: Scatterplot of median college ACT score by college admission rate. Figure 6.2 shows the bivariate relationship between median college ACT score and the college admission rate. If these two attributes are unrelated, it would not be possible to identify a trend in the scatterplot. However, in Figure 6.2, there is a noticeable trend. As median college ACT scores increase, the admission rates tend to decrease. This is an example of a negative trend and would represent a negative relationship, association, or correlation. Although there is a trend in the relationship, there are exceptions to this rule. For instance, notice the points that have very low median college ACT scores, there are cases that have lower admission rates (about 0.25 or 25%) and some have higher admission rates (above 0.5 or 50%). Focusing on the single point at about 0.25 admission rate and 9 median college ACT score, most of the median college ACT scores for an admission rate around 0.25 are above 30, with a few clustered around 20. This indicates the variation and imperfect relationship between the two attributes. The mantra, “correlation does not mean causation” is common in statistics courses and this variation and imperfect relationship shows one example of this. In short, it requires very specific designs to the data collection to be able to make very strong statements about causation, most often the data available represent observations that were simply observed rather than being part of a more stringent experimental design. The correlation statistic provides a numeric summary of the degree of relationship between two quantitative attributes. These can be estimated with the cor() function. The primary argument is a formula like has been used before and is similar to the gf_point() function used above. The data argument is specified and one other argument may be helpful when there are data missing. This is the use = 'complete.obs' argument. Specifying this argument with 'complete.obs' tells the cor() function to remove any missing data on the two attributes being specified and keep any data where pairs of data are present. cor(actcmmid ~ adm_rate, data = colleges, use = &#39;complete.obs&#39;) ## [1] -0.4058855 The correlation estimate depicted here is -0.41, which carries two pieces of information. One, it shows the direction of association, here negative means that as one attribute increases the other tends to decrease (an inverse relationship). This was what was noticed in 6.2. The second element the correlation shows is the magnitude of association. The correlation is standardized, meaning that regardless of the attributes used for the computation, the correlation will always be in the same range. The correlation will exclusively be between -1 to +1. Values of -1 or +1 indicate perfect negative or positive relationships respectively. A correlation of 0 means no relationship. The example above had a correlation of -0.41, which means the relationship is negative/inverse (as one attribute increases, the other tends to decrease) and the magnitude of association is about in between -1 and 0, but slightly closer to 0. What is considered a large, medium, or small association is largely dependent on the industry, but with education or social science data, this type of association would likely be considered a moderate or medium degree of association. The correlation makes one other assumption about the data. The correlation computed above and considered in this text is sometimes referred to as the Pearson correlation. This correlation assumes the data are best represented by a straight line or a linear relationship. If the relationship is not linear, but is best represented by a curved line, the correlation used in this course would likely underrepresent the degree of association. Note: Add some more detail on + or close to 0 correlations. 6.2.1 Correlation Computation The correlation can be represented by the following formula. In this formula, \\(\\bar{x}, \\bar{y}\\) are the means for x and y respectively. THe terms, \\(x_{i}\\), and \\(y_{i}\\), represent the individual data elements for each attribute. The numerator of the formula represents the deviations from the mean for the two attributes, which are multiplied together and added. The numerator is also referred to as the covariance and measures the degree to which the two attributes move together. \\[ r = \\frac{\\sum (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sqrt{\\sum(x_{i} - \\bar{x})^2}\\sqrt{\\sum(y_{i} - \\bar{y})^2}} \\] The denominator of the correlation are the standard deviation of the two attributes. These act to standardize the correlation so that no matter which attributes are entered into the formula, the correlation will always be between -1 and +1. Note: maybe add piece about the covariance graphically. 6.3 First Regression Tree Another, and related to the correlation, way to explore the relationship between two quantitative attributes is through the fitting a regression tree. A regression tree is similar to a classification tree, however now the output is a numeric or continuous type variable that takes on many different values. In the classification tree example, the focus in this class was predicting if a case belonged to one of two classes. The regression tree, in contrast, will predict the numeric variable with many potential values rather than just two. This will have implications for how the model is evaluated for accuracy as well as how well the model does at predicting specific values. These will be discussed in more detail later. The syntax for fitting a regression tree is very similar in R compared to the classification tree. The same function, rpart() is used and the function rpart.plot() will be used to visualize the fitted regression tree similar to before. The primary argument to the rpart() function is a formula where the left-hand side is the attribute of interest and the right hand side contains attributes that help predict the outcome. In the example below, the median college ACT score (actcmmid) is the outcome attribute and the college admission rate (adm_rate) is used as the sole continuous attribute used to predict the median college ACT score. The data argument is also specified and the only difference here between a classification tree and the regression tree here is the method argument. In the regression tree the method argument should be set to method = 'anova'. This tells the rpart() function that the outcome is numeric and that an anova method should be used in the model fitting. The anova stands for Analysis of Variance and we will discuss this in more detail moving forward. act_reg &lt;- rpart(actcmmid ~ adm_rate, data = colleges, method = &quot;anova&quot;) rpart.plot(act_reg, roundint = FALSE, type = 3, branch = .3) Figure 6.3: The first regression tree predicting median college ACT score based on the admission rate. The output from the regression tree is similar to that from a classification tree. One major difference however is that the predicted values in the end are numeric quantities instead of classes and the probabilities that were shown previously are not shown here as there is not a probability of being in a class. The percentage of cases in the predicted nodes at the end of the tree are still shown. The logic for following the tree is the same as before where each split can have two new paths to follow and then the variable(s) are re-evaluated after the first split to see if additional splits can help predict the outcome of interest. Below is a figure that builds on the scatterplot we saw above. Vertical lines are shown that indicate the splits that were established from the regression tree in Figure 6.3. These splits are where the end buckets lie and all of the data points residing in a single area have the same median ACT score. For example, all schools in the left-most quadrant of Figure 6.4 would all have the same predicted median ACT score of 32. In reality, one can see that the value of 32 is correct for some of the schools, but there are others in this quadrant that have median ACT scores of less than 20. The next section will explore how accuracy for the regression tree can be evaluated. gf_point(actcmmid ~ adm_rate, data = colleges, color = &#39;gray55&#39;) %&gt;% gf_labs(y = &quot;Median college ACT score&quot;, x = &quot;Admission Rate&quot;, title = &quot;Log salary by number of home runs&quot;) + geom_parttree(data = act_reg, aes(fill = actcmmid), alpha = 0.3) + scale_fill_continuous(&quot;ACT&quot;) ## Warning: Removed 730 rows containing missing values (geom_point). Figure 6.4: Visual depiction of the regression tree splits. 6.4 Evaluating accuracy In the classification tree example, a natural metric to evaluate how well the model was doing was the classification accuracy. Classification accuracy was often best computed for each class instead of a single overall accuracy statistic. In the regression tree example, there is no class membership, instead the original observed college ACT score and the predicted college ACT scores can be compared. One measure that could be used for accuracy is on average how far do the predicted scores deviate from the observed scores. Mathematically, this would look like: \\[ accuracy = predicted - observed \\] However, this will have issues, as when the deviation was computed, those values above and below will offset one another. Therefore, simply computing the accuracy as the predicted score minus the observed score will on average be 0. Similarly to the standard deviation, we could square the difference between the predicted and observed values to get the mean square error. We could also take the square root of the mean square error (MSE) to get the root mean square error (RMSE), the benefit of the RMSE over the MSE is that the RMSE will be on the same metric as the outcome of interest. Mathematically, these can be represented as: \\[ MSE = \\frac{\\sum (predicted - observed) ^ 2}{n} \\] \\[ RMSE = \\sqrt{\\frac{\\sum (predicted - observed) ^ 2}{n}} \\] Another statistic can also be useful here, the mean absolute error (MAE). This statistic was first introduced in chapter 3 and is similar to the RMSE above, but except for squaring values and taking the square root, the absolute value is used instead. This has the strength of never squaring values, so the metric is the same as the outcome, but has the weakness of having weaker statistical properties. The mathematical computation can be shown as: \\[ MAE = \\frac{\\sum \\left|predicted - observed \\right| }{n} \\] The below code chunk computes the deviation labeled as error. The first 10 rows are shown along with the institution name (instnm), median ACT score (actcmmid), and the predicted median ACT score (act_pred). Notice within the error column that there are some values above and below zero. You can see the values that have a negative error have an actual median ACT score less than the predicted values. Conversely, those that have positive errors have actual median ACT scores greater than the predicted ACT value. colleges_pred &lt;- colleges %&gt;% drop_na(actcmmid) %&gt;% mutate(act_pred = predict(act_reg), error = actcmmid - act_pred) %&gt;% select(instnm, actcmmid, act_pred, error) head(colleges_pred, n = 10) ## # A tibble: 10 × 4 ## instnm actcmmid act_pred error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama A &amp; M University 18 21.5 -3.50 ## 2 University of Alabama at Birmingham 25 21.5 3.50 ## 3 University of Alabama in Huntsville 28 23.1 4.89 ## 4 Alabama State University 18 21.5 -3.50 ## 5 The University of Alabama 28 23.1 4.89 ## 6 Auburn University at Montgomery 22 23.1 -1.11 ## 7 Auburn University 27 23.1 3.89 ## 8 Birmingham Southern College 26 23.1 2.89 ## 9 Faulkner University 20 23.1 -3.11 ## 10 Huntingdon College 22 23.1 -1.11 The df_stats() function can used to compute summary statistics for the error attribute which represented the difference between the observed and predicted college ACT score. A variety of statistics are computed on the error attribute including the mean, median, variance (var), standard deviation (sd), minimum (min), and maximum (max). colleges_pred %&gt;% df_stats(~ error, mean, median, var, sd, min, max) ## response mean median var sd min max ## 1 error -5.043527e-16 -0.111001 9.919378 3.149504 -16.68116 7.888999 Notice from the statistics computed, the mean and median of the deviation or raw error (i.e., \\(predicted - observed\\)), the values are very close to zero and are practically zero for the mean. This is occurring due to the positive and negative errors are offsetting one another. The median isn’t exactly zero as the distribution of errors are not perfectly symmetric. Figure 6.5 shows the distribution of errors. Notice there is a longer tail on the negative side, meaning that there are some predicted values that are much larger than the observed median ACT score. This is also shown by the min and max statistics computed above. gf_density(~ error, data = colleges_pred) %&gt;% gf_labs(x = &quot;Error = Predicted - Observed&quot;) Figure 6.5: Distribution of error deviations from the regression tree model. Circling back to the final statistic computed above, the variance (var) and standard deviation (sd) of the error term represents the MSE and RMSE respectively and are interpretable. In general, larger values of these statistics indicate less accuracy, that is, on average the squared deviations of the predicted minus the observed are larger. In practice, the RMSE is often used more than the MSE as the metric is the same as the outcome, in this case the median ACT scores. The RMSE of 3.15 says that on average the predicted median institution ACT scores are about 3.15 away from the observed median institution ACT score. The mean absolute error can be computed similarly, except instead of using the raw deviations (ie., errors), the absolute value of the deviations needs to be computed first. This is done using the abs() function shown below and specified as abs(error). The error term represents the errors computed as the difference in the predicted and observed median ACT scores. Similar to the statistics computed above, the mean, median, minimum, and maximum statistics are computed. colleges_pred %&gt;% df_stats(~ abs(error), mean, median, min, max) ## response mean median min max ## 1 abs(error) 2.396454 1.888999 0.111001 16.68116 When interpreting these statistics, the mean and median column would represent the mean and median absolute error and are on the same scale as the outcome attribute, in this case the median ACT score. Therefore, the value of 2.4 states that on average the predicted median ACT scores are within 2.4 from the observed scores. The median is interpreted similarly, but instead of being the average absolute deviation, this represents the median absolute deviation. Finally, notice how the absolute value worked, the minimum statistic is positive, whereas before it was negative. The largest negative deviation/error from before is now the largest shown in the maximum column. The absolute value of the errors can be shown graphically to fully understand the new distribution. gf_density(~ abs(error), data = colleges_pred) %&gt;% gf_labs(x = &quot;Error = |Predicted - Observed|&quot;) Figure 6.6: The distribution of errors after taking the absolute value . This distribution is not symmetric, rather it is more like an exponential distribution with many values close to zero and rather quickly decreasing in the number of errors that are larger. This indicates that much of the predictions are close to the observed value with a few that are further away indicating worse performance. It is also worth mentioning that these accuracy statistics are scale dependent. Therefore if the scales of two outcomes are different, the MAE, MSE, or RMSE are not directly comparable without standardization. For example, if instead of median ACT scores, the outcome was median SAT scores, the accuracy statistics would naturally be larger due to SAT scores being larger. SAT scores for individual sections range from 200 to 800 instead of ACT scores ranging from 0 to 36. The MAE, MSE, or RMSE statistics would look larger due to scale differences, but that would not necessarily mean the model is doing worse. To compare these two outcomes, the outcomes or the accuracy statistics would need to be put on the same scale. 6.4.1 Conditional Error Similar to problems with just using the overall classification accuracy with classification trees, it can be helpful to use conditional error with regression trees. The conditional error can help to identify areas of the model that are performing worse, more specifically, to understand where the model has larger errors. To do this with some descriptive statistics, the df_stats() function can be used where the formula input can include two attributes, the error on the left side of the ~ and the attribute to explore the conditional error by on the right hand side. The formula used below explores the MAE across different median ACT scores using the formula, abs(error) ~ actcmmid. The function length is used to calculate how many schools are in each of the computations. colleges_pred %&gt;% df_stats(abs(error) ~ actcmmid, mean, median, min, max, length) ## response actcmmid mean median min max length ## 1 abs(error) 6 15.5032258 15.5032258 15.5032258 15.503226 1 ## 2 abs(error) 7 16.1110010 16.1110010 16.1110010 16.111001 1 ## 3 abs(error) 9 16.6811594 16.6811594 16.6811594 16.681159 1 ## 4 abs(error) 11 12.1110010 12.1110010 12.1110010 12.111001 1 ## 5 abs(error) 15 9.3960802 9.3960802 8.1110010 10.681159 2 ## 6 abs(error) 16 7.0523290 7.1110010 5.5032258 9.681159 11 ## 7 abs(error) 17 6.1276847 6.1110010 4.5032258 8.681159 19 ## 8 abs(error) 18 5.4476764 5.1110010 3.5032258 13.642857 32 ## 9 abs(error) 19 4.0157276 4.1110010 2.5032258 12.642857 59 ## 10 abs(error) 20 2.9102946 3.1110010 1.5032258 5.681159 117 ## 11 abs(error) 21 1.8594407 2.1110010 0.5032258 4.681159 156 ## 12 abs(error) 22 1.0878288 1.1110010 0.4967742 3.681159 166 ## 13 abs(error) 23 0.3555768 0.1110010 0.1110010 2.681159 174 ## 14 abs(error) 24 1.0982335 0.8889990 0.8889990 7.642857 155 ## 15 abs(error) 25 2.0242671 1.8889990 0.6811594 3.496774 101 ## 16 abs(error) 26 2.9769006 2.8889990 0.3188406 5.642857 70 ## 17 abs(error) 27 3.9392465 3.8889990 3.8889990 5.496774 47 ## 18 abs(error) 28 4.7721736 4.8889990 2.3188406 4.888999 44 ## 19 abs(error) 29 5.5355597 5.8889990 2.6428571 5.888999 31 ## 20 abs(error) 30 5.7496633 6.8889990 1.6428571 6.888999 25 ## 21 abs(error) 31 5.5947035 5.3188406 0.6428571 7.888999 22 ## 22 abs(error) 32 2.1915114 0.3571429 0.3571429 6.318841 26 ## 23 abs(error) 33 1.7545894 1.3571429 1.3571429 7.318841 15 ## 24 abs(error) 34 2.3571429 2.3571429 2.3571429 2.357143 12 ## 25 abs(error) 35 3.3571429 3.3571429 3.3571429 3.357143 1 Notice that there is a row for each median ACT score found in the data and that each statistic is computed for each unique median ACT score. Procedurally, the data are split into compartments and the statistics are computed on the data that fit into those compartments. The MAE (shown by the mean column) shows larger errors at low values of median ACT score, which decreases quickly until a median ACT score of 23, then increases slightly, then decreases again for higher values. Before spending too much time with this table, take a peak at the last column, labeled length. This column tells how many data points are in each of the compartments to do the calculation. In general, compartments with less data in them will produce statistics that are more unstable. For instance, there are very few schools with very low ACT scores, for example, scores of 15 and under. Furthermore, there is only 1 school with a median ACT score of 35. When there are small numbers for some of these categories, it can be helpful to combine them for evaluation of model accuracy. In this example, I’m going to collapse scores less than 15 into one category and 34 and higher into another. The other median ACT scores I’m going to leave by themselves. colleges_pred &lt;- colleges_pred %&gt;% mutate(act_recode = ifelse(actcmmid &lt;= 15, &#39;15 or smaller&#39;, ifelse(actcmmid &gt;= 34, &#39;34 or larger&#39;, actcmmid))) The mutate() function above is used to collapse the categories and create a new attribute named act_recode. Then, this new attribute is used to compute the MAE and other statistics to evaluate conditional model performance. Notice that the length column at the end no longer has categories with a single data point in them, rather the smallest is now 6, which is still small, but at least there are more than one data point in them. Similar trends to those discussed above pertain here. conditional_error &lt;- colleges_pred %&gt;% df_stats(abs(error) ~ act_recode, mean, median, min, max, length) conditional_error ## response act_recode mean median min max length ## 1 abs(error) 15 or smaller 13.1997579 13.8071134 8.1110010 16.681159 6 ## 2 abs(error) 16 7.0523290 7.1110010 5.5032258 9.681159 11 ## 3 abs(error) 17 6.1276847 6.1110010 4.5032258 8.681159 19 ## 4 abs(error) 18 5.4476764 5.1110010 3.5032258 13.642857 32 ## 5 abs(error) 19 4.0157276 4.1110010 2.5032258 12.642857 59 ## 6 abs(error) 20 2.9102946 3.1110010 1.5032258 5.681159 117 ## 7 abs(error) 21 1.8594407 2.1110010 0.5032258 4.681159 156 ## 8 abs(error) 22 1.0878288 1.1110010 0.4967742 3.681159 166 ## 9 abs(error) 23 0.3555768 0.1110010 0.1110010 2.681159 174 ## 10 abs(error) 24 1.0982335 0.8889990 0.8889990 7.642857 155 ## 11 abs(error) 25 2.0242671 1.8889990 0.6811594 3.496774 101 ## 12 abs(error) 26 2.9769006 2.8889990 0.3188406 5.642857 70 ## 13 abs(error) 27 3.9392465 3.8889990 3.8889990 5.496774 47 ## 14 abs(error) 28 4.7721736 4.8889990 2.3188406 4.888999 44 ## 15 abs(error) 29 5.5355597 5.8889990 2.6428571 5.888999 31 ## 16 abs(error) 30 5.7496633 6.8889990 1.6428571 6.888999 25 ## 17 abs(error) 31 5.5947035 5.3188406 0.6428571 7.888999 22 ## 18 abs(error) 32 2.1915114 0.3571429 0.3571429 6.318841 26 ## 19 abs(error) 33 1.7545894 1.3571429 1.3571429 7.318841 15 ## 20 abs(error) 34 or larger 2.4340659 2.3571429 2.3571429 3.357143 13 It can be helpful to visualize some of these statistics to help facilitate any trend in the model performance across the median ACT values. A pointrange plot is used to show the average mean absolute error and also the minimum and maximum absolute error values for each median ACT score, using the gf_pointrangeh() function. The formula input for this function takes the following form, y-scores ~ mean-value + min-value + max-value. gf_pointrangeh(act_recode ~ mean + min + max, data = conditional_error) %&gt;% gf_labs(y = &quot;Median ACT Score&quot;, x = &quot;Absolute Error&quot;) Figure 6.7: Ranges of absolute errors across different median ACT scores with mean absolute error. Figure 6.7 shows these values with the MAE shown with the circles and the horizontal lines extend to the minimum and maximum absolute values for each median ACT score. The trend of decreasing absolute error from small median ACT scores up to a score of 23 is shown. Then the error tends to increase, then decrease again for higher median ACT scores. This shows that the regression tree model is doing well for median ACT scores around 23 and also for median ACT scores around 33. Another notable feature this figure helps to articulate is the range of absolute errors, for instance, with median ACT scores of 18 or 19, the range of absolute errors range from less than 5 to about 13 or 14. These large range of errors are not as surprising here as the regression tree used to generate these errors is only using a single attribute to help predict median ACT score for institutions, the admission rate. It would make sense that other attributes may help to reduce the error and increase the utility of the model, but this is a good baseline to compare how well other models perform. 6.4.1.1 Explore another attribute Let’s explore another attribute, the undergraduate enrollment of institutions to see if it is related to the median ACT score. First a scatterplot is shown then the correlation is computed. gf_point(actcmmid ~ ugds, data = colleges, size = 3) %&gt;% gf_labs(x = &quot;Undergraduate Enrollment&quot;, y = &quot;Median ACT Score&quot;) ## Warning: Removed 730 rows containing missing values (geom_point). The correlation is 0.27 between median ACT score and the undergraduate enrollment of institutions. act_reg2 &lt;- rpart(actcmmid ~ adm_rate + ugds, data = colleges, method = &quot;anova&quot;) rpart.plot(act_reg2, roundint = FALSE, type = 3, branch = .3) Figure 6.8: Regression tree with admission rates and ungraduate enrollments predicting median ACT score Figure 6.8 shows the regression tree when adding in the undergraduate enrollment attribute in addition to the admission rate. The regression tree shows that the admission rate is the most important attribute in predicting median ACT score (it is the first split in the regression tree), however undergraduate enrollment is important as this is the second split for those with admission rates greater than 25%. To fully understand how much better this model is from the one with only admission rates included, we can use MAE to evaluate the prediction accuracy. First, the predicted values from the new model and the new errors (predicted - observed) need to be added to the data. colleges_pred &lt;- colleges_pred %&gt;% mutate(act_pred2 = predict(act_reg2), error2 = actcmmid - act_pred2) colleges_pred ## # A tibble: 1,289 × 7 ## instnm actcmmid act_pred error act_recode act_pred2 error2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama A &amp; M University 18 21.5 -3.50 18 21.6 -3.55 ## 2 University of Alabama at… 25 21.5 3.50 25 24.8 0.222 ## 3 University of Alabama in… 28 23.1 4.89 28 23.3 4.69 ## 4 Alabama State University 18 21.5 -3.50 18 21.6 -3.55 ## 5 The University of Alabama 28 23.1 4.89 28 24.8 3.22 ## 6 Auburn University at Mon… 22 23.1 -1.11 22 23.3 -1.31 ## 7 Auburn University 27 23.1 3.89 27 24.8 2.22 ## 8 Birmingham Southern Coll… 26 23.1 2.89 26 21.7 4.34 ## 9 Faulkner University 20 23.1 -3.11 20 23.3 -3.31 ## 10 Huntingdon College 22 23.1 -1.11 22 21.7 0.342 ## # … with 1,279 more rows conditional_error2 &lt;- colleges_pred %&gt;% df_stats(abs(error2) ~ act_recode, mean, median, min, max, length) conditional_error2 ## response act_recode mean median min max length ## 1 abs(error2) 15 or smaller 11.9076028 13.6583541 6.6583541 15.658354 6 ## 2 abs(error2) 16 5.9580028 5.6583541 5.6583541 7.306422 11 ## 3 abs(error2) 17 5.4041307 4.6583541 4.5512821 9.153846 19 ## 4 abs(error2) 18 4.9394393 3.6583541 3.5512821 13.642857 32 ## 5 abs(error2) 19 3.6881805 2.6583541 2.5512821 12.642857 59 ## 6 abs(error2) 20 2.3972274 1.6583541 1.5512821 6.153846 117 ## 7 abs(error2) 21 1.4072426 0.6583541 0.5512821 5.153846 156 ## 8 abs(error2) 22 1.0661678 1.3064220 0.3416459 4.153846 166 ## 9 abs(error2) 23 0.8835657 0.3064220 0.3064220 6.590909 174 ## 10 abs(error2) 24 1.2956011 0.6935780 0.6935780 7.642857 155 ## 11 abs(error2) 25 1.7661025 1.6935780 0.2222222 3.448718 101 ## 12 abs(error2) 26 2.6643144 2.6935780 0.1538462 5.642857 70 ## 13 abs(error2) 27 3.6386582 3.6935780 2.2222222 5.341646 47 ## 14 abs(error2) 28 4.3625148 4.6935780 1.8461538 6.341646 44 ## 15 abs(error2) 29 4.0390696 4.2222222 0.5909091 7.341646 31 ## 16 abs(error2) 30 4.0587815 3.8461538 0.4090909 6.693578 25 ## 17 abs(error2) 31 4.4734457 4.8461538 0.6428571 9.341646 22 ## 18 abs(error2) 32 1.9545834 0.3571429 0.3571429 10.341646 26 ## 19 abs(error2) 33 1.4939394 1.3571429 1.3571429 3.409091 15 ## 20 abs(error2) 34 or larger 2.4340659 2.3571429 2.3571429 3.357143 13 bind_rows( mutate(conditional_error, model = &#39;model1&#39;), mutate(conditional_error2, model = &#39;model2&#39;) ) %&gt;% gf_pointrangeh(act_recode ~ mean + min + max, color = ~ model, position = position_dodge(width = 0.5)) %&gt;% gf_labs(y = &quot;Median ACT Score&quot;, x = &quot;Absolute Error&quot;) Figure 6.9: Compared absolute error across the two regression tree models fitted. Comparison of the conditional error can be helpful to evaluate areas of the model that improved and if any other areas did not improve or were worse. 6.5 Adding more attributes To evaluate how well this model can do, all of the attributes will be open to be included in the final model. The only one that is removed from the main data is the institution name. This can be done with the . character after the ~ in the model formula. The model tree is not printed as it is large and not able to be easily viewed. Instead, the evaluation of accuracy will be compared to the other two models fitted. The attributes included in the final model though can be accessed and ranked by those that are the most important. act_reg_sat &lt;- rpart(actcmmid ~ ., data = select(colleges, -instnm), method = &quot;anova&quot;) colleges_pred &lt;- colleges_pred %&gt;% mutate(act_pred_sat = predict(act_reg_sat), error_sat = actcmmid - act_pred_sat) conditional_error_sat &lt;- colleges_pred %&gt;% df_stats(abs(error_sat) ~ act_recode, mean, median, min, max, length) act_reg_sat$variable.importance ## city tuitionfee_out tuitionfee_in costt4_a adm_rate ## 10608.27504 8438.13169 7869.78358 6700.46334 2858.66475 ## stabbr debt_mdn grad_debt_mdn ugds region ## 1420.67466 696.36673 570.77579 210.91393 76.49506 ## locale ## 53.73178 The table below shows the attributes that are the most important, with those at the top of the table being more important than those at the bottom of the table. Notice that the most important seems to be the city, followed by the tuition, then cost, and so on. x city 10608.27504 tuitionfee_out 8438.13169 tuitionfee_in 7869.78358 costt4_a 6700.46334 adm_rate 2858.66475 stabbr 1420.67466 debt_mdn 696.36673 grad_debt_mdn 570.77579 ugds 210.91393 region 76.49506 locale 53.73178 The evaluation accuracy of this model that could contain most of the attributes in the data is shown in Figure 6.10. The figure also shows the comparisons to the other two models explore in this chapter, labeled as ‘model1’ and ‘model2’ respectively. Notice the improvement across all of the median ACT scores for the model that was fitted with the ability to include all of the attributes. bind_rows( mutate(conditional_error, model = &#39;model1&#39;), mutate(conditional_error2, model = &#39;model2&#39;), mutate(conditional_error_sat, model = &#39;model_sat&#39;) ) %&gt;% mutate(model = factor(model, levels = c(&#39;model1&#39;, &#39;model2&#39;, &#39;model_sat&#39;))) %&gt;% gf_pointrangeh(act_recode ~ mean + min + max, color = ~ model, position = position_dodge(width = 0.5)) %&gt;% gf_labs(y = &quot;Median ACT Score&quot;, x = &quot;Absolute Error&quot;) Figure 6.10: Evaluation of model accuracy across the three models fitted. "],["linear-regression.html", "Chapter 7 Linear Regression 7.1 Simple Regression continuous predictor 7.2 Conditional Means", " Chapter 7 Linear Regression Linear regression is another statistical model that can be used when the outcome is an integer or continuous, similar to that of the regression tree. There are similarities between linear regression and regression trees, however there are also very notable differences. Linear regression and regression trees are both used when the outcome is continuous or continuous. These methods are both able to make predictions for the outcome and identify which attributes are most important in aiding in making those predictions. There are differences though in the two methods that are important to distinguish. Linear regression makes an assumption on the relationship between the outcome and attributes entered into the model. Linear regression, as in the name, assumes that the relationship between the outcome and an attribute is linearly related.18 Regression trees however do not directly make this linear assumption between the outcome and an attribute. This assumption can make linear regression more parsimonious, meaning that the model can be simpler, which is often a goal of science, to explain the phenomenon of interest with the simplest possible explanation. 7.1 Simple Regression continuous predictor 7.1.1 Description of the Data These data contain information on weather from a variety of weather stations in the northern part of the United States during cooler months of the year (October to April). A total of 3400 observations are in the data, 425 observations from each station. library(tidyverse) library(ggformula) library(mosaic) library(rsample) library(statthink) # Set theme for plots theme_set(theme_statthinking()) us_weather &lt;- mutate(us_weather, snow_factor = factor(snow), snow_numeric = ifelse(snow == &#39;Yes&#39;, 1, 0)) To provide a bit more context to the data, the station locations are shown below using the function count(us_weather, location) and shown in Table 7.1. Various weather observations from these station locations were collected, including temperature, dewpoint, humidity, wind speed, precipitation type and amount, and other information. Table 7.1: Station locations for the United States weather data location n Boston, MA 425 Buffalo, NY 425 Chicago, IL 425 Detroit, MI 425 Duluth, MN 425 Iowa City, IA 425 Minneapolis, MN 425 Portland, ME 425 7.1.2 Scatterplots As we’ve explored before, scatterplots help to explore the relationship between two continuous, quantitative data attributes. These are created with the gf_point() function and adding lines to the figure to provide some guidance to the relationship between the two attributes can be done with the gf_smooth() function. Below, a scatterplot is created that explores the relationship between the low temperature and the average daily dew point.19 gf_point(drybulbtemp_min ~ dewpoint_avg, data = us_weather, size = 3, alpha = .2) %&gt;% gf_smooth(method = &#39;lm&#39;, size = 1) %&gt;% gf_labs(x = &quot;Average daily dew point&quot;, y = &quot;Minimum daily temperature (in F)&quot;) Figure ?? shows the relationship between minimum temperature and average daily dew point assuming a linear relationship (specified with gf_smooth(method = 'lm')) between the two attributes. In this example, the relationship between the two attributes goes from the lower left to the upper right, indicating a positive relationship. That is, as the average daily dew point tends to increase, the minimum daily temperature also tends to increase. The relationship is not perfect, which is shown by the points not falling perfectly on the blue line, but the data are clustered relatively closely to this blue line. This would suggest that the relationship is stronger rather than being weaker and closer to +1 than 0. To estimate as a single quantity the relationship between the two attributes, the correlation can be calculated with the cor() function with the primary argument being a formula depicting the two variables to compute the correlation. The optional argument, use = 'complete.obs' is used as there are some missing data on these attributes and these data are removed prior to the calculation. cor(drybulbtemp_min ~ dewpoint_avg, data = us_weather, use = &#39;complete.obs&#39;) ## [1] 0.9102476 Here the correlation represents the degree of linear relationship between the two variables. Values closer to 1 in absolute value (i.e. +1 or -1) show a stronger linear relationship and values closer to 0 indicate no relationship or weaker relationship. The correlation between the two variables above was about 0.91 indicating that there is a strong positive linear relationship between the minimum temperature and the daily dew point. The correlation is shown to be positive due to the coefficient being positive and the general trend from the scatterplot shows a direction of relationship moving from the lower left of the figure to the upper right of the figure. A negative correlation would have a negative sign associated with it and would trend from the upper left to the lower right of a scatterplot. 7.1.3 Fitting a linear regression model Now that the correlation was computed, we have evidence that there is a relationship between the minimum temperature and the average daily dew point. To provide some more evidence about the strength of this relationship and how much error is involved, fitting a linear regression model is often done. This can be done with the lm() function where the two arguments that need to be specified are a formula and the data to use for the model fitting. The formula takes the following form: drybulbtemp_min ~ dewpoint_avg, where the minimum temperature is the outcome of interest (in language we’ve used previously, this is the attribute we want to predict) and the average daily dew point is the attribute we want to use to help to predict the minimum temperature. Another way to think about what these variables represent is to explain variation in the minimum temperature with the average daily dew point. In other words, the assumption is made that the average daily dew point impacts or explains differences in the minimum temperature. temp_reg &lt;- lm(drybulbtemp_min ~ dewpoint_avg, data = us_weather) coef(temp_reg) ## (Intercept) dewpoint_avg ## 4.8343103 0.8999469 The following coefficients represent the linear regression equation that more generally can be show as: \\[\\begin{equation} drybulbtemp\\_min = 4.83 + 0.90 dewpoint\\_avg + \\epsilon \\end{equation}\\] The equation can also be represented without the error, \\(\\epsilon\\) as: begin{equation} = 4.83 + 0.90 dewpoint_avg \\end{equation} where now the minimum temperature outcome has a hat (i.e. \\(\\hat{y}\\)) that denotes mathematically that the equation predicts a value of minimum temperature given solely the average daily dew point. The first equation above says that the original observed minimum temperature is a function of the average daily dew point plus some error. Using the equation above, the predicted minimum temperature can be obtained by including a value inserted for the average daily dew point. Let’s pick a few values for the average daily dew point to try. 4.83 + 0.90 * -15 ## [1] -8.67 4.83 + 0.90 * 0 ## [1] 4.83 4.83 + 0.90 * 25 ## [1] 27.33 4.83 + 0.90 * 26 ## [1] 28.23 You may notice that the predicted value of minimum temperature increases by 0.90 degrees Fahrenheit for every one degree increase in average daily dew point, often referred to as the linear slope. The predicted values would fit on the line shown in Figure ??. This highlights the assumption made from the linear regression model in which the relationship between the minimum temperature and the average daily dew point is assumed to be linear. It is possible to relax this assumption with a more complicated model, however that is not explored here. The y-intercept, shown as 4.83 in the equations above will be explored in the next section. 7.1.4 Explore the y-intercept So far the discussion has focused on the linear slope, often a term that is of most interest. However, the y-intercept can also be an important term. The y-intercept from the equations above when the minimum temperature was predicted or explained by the average daily dew point was 4.83. What does this term mean? Exploring the scatterplot can provide some insight to the interpretation of the intercept. Figure ?? shows a scatterplot where the y-axis is the minimum temperature and the x-axis is the average daily dew point. This figure shows a horizontal and vertical line that cross on the blue line shown in the figure. The vertical line is shown at an average daily dew point of 0 and the horizontal line is at a minimum temperature at 4.83. These cross or intersect on the blue line which represents the relationship between the average daily dew point and the minimum temperature. This depicts what the y-intercept is, the y-intercept represents the average minimum temperature for an average daily dew point of 0. More generally, the y-intercept is the average value of the outcome when all of the attributes included in the linear regression are 0. gf_point(drybulbtemp_min ~ dewpoint_avg, data = us_weather, size = 3, alpha = .2) %&gt;% gf_smooth(method = &#39;lm&#39;, size = 1) %&gt;% gf_vline(xintercept = 0) %&gt;% gf_hline(yintercept = 4.83) %&gt;% gf_labs(x = &quot;Average daily dew point&quot;, y = &quot;Minimum daily temperature (in F)&quot;) In the example that has been explored so far, a value of 0 for the average daily dew point occurs within the sample of data. That is, an average daily dew point of 0 occurs in the data are is directly interpretable. This would mean that when an average daily dew point of 0 happens in real life, the minimum temperature is 4.83 degrees Fahrenheit on average for the data that is included in this sample. There are situations where a value of 0 for an attribute is not plausible to occur. An example could be predicting a baby’s birth weight based on the number of gestation days.20 In situations where the value of 0 is not plausible or meaningful, the data can be centered on a value that is more meaningful or plausible to help with the interpretation of the y-intercept. Before this is done, below is a more concrete example that still tries to predict/explain minimum temperature, but now the attribute to explain differences in the minimum temperature is the average daily sea level pressure. Sea-level pressure is a way to measure atmospheric pressure and is measured in inches of mercury21. Figure ?? shows a scatterplot of this relationship. First, notice that the direction of the blue line is different than before, it starts in the upper left quadrant and decreases as the sea level pressure increases. This is indicative of a negative relationship between the two attributes. The correlation can be estimated to be -0.38, a moderate negative relationship. This relationship is not as strong as the relationship between minimum temperature and average daily dew point. The variation around the blue line in ?? is much larger which depicts a weaker relationship (ie., more error) compared to the minimum temperature and dew point relationship (see Figure ??). gf_point(drybulbtemp_min ~ sealevelpressure_avg, data = us_weather, size = 3, alpha = .2) %&gt;% gf_smooth(method = &#39;lm&#39;, size = 1) %&gt;% gf_labs(x = &quot;Average daily sea level pressure&quot;, y = &quot;Minimum daily temperature (in F)&quot;) If a linear regression is estimated with these two attributes, the following coefficients and resulting regression equation can be generated. sea_temp &lt;- lm(drybulbtemp_min ~ sealevelpressure_avg, data = us_weather) coef(sea_temp) ## (Intercept) sealevelpressure_avg ## 660.05123 -21.05011 \\[\\begin{equation} drybulbtemp\\_min = 660.05 - 21.05 sealevelpressure\\_avg + \\epsilon \\end{equation}\\] For these, the y-intercept is about 660 degrees Fahrenheit and the linear slope is about -21 degrees Fahrenheit. How are these interpreted? Starting first with the linear slope, the -21 means that for every one unit increase in sea level pressure (ie., moving from 29 to 30 inches of mercury), the average minimum temperature decreases by 21 degrees Fahrenheit. Why is the y-intercept 660 degrees Fahrenheit, a temperature that is much above the boiling point of water and is hotter than most ovens. Furthermore, this is not an atmospheric temperature that we have seen in the data, so why is the y-intercept 660 degrees Fahrenheit? Ultimately, this comes down to the range of sea level pressure, where the minimum value in the data is 29.02, a value very far from 0. Therefore, the linear relationship depicted above is extrapolated outside the range of the data (ie, decreased by 29.02 sea level units) which increases the temperature by 610.871 degrees Fahrenheit units. This value is not interpretable or meaningful as it would not be possible to get a sea level pressure of 0 in the metric of inches of mercury. It is possible to modify the data, but not change the overall relationship between minimum temperature and sea level pressure to help increase the interpretation of the y-intercept. Three options will be explored, mean value centered, minimum value centered, and maximum value centered, although there are many other potential options. 7.1.4.1 Mean center sea level pressure First, mean centering the x attribute can often be a way to make the y-intercept more interpretable. The code below shows a scatterplot by subtracting the mean from all the values of sea level pressure. More explicitly, the value of 30.04 was subtracted from each sea level pressure value in the data. gf_point(drybulbtemp_min ~ I(sealevelpressure_avg - mean(sealevelpressure_avg, na.rm = TRUE)), data = us_weather, size = 3, alpha = .2) %&gt;% gf_smooth(method = &#39;lm&#39;, size = 1) %&gt;% gf_labs(x = &quot;Average daily sea level pressure&quot;, y = &quot;Minimum daily temperature (in F)&quot;) Notice that the relationship is the same as before, but now the scale of sea level pressure is different. From the coefficients shown below, notice that the linear slope is the same as before, -21.05, showing that the subtraction of the mean for each value did not impact the overall relationship between the two attributes. The difference, is now the y-intercept is much smaller and would represent the mean minimum temperature when the mean-centered sea level pressure is 0 (ie., a value of 30.04 in the original metric). sealevel_reg_centered &lt;- lm(drybulbtemp_min ~ I(sealevelpressure_avg - mean(sealevelpressure_avg, na.rm = TRUE)), data = us_weather) coef(sealevel_reg_centered) ## (Intercept) ## 27.63117 ## I(sealevelpressure_avg - mean(sealevelpressure_avg, na.rm = TRUE)) ## -21.05011 The new equation would look like: begin{equation} = 27.63 - 21.05 (sealevelpressure_avg - mean(sealevelpressure_avg)) \\end{equation} 27.63 - 21.05 * -1 ## [1] 48.68 27.63 - 21.05 * 0 ## [1] 27.63 27.63 - 21.05 * .5 ## [1] 17.105 This can provide an intercept that is more interpretable and can be a model parameter that is of direct interest. The value for the y-intercept also makes more intuitive sense, which can aid in model interpretation. It should be kept in mind that this model would fit the data the same as before, that is, this model is not inherently better. Instead the benefits of this model is simply in the interpretation and intuitiveness of the y-intercept. The regression line that has been shown in the scatterplots in Figures ?? and ?? are the same ones. 7.1.4.2 Minimum or Maximum centered sea level pressure Mean centering an attribute can be an attractive way to make the y-intercept more intuitive and interpretable, but it is not the only option. A few other options that are common include subtracting the minimum or maximum values from the predictor attribute. These are shown below, starting first with the minimum centered and then the maximum centered regression. sealevel_reg_min &lt;- lm(drybulbtemp_min ~ I(sealevelpressure_avg - min(sealevelpressure_avg, na.rm = TRUE)), data = us_weather) coef(sealevel_reg_min) ## (Intercept) ## 49.17705 ## I(sealevelpressure_avg - min(sealevelpressure_avg, na.rm = TRUE)) ## -21.05011 sealevel_reg_max &lt;- lm(drybulbtemp_min ~ I(sealevelpressure_avg - max(sealevelpressure_avg, na.rm = TRUE)), data = us_weather) coef(sealevel_reg_max) ## (Intercept) ## 11.28686 ## I(sealevelpressure_avg - max(sealevelpressure_avg, na.rm = TRUE)) ## -21.05011 Two main takeaways from these two new models. First, the relationship between the minimum temperature and the sea level pressure does not change. The average change in the minimum temperature for a unit change of sea level pressure is still 21 degrees Fahrenheit and the relationship is always negative. The element that changes is the y-intercept. For the minimum centered model, the y-intercept is 49.18 degrees Fahrenheit. The interpretation is that when the sea level pressure has a value of 0 (ie., a value of 29.02 in the original metric), the average minimum temperature is 49.18 degrees Fahrenheit. Similarly, the maximum centered model has a y-intercept of 11.29. This represents the average minimum temperature when the sea level pressure is 0 (ie., a value of 30.82 in the original metric). If the three different centered y-intercepts are compared, min-centered = 49.18, mean-centered = 27.63, and max-centered = 11.29. The mean-centered y-intercept is in between the minimum and maximum centered values, why is this occurring? This is happening due to the linear relationship that is assumed between the minimum temperature and the sea level pressure with the linear regression model. If a different functional form was assumed (ie., a curved relationship), there would likely be larger differences between the pairs of y-intercept values shown above. 7.2 Conditional Means To come … Although there is an assumption that the outcome and attribute are linearly related in linear regression, it is possible to specify the model to explore non-linear trends.↩︎ Dew point is the temperature to which air must be cooled to become saturated with water vapor. Wikipedia↩︎ Gestational days is the number of days that the baby is in the womb before being born, see gestation on Wikipedia↩︎ See atmospheric pressure on Wikipedia↩︎ "],["estimation-bootstrap-and-uncertainty.html", "Chapter 8 Estimation, Bootstrap and Uncertainty 8.1 Bootstrap and Uncertainty 8.2 Estimating Error or uncertainty", " Chapter 8 Estimation, Bootstrap and Uncertainty This section will explore how the estimates for the linear regression model are obtained. Within this section the characteristics of those estimates and the assumptions for the linear regression model will be explored. To come … 8.1 Bootstrap and Uncertainty Bootstrap and resampling methods can be used to estimate the variability in the estimated effects. This is needed as it is most common to have a subset of the entire population rather than the entire population, therefore the model estimates are approximations of the true population parameters. If another sample of data were obtained, the model estimates would be different from the previous sample. This would occur due to different individuals comprised within the sample. The goal of a good sample is to be able to get good estimates of the population parameters of interest. This happens when the sample obtained is as representative as possible of the population, most notably this can be gathered by collected the sample using a random process or ensuring that everyone in the population has the same random chance of being selected to be in the population. This is more challenging than one would expect where practical issues can often tarnish a good sampling plan. For instance, a portion of the population of interest may be difficult to gain access to or a portion of the population may be less likely to participate. These issues could bias the sample to be dissimilar from the population. For this book, we are going to assume that a representative sample is obtained from the population, but this needs to always be explored compared whenever data is gathered. For this chapter, the United States (US) weather data will be used again. The following code chunk loads the packages used in this chapter, sets the figure theme, and creates a few new attributes for the US weather data. library(tidyverse) library(ggformula) library(mosaic) library(broom) library(statthink) # Set theme for plots theme_set(theme_statthinking()) us_weather &lt;- mutate(us_weather, snow_factor = factor(snow), snow_numeric = ifelse(snow == &#39;Yes&#39;, 1, 0)) 8.2 Estimating Error or uncertainty To get some sense of the amount of error in the estimate of the linear slope, a bootstrap or resampling can be done to provide evidence for the likely range of slope values. This is an empirical approach to estimating uncertainty and the sampling process will be simulated many times. Upon every new data after resampling, the estimate for the linear slope will be estimated. The bootstrap/resampling will take the following general steps: Resample the observed data available, with replacement Fit the linear regression model, or more generally any statistic or statistical model. Save the coefficient of interest. Repeat steps 1 - 3 many times Explore the distribution of the coefficient from the many resampled data sets. 8.2.1 Resample with replacement Resampling the data with replacement, is the step in the bootstrap/resampling procedure that is needed to ensure that the estimates obtained are accurate and that there are different samples obtained throughout the process. Before moving on, this idea is worth expanding on to understand the differences and what exactly sampling with replacement means. To do this, a simple example of fruit names will be used. An object is created with code below that has 7 fruits in it. The data also contain a made up popularity measure that shows how much individuals like each fruit. These are also printed to show that these elements each show up in the object exactly one time. fruit &lt;- data.frame( names = c(&#39;apple&#39;, &#39;banana&#39;, &#39;kiwi&#39;, &#39;orange&#39;, &#39;plum&#39;, &#39;pear&#39;, &#39;kumquat&#39;), popularity = c(10, 25, 5, 15, 5, 8, 2) ) fruit ## names popularity ## 1 apple 10 ## 2 banana 25 ## 3 kiwi 5 ## 4 orange 15 ## 5 plum 5 ## 6 pear 8 ## 7 kumquat 2 Imagine for the setup of the resampling procedure, that these 7 fruit are placed inside a hat, box, or some container and fruit are not able to be seen. Then, to perform the resampling, someone would reach inside and select a single fruit at random. In this case, this would mean to also ignore the shape and size of the fruit as well to keep the process random. What happens next would be the difference between the two primary sampling procedures. For sampling with replacement, the single fruit that was selected will then be placed back into the hat or box. Once the selected fruit is returned, the individual would again reach in to select another fruit at random. The fruit would then be returned to the hat or box and the individual would select another fruit at random. This process would be repeated until the same size as the original data, in this case 7 fruits, were selected randomly. For sampling without replacement, the process is similar, except that when a fruit is selected from the box or hat it is not placed back to potentially be selected again. For example, if on the first selection with the 7 original fruit in the hat, an individual selects the kiwi, then this kiwi would not go back into the hat and could not be selected another time. Using the fruit example above, the sample_n() function can be used to generate random samples. The default behavior of this function is to do sampling without replacement. The primary argument for the function is the data to sample from and the second argument is the number of samples to perform. In this case, the number of samples will want to match the number of elements in the original data. The replace = FALSE is an explicit way to specify that the sampling should be done without replacement. set.seed(50) sample_n(fruit, 7, replace = FALSE) ## names popularity ## 1 kiwi 5 ## 2 orange 15 ## 3 banana 25 ## 4 plum 5 ## 5 kumquat 2 ## 6 apple 10 ## 7 pear 8 Notice that the data each show up 1 time, just like the original sample, however, the data are in a different order. This order is the order that the sampling algorithm selected each element, namely, the kiwi was selected first, then the orange, then the banana, and so on. Sampling with replacement is now shown below and is specified by saying, replace = TRUE. The process is done twice to show what happens if sample_n(fruit, 7, replace = TRUE) ## names popularity ## 1 orange 15 ## 2 banana 25 ## 3 kiwi 5 ## 4 plum 5 ## 5 plum 5 ## 6 kiwi 5 ## 7 orange 15 sample_n(fruit, 7, replace = TRUE) ## names popularity ## 1 banana 25 ## 2 kumquat 2 ## 3 plum 5 ## 4 banana 25 ## 5 kumquat 2 ## 6 orange 15 ## 7 apple 10 First, notice that each time the sampling algorithm is ran, a different set of fruits are returned. Also, notice that for each time, there are more than 1 of some of the fruit and some of the fruit do not show up. For example, in the first resampling with replacement, the orange, kiwi, and plum each show up 2 times, the banana shows up once and the remaining fruit do not show up. For the second resampling with replacement, the banana and kumquat show up 2 times and the plum, orange, and apple each show up 1 time, but the other two fruit do not show up. This is how sampling with replacement works and differs from sampling without replacement here, namely, each element could show up more than one time and that each time the sampling algorithm is performed, a different sample is obtained. Sampling without replacement is commonly done for the selection of subjects from a population of interest. However, this procedure would not work well for the bootstrap/resampling procedure to estimate uncertainty because sampling with replacement would always produce the same sample. That is, each element of the original sample would be selected 1 time, therefore, the sample would be the same every time and the estimated statistic of interest would always be the same. This will be explored more in the next section. 8.2.2 Calculate statistic of interest If these elements are saved, we could also compute the mean of the popularity column. This would represent The mean of the original sample is 10. Performing sampling without replacement, the mean will be the same as the elements within the sample will be the same as the original sample. Notice that the mean computed below with the df_stats() function is the same as the original data. set.seed(50) sample_n(fruit, 7, replace = FALSE) %&gt;% df_stats(~ popularity, mean) ## response mean ## 1 popularity 10 However, performing sampling with replacement, these means will differ as the elements within the sample will differ. Notice below that both means are slightly larger than 10, but more importantly they differ from the original sample mean. These represent 2 of a total 8.2354310^{5} total resampled values that could be obtained. Even though this is almost a million possible resampled data samples that could be obtained, we can get a large number of these to see how much variation there can be in these statistics. sample_n(fruit, 7, replace = TRUE) %&gt;% df_stats(~ popularity, mean) ## response mean ## 1 popularity 10.71429 sample_n(fruit, 7, replace = TRUE) %&gt;% df_stats(~ popularity, mean) ## response mean ## 1 popularity 12 8.2.3 Replicating the resampling/bootstrap The process of resampling without replacement and computing the statistic of interest can be combined into a single operation by creating a function that does both steps at once. This function does those steps in the second and third line of the function. You could read this function as the following, first take the data, then do the resampling of the same length (ie., number of observation) and with replacement, and finally compute the mean of the popularity attribute. fruit_boot &lt;- function(...) { fruit %&gt;% sample_n(nrow(fruit), replace = TRUE) %&gt;% df_stats(~ popularity, mean) } The resampling procedure can then be replicated a single time by calling this function directly. The function does not need an argument, rather the function directly calls the fruit data and the popularity attribute. Some additional programming would be needed to generalize this resampling/bootstrap function to work on other data. To show that the function is working as intended, that is sampling with replacement, the function is ran twice. fruit_boot() ## response mean ## 1 popularity 14.71429 fruit_boot() ## response mean ## 1 popularity 10.57143 Different mean values are generated, suggesting that the sampling with replacement is working properly. Compiling the different mean values would be tedious to do by hand, fortunately, the function can be replicated many times using the computer with the map_dfr() function. This function takes the number of times to be replicated in the form of 1:replications where replications is replaced with the number of times to replicate the function. popularity_stats &lt;- map_dfr(1:10000, fruit_boot) Once these are generated, the distribution of the mean fruit popularity can directly be shown using a density curve. Figure 8.1 shows this distribution which has a minimum value of 3.29 to a maximum value of ’r round(max(popularity_stats$mean), 2)` with a mean of 9.97. This indicates a range of possible values that the mean fruit popularity falls and helps to estimate the amount of uncertainty in the statistic of interest. In general, ranges that are smaller indicate less uncertainty and more accurate estimate for the statistic of interest. However, the overall range of a distribution can be misleading and by itself is not a great measure of variation. As can be seen in Figure 8.1, there is a slight positive skew in the distribution, therefore other statistics of variation are often better to truly represent the variation in the distribution. gf_density(~ mean, data = popularity_stats) %&gt;% gf_labs(x = &quot;Mean Popularity&quot;) Figure 8.1: Density curve of the mean fruit popularity based on 10,000 resampled data sets. These statistics could be the IQR, standard deviation, or differences between other percentiles that are deemed meaningful (ie., the difference between the 10th and 90th percentiles). In these metric, the amount of variation would be, IQR = 3.71, SD = 2.72, and difference between 10th and 90th percentiles = 7. These would be a better representation of the amount of uncertainty for this distribution and represent the range of plausible values for the distribution. For example, the SD shows that the on average most of the mean fruit popularity statistics would fall about 2.72 units away from the mean of 9.97. 8.2.4 Linear regression bootstrap/resampling example An example of using the bootstrap/resampling methods with the US weather data will be explored next. In chapter 7, the minimum temperature was predicted or explained using the average daily dew point. As a reminder, here is the model again. Recall that the intercept here represents the average minimum temperature (in Fahrenheit) for an average daily dew point of 0 and that the slope term is the average increase in minimum temperature for every one unit increase in the average daily dew point. temp_lm &lt;- lm(drybulbtemp_min ~ dewpoint_avg, data = us_weather) coef(temp_lm) ## (Intercept) dewpoint_avg ## 4.8343103 0.8999469 The one element that was not explored in Chapter 7 is the amount of uncertainty, error, or variation in these estimates from the linear regression model. As shown in this chapter, the bootstrap or resampling methods can be an empirical way to try to capture the uncertainty. The steps to conduct the bootstrap or resample in this situation will be as follows. Resample, with replacement, from the original US weather data Fit the linear regression model with minimum temperature and average dew point Save the regression coefficients Repeat steps 1 - 3 many times (ie., 10,000) Visualize distribution of save regression coefficients across replications The following function aims to do steps 1 through 3 above. First, it resamples the data with replacement of the same size as the original data. Then, using the resampled data, the linear regression model is fitted and the model coefficients are saved. The function is also ran one time to show the estimated coefficients from the resampled data. Notice how these differ from the regression model fitted above to the original data, why do these differ here? set.seed(2021) resample_weather &lt;- function(...) { weather_resample &lt;- us_weather %&gt;% sample_n(nrow(us_weather), replace = TRUE) weather_resample %&gt;% lm(drybulbtemp_min ~ dewpoint_avg, data = .) %&gt;% coef(.) %&gt;% broom::tidy() } resample_weather() ## # A tibble: 2 × 2 ## names x ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 4.80 ## 2 dewpoint_avg 0.904 Now that there is a function that does steps 1 - 3, these processes can now be repeated many times with the map_dfr() function. In this example, the resampling is done 10,000 times and the coefficients from each of those are saved. The distribution for the intercept and the linear slope are visualized with density curves in Figure 8.2. This figure shows that the regression slopes (right-most) figure, are distributed closely together with most of the slopes ranging from about 0.89 to 0.91. There are some that are smaller than 0.89 or larger than 0.91, but these occur less frequently than between those two values. A similar interpretation can be made for the intercept where the majority of the intercepts fall between about 4.5 and 5.25 or so. weather_coef &lt;- map_dfr(1:10000, resample_weather) gf_density(~ x, data = weather_coef) %&gt;% gf_facet_wrap(~ names, scales = &#39;free&#39;) %&gt;% gf_labs(x = &quot;Regression Estimates&quot;) Figure 8.2: Distribution of regression coefficients from the bootstrapped samples. To get a more accurate sense of where notable percentiles are found, these can be computed with the df_stats() function directly. The quantile() function helps compute the percentiles of interest with the 5th, 50th (median), and 95th percentiles asked for in the code shown below. The difference between the 95th and 5th percentiles gives evidence for where the majority of the regression coefficients fall. These could be shown using violin plots for a visual depiction of the distributions with percentiles (see Figure 8.3. weather_coef %&gt;% df_stats(x ~ names, quantile(c(0.05, 0.5, 0.95))) ## response names 5% 50% 95% ## 1 x (Intercept) 4.3562898 4.8390742 5.3250096 ## 2 x dewpoint_avg 0.8844869 0.8997507 0.9149627 gf_violin(x ~ 1, data = weather_coef, fill = &#39;gray85&#39;, draw_quantiles = c(0.05, 0.5, 0.95)) %&gt;% gf_facet_wrap(~ names, scales = &#39;free&#39;) %&gt;% gf_refine(coord_flip()) Figure 8.3: Violin plots showing the distribution of regression coefficients from the bootstrapped samples, with percentiles. 8.2.5 Determining if a statistic is “significant” To come … "],["linear-regression-with-categorical-predictors.html", "Chapter 9 Linear Regression with Categorical Predictors 9.1 Categorical Predictor(s) 9.2 More than two categorical groups 9.3 Exploring Model Fit 9.4 Multiple Regression", " Chapter 9 Linear Regression with Categorical Predictors In previous chapters, linear regression has only included a continuous attribute to help predict or explain variation in a continuous outcome. In previous models from chapter 7 and 8, linear regression models were considered that tried to explain variation in the minimum temperature with the sea level pressure and the average dew point. With both of these models, linear regression estimated how much the minimum temperature changed with a one unit increase for the predictor attribute (eg., either sea level pressure or the average dew point. What happens when a categorical predictor is used instead of a continuous predictor? For example, in the US weather data that has been used so far, one categorical attribute would be whether it snowed or rained on a particular day. In this case, these represents categories rather than continuous attributes that take on many different data points. This chapter will explore using linear regression with a categorical attribute to predict/explain variation in the outcome. To mimic other chapters, the outcome attribute will be kept the same, the minimum temperature. First, a single categorical attribute with two categories will be explored. Then, a linear regression model with two terms will be explored, including one that is continuous and another that is categorical. Finally, the idea of a statistical interaction will be introduced and explored within a linear regression model. library(tidyverse) library(ggformula) library(mosaic) library(rsample) library(statthink) library(broom) # Set theme for plots theme_set(theme_statthinking()) us_weather &lt;- mutate(us_weather, snow_factor = factor(snow), snow_numeric = ifelse(snow == &#39;Yes&#39;, 1, 0)) 9.1 Categorical Predictor(s) Precipitation can impact the temperature on a given day. For example, days that are sunny are often warmer than days that are cloudy or rainy. Similarly, days that snow often are colder for the precipitation to stay frozen as it falls from the clouds. Therefore, to show how categorical predictors can be entered into a linear regression model, differences in the minimum temperature will be explored based on whether it snows or not on a given day. Take a minute or so to hypothesize potential differences in the minimum temperature for days in which it snows. Prior to fitting the linear regression model, it is first common to descriptively explore the distribution of the outcome by the different groups of the categorical attribute. This descriptive analysis can help to identify similarities or differences in center, variation, extreme cases, or other features across the two groups. For example, are there descriptive differences in the distribution of minimum temperature for days in which it snows vs does not snow? There are numerous types of figures that can help explore this, but one way to explore this is through a violin plot. Below is the code to generate a violin plot depicting the distribution of minimum temperature for days in which it does snow vs does not snow. gf_violin(drybulbtemp_min ~ snow, data = us_weather, draw_quantiles = c(0.1, 0.5, 0.9), fill = &#39;gray85&#39;) %&gt;% gf_refine(coord_flip()) %&gt;% gf_labs(y = &#39;Minimum Temperature (in F)&#39;, x = &#39;Snowed?&#39;) gf_violin(drybulbtemp_min ~ snow, data = us_weather, draw_quantiles = c(0.1, 0.5, 0.9), fill = &#39;gray85&#39;) %&gt;% gf_refine(coord_flip()) %&gt;% gf_labs(y = &#39;Minimum Temperature (in F)&#39;, x = &#39;Snowed?&#39;) %&gt;% plotly::ggplotly() Figure 9.1: Violin plots showing the distribution of minimum temperature by whether it has snowed on a day or not. Figure 9.1 shows the two violin plots, the one on top is for days in which it does snow and the one on the bottom is for days in which it does not snow. What notable feature differences do you notice in this figure? One element that stands out is the center differences in the median of the two groups, depicted by the middle vertical line in each violin plot. For days that it does snow, the median minimum temperature is just under 25 degrees whereas this is around 30 degrees for days in which it does not snow. Another notable difference is the shape of the two distributions. For days in which it snows (top violin plot), the distribution is left/negative skewed, whereas the distribution for days in which it does snow is much more symmetric. This makes some sense theoretically as days in which it does snow likely would have lower temperatures to ensure the precipitation does stay frozen as it falls. Furthermore, there may be some slight evidence of differences in variation across the two distributions with the minimum temperatures being more condensed for days in which it does snow compared to days it does not snow. Finally, both seem to have some extreme values on the lower end of the distribution, around or below -25 degrees Fahrenheit. However, for days in which it does not snow, there are also some extreme values on the positive end where minimum temperatures are close to 70 degrees Fahrenheit. Although the range is not the best measure of variation, it can be helpful to descriptively explore differences in extreme values in the distributions. In addition to the visualization, computing descriptive statistics can also be helpful. These can provide a more accurate values for the center, variation, and can also highlight sample size in each group. The df_stats() function can be used to compute statistics of interest. Below, these statistics were computed into three different groups. First, the mean and median were computed to represent the center of the distribution, then the standard deviation and IQR related to variation were computed, and finally information on the minimum and maximum values as well as the sample size of each group with the length() function. us_weather %&gt;% df_stats(drybulbtemp_min ~ snow, mean, median, sd, quantile(c(0.25, 0.75)), min, max, length) ## response snow mean median sd 25% 75% min max length ## 1 drybulbtemp_min No 30.98119 32 13.16565 24 39 -31 71 2392 ## 2 drybulbtemp_min Yes 19.63095 23 12.32797 13 29 -30 38 1008 The descriptive statistics show much of the same picture as the visualization, but provide specific values to confirm the initial thoughts based on Figure 9.1. In particular, the center for days in which it snows is lower for the mean and median by about 10 degrees Fahrenheit. Secondly, it was noted that there may be less variation for days in which it did snow, but the descriptive statistics (standard deviation and IQR) show that these values are similar across the two distributions. The upper end of the distribution for days in which it snows does appear to be more condensed from Figure 9.1 however. Finally, the minimum and maximum values mimic the sentiments from the violin plot and it should be noted that there are about half the number of days in which it snows compared to days in which it does not snow. 9.1.1 Linear Regression - Categorical Predictor Performing a linear regression with a categorical attribute works programmatically just like a linear regression with a continuous attribute. More specifically, the same function is used, lm(), and the specification of the attributes in the model formula are the same. The code below fits the linear regression with the snow attribute as the sole categorical attribute that helps to explain variation in the minimum temperature. The coefficients associated with the linear regression are extracted and printed with the coef() function. temp_snow_reg &lt;- lm(drybulbtemp_min ~ snow, data = us_weather) coef(temp_snow_reg) ## (Intercept) snowYes ## 30.98119 -11.35023 The output shows two parameters being estimated just like before. One is the y-intercept and is interpreted the same as with a continuous attribute. This term would represent the average minimum temperature when all of the attributes in the model are 0. What is not clear is what value is 0 from the model above. This will become more clear once the interpretation for the slope term is expanded upon next. Before expanding on this, the linear slope term is interpreted the same as well. For example, the slope term shown above is still interpreted as the change in the outcome for a one unit increase in the predictor attribute. For the weather data example, this would mean that the linear slope coefficient of -11.35 indicates that the average minimum temperature decreases by about -11.35 degrees Fahrenheit for a one unit increase in the snow attribute. Similar to the intercept, it is not intuitive or clear as to what a one unit increase would represent here as the snow attribute represents categories rather than a continuous attribute. 9.1.1.1 Interpreting the Linear Slope for a Categorical Attribute To explore what these coefficients mean in a bit more detail, let’s look at the data a bit more and how the linear regression uses the categorical attribute. In the model internals, the categorical attribute is converted from the category names (ie., Yes vs No) to a numeric representation of those categories. By default, the numbers used to represent the categories used are 0 and 1. Table 9.1 shows this default numeric representation that R would use. R uses the category that is closer to the letter “A” for the number 0. The category that uses the value of 0 is referred to as the reference category in statistics. knitr::kable( data.frame(snow = c(&#39;No&#39;, &#39;Yes&#39;), snow_numeric = c(0, 1)), caption = &quot;Conversion of categories to numeric representation.&quot; ) Table 9.1: Conversion of categories to numeric representation. snow snow_numeric No 0 Yes 1 Within the data, there is an attribute called snow_numeric that follows the logic shown in Table 9.1. The count() function below shows these two attributes again and also show the number of observations or sample size for each group. As can be seen, the number of days in which it does not snow is over double those days in which it does snow. count(us_weather, snow, snow_numeric) ## # A tibble: 2 × 3 ## snow snow_numeric n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 No 0 2392 ## 2 Yes 1 1008 To understand the interpretation of the linear regression coefficient for the snow attribute shown above (ie., the estimated slope), a new linear regression model is fitted that uses the numeric representation directly rather than the categorical representation. Reference back to Table 9.1 to show what the numeric representation of the categorical element is. For days in which it snows, the numeric attribute would have a value of 1, whereas for days in which it does not snow, the numeric attribute would have a value of 0. snow_reg_new &lt;- lm(drybulbtemp_min ~ snow_numeric, data = us_weather) coef(snow_reg_new) ## (Intercept) snow_numeric ## 30.98119 -11.35023 Notice that the coefficients for the linear regression are the same no matter which attribute is entered into the model. When a categorical attribute is entered into the regression in R, the attribute is automatically converted into something called an indicator or dummy variable. This means that one of the two values are represented with a 1, the other with a 0. The value that is represented with a 0 is the one that is closer to the letter “A”, meaning that the 0 is the first category in alphabetical order. As mentioned earlier, the linear slope in this model indicates the change in the outcome for a one unit increase in the predictor attribute. In this example, this means that for a one unit change in the snow attribute (or snow numeric attribute) the average minimum temperature decreased by -11.35 degrees Fahrenheit. More specifically though, since there is only a single unit change for the snow numeric attribute, the one unit change can also be interpreted as a change in the categories. Therefore, the one unit change is the average change in the temperature moving from a 0 to a 1 or from a day in which it did not snow to a day in which it did snow. The descriptive statistics and the coefficients from the regression are shown together below. Compare the difference in the mean statistics from the descriptive statistics below. How does this related to the slope from the linear regression with a categorical attribute. us_weather %&gt;% df_stats(drybulbtemp_min ~ snow, mean, median, sd, quantile(c(0.25, 0.75)), min, max, length) ## response snow mean median sd 25% 75% min max length ## 1 drybulbtemp_min No 30.98119 32 13.16565 24 39 -31 71 2392 ## 2 drybulbtemp_min Yes 19.63095 23 12.32797 13 29 -30 38 1008 coef(temp_snow_reg) ## (Intercept) snowYes ## 30.98119 -11.35023 More specifically, the linear slope here can be computed from the descriptive statistics as the mean minimum temperature for days in which it does snow minus the mean minimum temperature for days in which is does not snow. Mathematically, this can be represented with the following relationship and computation. In the equation below, \\(\\bar{Y}_{Yes}\\) represents the mean minimum temperature for days in which it does snow and \\(\\bar{Y}_{No}\\) represents the mean minimum temperature for days in which it does not snow. \\[ slope = \\bar{Y}_{Yes} - \\bar{Y}_{No} \\\\ slope = 19.63 - 30.98 = -11.35 \\] Finally, circling back to the interpretation of the y-intercept. This term is interpreted as the average value of the outcome when all the terms in the linear regression are equal to zero. In this case, there is a single categorical attribute included in the model, whether it snows or not. From Table 9.1, the numeric representation of the categories shows that days in which it does not snow is represented with a value of 0, this category would also be referred to as the reference group. Therefore, the y-intercept (or more generally the intercept), would equal the mean minimum temperature for days in which it does not snow. Notice how the intercept coefficient from the linear regression and the mean minimum temperature for days in which it does not snow are the same value. Of final note, although this was fitted with a linear regression, this procedure is equivalent to a two sample independent t-test. We find the unified framework of conducting tests like this using the linear regression model provides an introduction that is easier to extend as the comfort level with statistics increases. Furthermore, the inferential procedures with the resampling/bootstrap techniques discussed next and in the previous chapter will remain the same no matter how complicated the linear regression model becomes. 9.1.2 Inference Similar to the continuous predictor, resampling/bootstrapping takes a similar method compared to linear regression with a single categorical predictor. More specifically, the same general steps that were outlined in chapter 8 are done again here. These steps are outlined below for this specific example. Resample the observed data available, with replacement. Fit the same linear regression model as above. Save the slope coefficient representing the mean difference in minimum temperature between days that it snows or does not snow. Repeat steps 1 - 3 many times. Explore the distribution of slope estimates from the many resampled data sets. The only difference in this example compared to the one outlined in chapter 8 is the linear regression model that is being fitted. In this example, the model is somewhat different to include a categorical predictor rather than a continuous predictor. Therefore, the interpretation of the linear slope term differs, however the framework is still the same. Resample data, fit the linear regression model, and then save the regression coefficients. The first three steps outlined above are performed in the following code. The original data are resampled of the same size as the original data, with replacement. Then, the linear regression model is fitted with the snow attribute as the sole predictor helping to explain differences in the minimum temperature. Then, the tidy() function is used to extract the coefficients of interest. This process is saved to the function, resample_snow(); then this function is processed one time. resample_snow &lt;- function(...) { snow_resample &lt;- us_weather %&gt;% sample_n(nrow(us_weather), replace = TRUE) snow_resample %&gt;% lm(drybulbtemp_min ~ snow, data = .) %&gt;% tidy() } resample_snow() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 31.3 0.260 120. 0 ## 2 snowYes -12.0 0.482 -24.8 3.27e-125 Notice from the single instance of running the resampling/bootstrapping function that the coefficients estimated are different from the linear regression using the original data. This shouldn’t be surprising given that the data were resampled. This means that the data being used to estimate the model coefficients are different than the original data. Therefore, the coefficient estimates are different. However, the goal of the resampling/bootstrapping procedure is to estimate how much uncertainty would be expected if the sample was obtained again. Getting another sample in the real world would be costly, can take significant time, and is usually not done. The resampling/bootstrapping procedure however, aims to replicate this process using computation. To do this, steps 1 - 3 of the resampling/bootstrapping process is repeated many times. This process of repetition allows the uncertainty found in the coefficients to be estimated. The resampling function written above is replicated 10,000 times below. The resulting estimates are visualized with a density plot shown in Figure 9.2. There will be one density figure representing the 10,000 estimates for the intercept and another density figure for the 10,000 estimates for the linear slope (ie., mean difference in minimum temperatures for days in which it snows compared to when it does not snow). snow_coef &lt;- map_dfr(1:10000, resample_snow) gf_density(~ estimate, data = snow_coef) %&gt;% gf_facet_wrap(~ term, scales = &#39;free_x&#39;) %&gt;% gf_labs(x = &quot;&quot;) snow_coef &lt;- map_dfr(1:10000, resample_snow) gf_density(~ estimate, data = snow_coef) %&gt;% gf_facet_wrap(~ term, scales = &#39;free_x&#39;) %&gt;% gf_labs(x = &quot;&quot;) %&gt;% plotly::ggplotly() Figure 9.2: Density figures of the resampled/bootstrapped linear regression estimates. Interpreting Figure 9.2 is similar to other density figures first explored in Chapter 2, however the primary difference here is that the density does not represent observed data. Instead, these density figures are representing 10,000 different estimates for the intercept and linear slope from the regression model. These estimates differ as part of the resampling process due to different data being used to estimate the intercept and slope coefficients. Notice from the left-hand side of Figure 9.2 that the center of the distribution of intercepts is around 31. There is some variation in these estimates. This can be explored explicitly by looking at how wide the middle 90% of the distribution is. This will be computed more explicitly below, but this can be estimated from the figure where it appears most of the intercept values fall between about 30.5 and 31.5. Recall what the intercept represents here, the intercept is the mean minimum temperature when all the predictor terms are 0. For this model with a single categorical attribute included, the intercept is the mean minimum temperature for days in which it does not snow. Therefore, the average minimum temperature for these locations for days in which it does not snow is likely between 30.5 and 31.5 degrees Fahrenheit. Moving to the right-hand side of Figure 9.2, the center of the distribution of linear slopes is around -11. There is also some evidence of variation in the linear slopes. Most of the slope estimates seem to fall between -12 and -10.5 degrees Fahrenheit. Recall that for a categorical attribute, the linear slope represents the mean change between the two categories. Since the intercept is the average minimum temperature for days in which it does not snow, the linear slope represents the change in average minimum temperatures moving from days it does not snow to days in which it does snow. More explicitly, the linear slope here says that the average minimum temperature is about 12 to 10.5 degrees Fahrenheit cooler than days in which it does not snow. Computing specific descriptive statistics can also be helpful to supplement the distributions shown in Figure 9.2. The df_stats() function can do this and these statistics are computed separately for the intercept and linear slope. The 5th, 50th (median), and 95% percentiles are computed for each term. The output shows similar statements that were estimated from the density figures. These statistics will be used in the next section to provide evidence for or against the hypotheses of interest. snow_coef %&gt;% df_stats(estimate ~ term, quantile(c(0.05, 0.5, 0.95))) ## response term 5% 50% 95% ## 1 estimate (Intercept) 30.54017 30.98197 31.42003 ## 2 estimate snowYes -12.13849 -11.34542 -10.57182 9.1.2.1 How to tell if an effect is “significant” To come … 9.2 More than two categorical groups Categorical attributes are not limited to the simply two groups. For example, earlier in the book when the college scorecard data was used, the distributions of admission rate was explored by the primary degree status. This attribute had three groups representing schools that primarily granted associate, bachelor, or certificate degrees. This section is going to explore how a linear regression can be generalized to an attribute that has more than two groups. Before exploring the linear regression with more than two categories or groups, first the college scorecard data are read in. Recall, that these data are for higher education institutions, so each row in the data below is for a single higher education institution. The subsequent columns are specific attributes about those higher education institutions, for example, their average admission rate, the median ACT score, the undergraduate enrollment, primary degree status, and a few other attributes. library(tidyverse) library(ggformula) library(mosaic) college_score &lt;- read_csv(&quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000) ## Rows: 2019 Columns: 17 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (6): instnm, city, stabbr, preddeg, region, locale ## dbl (11): adm_rate, actcmmid, ugds, costt4_a, costt4_p, tuitionfee_in, tuiti... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(college_score) ## # A tibble: 6 × 17 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama … Normal AL Bachel… South… City:… 0.903 18 4824 22886 ## 2 Universi… Birmi… AL Bachel… South… City:… 0.918 25 12866 24129 ## 3 Universi… Hunts… AL Bachel… South… City:… 0.812 28 6917 22108 ## 4 Alabama … Montg… AL Bachel… South… City:… 0.979 18 4189 19413 ## 5 The Univ… Tusca… AL Bachel… South… City:… 0.533 28 32387 28836 ## 6 Auburn U… Montg… AL Bachel… South… City:… 0.825 22 4211 19892 ## # … with 7 more variables: costt4_p &lt;dbl&gt;, tuitionfee_in &lt;dbl&gt;, ## # tuitionfee_out &lt;dbl&gt;, debt_mdn &lt;dbl&gt;, grad_debt_mdn &lt;dbl&gt;, female &lt;dbl&gt;, ## # bachelor_degree &lt;dbl&gt; 9.2.1 Explore distribution 3 groups The first step in any analysis commonly starts with exploring the attributes of interest visually. This is particularly true if this is new data that is being explored for the first time. The code below creates violin plots of the college admission rate by the primary degree status. The 10th, 50th, and 90th percentiles are shown by the vertical lines within each violin plot. These can help guide the exploration by showing the center (50th percentile; median) and variation (difference in the 10th and 90th percentiles). gf_violin(adm_rate ~ preddeg, data = college_score, fill = &#39;gray85&#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&gt;% gf_labs(x = &#39;Primary Degree Status&#39;, y = &#39;Admission Rate&#39;) %&gt;% gf_refine(coord_flip()) Figure 9.3: Violin plots of the college admission rates by the primary degree status. Exploring Figure 9.3 shows that there are some small differences in the center (median) of the admission rates across the three primary degree statuses. In particular, schools that are primarily bachelor degree granting schools have evidence of slightly lower admission rates compared to primarily associate and certificate degree granting schools. The variation (shown by the gap between the 10th and 90th percentiles) and shapes of the distributions are relatively similar, being left or negatively skewed. 9.2.2 Linear Regression with three categories/groups As there was small observed differences noted in the center of the admission rates across the three degree statuses, this can be explored more explicitly by fitting a linear regression model. This will make use of the lm() function in R and the formula is very similar to what was done before when the attribute only had two categories or groups. This also mimics the formula from the violin plot above. adm_model &lt;- lm(adm_rate ~ preddeg, data = college_score) coef(adm_model) ## (Intercept) preddegBachelor Degree preddegCertificate Degree ## 0.72296993 -0.05170254 0.02193828 Notice that now there are three coefficients from the linear regression now instead of the two before. There is still the y-intercept, but now there are two linear slope terms. What do these terms represent? The names of the terms may give some hint into what these terms may represent. To explore this issue, it is helpful to explore how these linear slope terms are created from the primary degree status attribute. Recall when there are two categories with an attribute, a dummy or indicator attribute is created by default where the categories are represented numerically. By default, one category is represented by a value of 0 and the second is represented by a value of 1 (see Table 9.1 for a reminder to how this was created for the snow attribute). When there are more than two categories or groups, it now takes more than one attribute to represent the categories in a similar fashion. Table 9.2 shows this default behavior in creating the dummy or indicator attributes that the linear regression uses internally. Notice that there need to be two attributes to fully represent all three categories from the original attribute. By default, the lm() function represents the category that is closer to the letter “A” to be the reference group. This category is represented solely by 0’s in the new attributes that are created and similar to the analysis with two categories or groups, the referent group is tied to the y-intercept. knitr::kable( data.frame(primary_degree = c(&#39;Associate&#39;, &#39;Bachelor&#39;, &#39;Certificate&#39;), bachelor = c(0, 1, 0), certificate = c(0, 0, 1)), caption = &quot;Default dummy/indicator attributes for an attribute with three categories/groups for a linear regression.&quot; ) Table 9.2: Default dummy/indicator attributes for an attribute with three categories/groups for a linear regression. primary_degree bachelor certificate Associate 0 0 Bachelor 1 0 Certificate 0 1 The two additional terms, labeled “bachelor” and “certificate” in Table 9.2, are the terms associated with the two linear slopes from the coefficients. The linear slope terms are interpreted as the change in the outcome for a one unit change in the attribute. For the two attributes in Table 9.2, there is only a one unit change possible and the one unit change indicates a movement from one category (the associate degree group) to either the bachelor or certificate degree groups. Therefore, as with the two category analysis, each linear slope is interpreted as the mean difference between the reference category (associate degree group here) and the category represented by a value of 1. Going back to the coefficients (the means for the categories are also shown for reference) from the linear regression, the y-intercept would represent the mean value for the associate degree group. This can be confirmed by looking at the results below, but theoretically, this occurs because the y-intercept represents the mean of the outcome when all terms in the regression are 0, this occurs for this model for those associate degree schools as shown in Table 9.2. The remaining two terms represent the change in admission rates for a one unit change in those attributes. The one with the bachelor degree in the name represents the change in admission rate comparing an associate degree school with a bachelor degree school. Similarly, the term with certificate degree in the name represents the change in admission rates comparing an associate degree school with a certificate degree school. If the differences in the mean values are computed, the values from the linear regression coefficients can be recreated. coef(adm_model) ## (Intercept) preddegBachelor Degree preddegCertificate Degree ## 0.72296993 -0.05170254 0.02193828 college_score %&gt;% df_stats(adm_rate ~ preddeg, mean) ## response preddeg mean ## 1 adm_rate Associate Degree 0.7229699 ## 2 adm_rate Bachelor Degree 0.6712674 ## 3 adm_rate Certificate Degree 0.7449082 By default, the linear regression model will not compare the mean difference between all pairwise groups. More specifically, the difference between the bachelor degree and certificate degree is not done. There are ways to do this, often referred to as contrasts, but these are not discussed here. If the difference between bachelor degree and certificate degree schools is of interest, the contrasts could be explored or the reference group could be modified. If the reference group is modified, then another comparison would be missing. The following shows what changing the reference group could have on the model. Table 9.3 shows what the changing of the reference group can be. In this example, the certificate degree group will serve as the reference group, that is, will be the group that is represented as all zeros. What impact will this have on the resulting linear regression model? Also, what mean difference will be missing from the results? knitr::kable( data.frame(primary_degree = c(&#39;Associate&#39;, &#39;Bachelor&#39;, &#39;Certificate&#39;), associate = c(1, 0, 0), bachelor = c(0, 1, 0)), caption = &quot;Modifying dummy/indicator attributes for an attribute with three categories/groups for a linear regression.&quot; ) Table 9.3: Modifying dummy/indicator attributes for an attribute with three categories/groups for a linear regression. primary_degree associate bachelor Associate 1 0 Bachelor 0 1 Certificate 0 0 The following code manually creates these two new attributes shown in Table 9.3. Then, the linear regression model is fitted with these two newly created attributes. college_score &lt;- college_score %&gt;% mutate(associate = ifelse(preddeg == &#39;Associate Degree&#39;, 1, 0), bachelor = ifelse(preddeg == &#39;Bachelor Degree&#39;, 1, 0)) adm_model_cert &lt;- lm(adm_rate ~ associate + bachelor, data = college_score) coef(adm_model_cert) ## (Intercept) associate bachelor ## 0.74490821 -0.02193828 -0.07364082 Notice that the y-intercept differs from before and represents the mean of the certificate degree group. The two linear slope terms represent the mean difference between the associate and bachelor degree group compared to the associate degree group. The missing mean difference comparison is that between the associate degree and bachelor degree groups. 9.2.3 Inference for model with three groups To come … 9.3 Exploring Model Fit 9.3.1 R-Squared summary(adm_model)$r.squared ## [1] 0.01404376 9.3.2 Residual Standard Error summary(adm_model)$sigma ## [1] 0.2099718 9.4 Multiple Regression To come … 9.4.1 Distribution of Effects To come … 9.4.2 Interactions To come … 9.4.3 Evaluating model fit To come … "],["prediction-for-individuals.html", "Chapter 10 Prediction for individuals 10.1 Comparison of classification / linear model 10.2 Compared linear model with median", " Chapter 10 Prediction for individuals 10.1 Comparison of classification / linear model 10.2 Compared linear model with median 10.2.1 Skewed Data - Inference In one example, a skewed distribution was transformed prior to conducting the analysis with a regression tree. Another approach could be to use a more robust statistic such as the median. One limitation of the median, is that a linear regression model as we have covered so far, does not allow you to fit the model while using the median. library(tidyverse) library(ggformula) library(mosaic) library(broom) library(statthink) # Set theme for plots theme_set(theme_statthinking()) college_score &lt;- read_csv(&quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000) head(college_score) ## # A tibble: 6 × 17 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama … Normal AL Bachel… South… City:… 0.903 18 4824 22886 ## 2 Universi… Birmi… AL Bachel… South… City:… 0.918 25 12866 24129 ## 3 Universi… Hunts… AL Bachel… South… City:… 0.812 28 6917 22108 ## 4 Alabama … Montg… AL Bachel… South… City:… 0.979 18 4189 19413 ## 5 The Univ… Tusca… AL Bachel… South… City:… 0.533 28 32387 28836 ## 6 Auburn U… Montg… AL Bachel… South… City:… 0.825 22 4211 19892 ## # … with 7 more variables: costt4_p &lt;dbl&gt;, tuitionfee_in &lt;dbl&gt;, ## # tuitionfee_out &lt;dbl&gt;, debt_mdn &lt;dbl&gt;, grad_debt_mdn &lt;dbl&gt;, female &lt;dbl&gt;, ## # bachelor_degree &lt;dbl&gt; adm_model &lt;- lm(adm_rate ~ preddeg, data = college_score) coef(adm_model) ## (Intercept) preddegBachelor Degree preddegCertificate Degree ## 0.72296993 -0.05170254 0.02193828 Prior to doing the median, we can bootstrap the mean difference from the model above. resample_admrate &lt;- function(...) { college_resample &lt;- college_score %&gt;% sample_n(nrow(college_score), replace = TRUE) college_resample %&gt;% lm(adm_rate ~ preddeg, data = .) %&gt;% tidy(.) %&gt;% select(term, estimate) } resample_admrate() ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 0.749 ## 2 preddegBachelor Degree -0.0770 ## 3 preddegCertificate Degree -0.0270 admrate_coef &lt;- map(1:10000, resample_admrate) %&gt;% bind_rows() admrate_coef %&gt;% gf_density(~ estimate) %&gt;% gf_facet_wrap(~ term, scales = &#39;free_x&#39;) 10.2.2 Bootstrap Median he bootstrap for the median will take much of a similar process as before, the major difference being that a model will not be fitted. Instead, we will compute statistics for the median of each group, take differences of the median to represent the median difference between the groups and then replicate. Resample the observed data available, with replacement Estimate median for each group. Calculate median difference between the groups Repeat steps 1 - 3 many times Explore the distribution of median differences from the many resampled data sets. resample_admrate_median &lt;- function(...) { college_resample &lt;- college_score %&gt;% sample_n(nrow(college_score), replace = TRUE) med_est &lt;- college_resample %&gt;% df_stats(adm_rate ~ preddeg, median) %&gt;% pivot_wider(names_from = preddeg, values_from = median_adm_rate) names(med_est) &lt;- c(&quot;Associate&quot;, &quot;Bachelor&quot;, &quot;Certificate&quot;) med_est %&gt;% mutate(bachelor_associate = Bachelor - Associate, certificate_associate = Certificate - Associate, bachelor_certificate = Bachelor - Certificate) %&gt;% pivot_longer(Associate:bachelor_certificate, names_to = &quot;Term&quot;, values_to = &quot;Median_Difference&quot;) } resample_admrate_median() admrate_median &lt;- map(1:10000, resample_admrate_median) %&gt;% bind_rows() admrate_median %&gt;% filter(Term %in% c(&#39;bachelor_associate&#39;, &#39;certificate_associate&#39;, &#39;bachelor_certificate&#39;)) %&gt;% gf_density(~ Median_Difference) %&gt;% gf_facet_wrap(~ Term, scales = &#39;free_x&#39;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
