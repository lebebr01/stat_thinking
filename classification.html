<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Classification | Statistical Reasoning through Computation and R</title>
  <meta name="description" content="Chapter 5 Classification | Statistical Reasoning through Computation and R" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Classification | Statistical Reasoning through Computation and R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="true" />
  
  
  <meta name="github-repo" content="lebebr01/stat_thinking" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Classification | Statistical Reasoning through Computation and R" />
  
  
  

<meta name="author" content="Brandon LeBeau and Andrew S. Zieffler" />


<meta name="date" content="2020-09-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multivariate-visualization.html"/>
<link rel="next" href="linear-model.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.15/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-43425299-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-43425299-4');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Reasoning through Computation and R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#statistics-vs-data-science"><i class="fa fa-check"></i><b>1.1</b> Statistics vs Data Science</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#experiments-vs-observations"><i class="fa fa-check"></i><b>1.2</b> Experiments vs Observations</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#data-structure"><i class="fa fa-check"></i><b>1.3</b> Data Structure</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>2</b> Visualization</a>
<ul>
<li class="chapter" data-level="2.0.1" data-path="visualization.html"><a href="visualization.html#college-scorecard-data"><i class="fa fa-check"></i><b>2.0.1</b> College Scorecard Data</a></li>
<li class="chapter" data-level="2.0.2" data-path="visualization.html"><a href="visualization.html#view-the-data"><i class="fa fa-check"></i><b>2.0.2</b> View the Data</a></li>
<li class="chapter" data-level="2.1" data-path="visualization.html"><a href="visualization.html#exploring-attributes"><i class="fa fa-check"></i><b>2.1</b> Exploring Attributes</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="visualization.html"><a href="visualization.html#histograms"><i class="fa fa-check"></i><b>2.1.1</b> Histograms</a></li>
<li class="chapter" data-level="2.1.2" data-path="visualization.html"><a href="visualization.html#interpretting-histograms"><i class="fa fa-check"></i><b>2.1.2</b> Interpretting Histograms</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="visualization.html"><a href="visualization.html#plot-customization"><i class="fa fa-check"></i><b>2.2</b> Plot Customization</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="visualization.html"><a href="visualization.html#axes-labels"><i class="fa fa-check"></i><b>2.2.1</b> Axes labels</a></li>
<li class="chapter" data-level="2.2.2" data-path="visualization.html"><a href="visualization.html#plot-title-and-subtitle"><i class="fa fa-check"></i><b>2.2.2</b> Plot title and subtitle</a></li>
<li class="chapter" data-level="2.2.3" data-path="visualization.html"><a href="visualization.html#plot-theme"><i class="fa fa-check"></i><b>2.2.3</b> Plot theme</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="visualization.html"><a href="visualization.html#density-plots"><i class="fa fa-check"></i><b>2.3</b> Density plots</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics: Numerically Describing the Sample Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#summarizing-attributes"><i class="fa fa-check"></i><b>3.1</b> Summarizing Attributes</a></li>
<li class="chapter" data-level="3.2" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#understanding-the-median-and-mean"><i class="fa fa-check"></i><b>3.2</b> Understanding the Median and Mean</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#summarize-with-the-mean-or-median"><i class="fa fa-check"></i><b>3.2.1</b> Summarize with the Mean or Median?</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#numerically-summarizing-variation"><i class="fa fa-check"></i><b>3.3</b> Numerically Summarizing Variation</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#range"><i class="fa fa-check"></i><b>3.3.1</b> Range</a></li>
<li class="chapter" data-level="3.3.2" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#percentile-range"><i class="fa fa-check"></i><b>3.3.2</b> Percentile Range</a></li>
<li class="chapter" data-level="3.3.3" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#interquartile-range-iqr"><i class="fa fa-check"></i><b>3.3.3</b> Interquartile Range (IQR)</a></li>
<li class="chapter" data-level="3.3.4" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#empirical-cumulative-density"><i class="fa fa-check"></i><b>3.3.4</b> Empirical Cumulative Density</a></li>
<li class="chapter" data-level="3.3.5" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>3.3.5</b> Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#summarizing-categorical-attributes"><i class="fa fa-check"></i><b>3.4</b> Summarizing Categorical Attributes</a></li>
<li class="chapter" data-level="3.5" data-path="descriptive-statistics-numerically-describing-the-sample-data.html"><a href="descriptive-statistics-numerically-describing-the-sample-data.html#advanced-extension-computing-your-own-measure-of-variation"><i class="fa fa-check"></i><b>3.5</b> Advanced Extension: Computing Your Own Measure of Variation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multivariate-visualization.html"><a href="multivariate-visualization.html"><i class="fa fa-check"></i><b>4</b> Multivariate Visualization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multivariate-visualization.html"><a href="multivariate-visualization.html#multivariate-distributions"><i class="fa fa-check"></i><b>4.1</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="visualization.html"><a href="visualization.html#histograms"><i class="fa fa-check"></i><b>4.1.1</b> Histograms</a></li>
<li class="chapter" data-level="4.1.2" data-path="multivariate-visualization.html"><a href="multivariate-visualization.html#density-curves"><i class="fa fa-check"></i><b>4.1.2</b> Density Curves</a></li>
<li class="chapter" data-level="4.1.3" data-path="multivariate-visualization.html"><a href="multivariate-visualization.html#violin-plots"><i class="fa fa-check"></i><b>4.1.3</b> Violin Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multivariate-visualization.html"><a href="multivariate-visualization.html#faceting"><i class="fa fa-check"></i><b>4.2</b> Faceting</a></li>
<li class="chapter" data-level="4.3" data-path="multivariate-visualization.html"><a href="multivariate-visualization.html#multivariate-descriptive-statistics"><i class="fa fa-check"></i><b>4.3</b> Multivariate Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="multivariate-visualization.html"><a href="multivariate-visualization.html#adding-additional-groups"><i class="fa fa-check"></i><b>4.3.1</b> Adding additional groups</a></li>
<li class="chapter" data-level="4.3.2" data-path="multivariate-visualization.html"><a href="multivariate-visualization.html#adding-more-descriptive-statistics"><i class="fa fa-check"></i><b>4.3.2</b> Adding more descriptive statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>5</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1" data-path="classification.html"><a href="classification.html#topic-decision-trees"><i class="fa fa-check"></i><b>5.1</b> Topic: Decision Trees</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="classification.html"><a href="classification.html#fitting-a-classification-tree"><i class="fa fa-check"></i><b>5.1.1</b> Fitting a Classification Tree</a></li>
<li class="chapter" data-level="5.1.2" data-path="classification.html"><a href="classification.html#accuracy"><i class="fa fa-check"></i><b>5.1.2</b> Accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-model.html"><a href="linear-model.html"><i class="fa fa-check"></i><b>6</b> Linear Model</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-model.html"><a href="linear-model.html#regression-trees"><i class="fa fa-check"></i><b>6.1</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="linear-model.html"><a href="linear-model.html#visualize-distributions"><i class="fa fa-check"></i><b>6.1.1</b> Visualize distributions</a></li>
<li class="chapter" data-level="6.1.2" data-path="linear-model.html"><a href="linear-model.html#decision-tree---regression-tree"><i class="fa fa-check"></i><b>6.1.2</b> Decision Tree - Regression Tree</a></li>
<li class="chapter" data-level="6.1.3" data-path="linear-model.html"><a href="linear-model.html#evaluating-accuracy"><i class="fa fa-check"></i><b>6.1.3</b> Evaluating accuracy</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="linear-model.html"><a href="linear-model.html#simple-regression-continuous-predictor"><i class="fa fa-check"></i><b>6.2</b> Simple Regression continuous predictor</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="linear-model.html"><a href="linear-model.html#description-of-the-data"><i class="fa fa-check"></i><b>6.2.1</b> Description of the Data</a></li>
<li class="chapter" data-level="6.2.2" data-path="linear-model.html"><a href="linear-model.html#scatterplots"><i class="fa fa-check"></i><b>6.2.2</b> Scatterplots</a></li>
<li class="chapter" data-level="6.2.3" data-path="linear-model.html"><a href="linear-model.html#fitting-a-linear-regression-model"><i class="fa fa-check"></i><b>6.2.3</b> Fitting a linear regression model</a></li>
<li class="chapter" data-level="6.2.4" data-path="linear-model.html"><a href="linear-model.html#explore-the-y-intercept"><i class="fa fa-check"></i><b>6.2.4</b> Explore the y-intercept</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="linear-model.html"><a href="linear-model.html#conditional-means"><i class="fa fa-check"></i><b>6.3</b> Conditional Means</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html"><i class="fa fa-check"></i><b>7</b> Estimation / Bootstrap / Uncertainty</a>
<ul>
<li class="chapter" data-level="7.1" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#estimating-error"><i class="fa fa-check"></i><b>7.1</b> Estimating Error</a></li>
<li class="chapter" data-level="7.2" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#categorical-predictors"><i class="fa fa-check"></i><b>7.2</b> Categorical Predictor(s)</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#mothers-smoking"><i class="fa fa-check"></i><b>7.2.1</b> Mother‚Äôs smoking</a></li>
<li class="chapter" data-level="7.2.2" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#linear-regression---categorical-predictor"><i class="fa fa-check"></i><b>7.2.2</b> Linear Regression - Categorical Predictor</a></li>
<li class="chapter" data-level="7.2.3" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#inference"><i class="fa fa-check"></i><b>7.2.3</b> Inference</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#more-than-2-categorical-groups"><i class="fa fa-check"></i><b>7.3</b> More than 2 categorical groups</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#explore-distribution-3-groups"><i class="fa fa-check"></i><b>7.3.1</b> Explore distribution 3 groups</a></li>
<li class="chapter" data-level="7.3.2" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#overall-model-fit"><i class="fa fa-check"></i><b>7.3.2</b> Overall model fit</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#multiple-regression"><i class="fa fa-check"></i><b>7.4</b> Multiple Regression</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#interactions"><i class="fa fa-check"></i><b>7.4.1</b> Interactions</a></li>
<li class="chapter" data-level="7.4.2" data-path="estimation-bootstrap-uncertainty.html"><a href="estimation-bootstrap-uncertainty.html#evaluating-model-fit"><i class="fa fa-check"></i><b>7.4.2</b> Evaluating model fit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="prediction-for-individuals.html"><a href="prediction-for-individuals.html"><i class="fa fa-check"></i><b>8</b> Prediction for individuals</a>
<ul>
<li class="chapter" data-level="8.1" data-path="prediction-for-individuals.html"><a href="prediction-for-individuals.html#comparison-of-classification-linear-model"><i class="fa fa-check"></i><b>8.1</b> Comparison of classification / linear model</a></li>
<li class="chapter" data-level="8.2" data-path="prediction-for-individuals.html"><a href="prediction-for-individuals.html#compared-linear-model-with-median"><i class="fa fa-check"></i><b>8.2</b> Compared linear model with median</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="prediction-for-individuals.html"><a href="prediction-for-individuals.html#skewed-data---inference"><i class="fa fa-check"></i><b>8.2.1</b> Skewed Data - Inference</a></li>
<li class="chapter" data-level="8.2.2" data-path="prediction-for-individuals.html"><a href="prediction-for-individuals.html#bootstrap-median"><i class="fa fa-check"></i><b>8.2.2</b> Bootstrap Median</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Reasoning through Computation and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Classification</h1>
<p>Classification is a task that tries to predict group membership using the data attributes available. For example, one could try to predict if an individual is left or right handed based on their preferences to music or hobbies. In this situation, the classification technique would look for patterns in the music or hobby preferences to help differentiate between those that are left or right handed. Perhaps the data would show that left handed individuals would be more likely to be artistic, therefore those that rated more highly artistic tasks would be more likely to be classified as left handed.</p>
<p>To perform the classification tasks in this chapter, we are going to consider a group of <strong>statistical models</strong> called decision trees, or more specifically in this case, classification trees. Statistical models are used to help us as humans understand patterns in the data and estimate uncertainty. Uncertainty comes from the variation in the data. For example, those that are left handed are likely not all interested or like artistic hobbies or tasks, but on average maybe they are more likely to enjoy these tasks compared to right handed individuals. Statistical models help us to understand if the differences shown in our sample of data are due to signal (true differences) or noise (uncertainty).</p>
<p>In the remaining sections of this chapter, we will build off of this idea of statistical models to understanding how these work with classification trees to classify. Furthermore, we will aim to develop heuristics to understand if our statistical model is practically useful. That is, does our model help us to do our classification task above just randomly guessing. We will use a few additional packages to perform the classification tasks, including <code>rpart</code>, <code>rpart.plot</code>, and <code>rsample</code>. The following code chunk loads all the packages that will be used in the current chapter.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="classification.html#cb93-1" aria-hidden="true"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb93-2"><a href="classification.html#cb93-2" aria-hidden="true"></a><span class="kw">library</span>(ggformula)</span>
<span id="cb93-3"><a href="classification.html#cb93-3" aria-hidden="true"></a><span class="kw">library</span>(statthink)</span>
<span id="cb93-4"><a href="classification.html#cb93-4" aria-hidden="true"></a><span class="kw">library</span>(rpart)</span>
<span id="cb93-5"><a href="classification.html#cb93-5" aria-hidden="true"></a><span class="kw">library</span>(rpart.plot)</span>
<span id="cb93-6"><a href="classification.html#cb93-6" aria-hidden="true"></a><span class="kw">library</span>(rsample)</span>
<span id="cb93-7"><a href="classification.html#cb93-7" aria-hidden="true"></a></span>
<span id="cb93-8"><a href="classification.html#cb93-8" aria-hidden="true"></a>remotes<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;grantmcdermott/parttree&quot;</span>)</span>
<span id="cb93-9"><a href="classification.html#cb93-9" aria-hidden="true"></a><span class="kw">library</span>(parttree)</span>
<span id="cb93-10"><a href="classification.html#cb93-10" aria-hidden="true"></a></span>
<span id="cb93-11"><a href="classification.html#cb93-11" aria-hidden="true"></a><span class="co"># Add plot theme</span></span>
<span id="cb93-12"><a href="classification.html#cb93-12" aria-hidden="true"></a><span class="kw">theme_set</span>(<span class="kw">theme_statthinking</span>())</span>
<span id="cb93-13"><a href="classification.html#cb93-13" aria-hidden="true"></a></span>
<span id="cb93-14"><a href="classification.html#cb93-14" aria-hidden="true"></a>us_weather &lt;-<span class="st"> </span><span class="kw">mutate</span>(us_weather, <span class="dt">snow_factor =</span> <span class="kw">factor</span>(snow))</span></code></pre></div>
<div id="topic-decision-trees" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Topic: Decision Trees</h2>
<p>We will continue to use the United States weather data introduced in Chapter 4. Given that this data was for the winter months in the United States, the classification task we will attempt to perform is to correct predict if it will snow on a particular day that precipitation occurs. To get a sense of how often it rains vs snows in these data, we can use the <code>count()</code> function to do this. For the <code>count()</code> function, the first argument is the name of the data, followed by the attributes we wish to count the number of observations in the unique values of those attributes.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="classification.html#cb94-1" aria-hidden="true"></a><span class="kw">count</span>(us_weather, rain, snow)</span></code></pre></div>
<pre><code>## [90m# A tibble: 4 x 3[39m
##   rain  snow      n
##   [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m
## [90m1[39m No    No     [4m1[24m571
## [90m2[39m No    Yes     728
## [90m3[39m Yes   No      821
## [90m4[39m Yes   Yes     280</code></pre>
<p>The following table counts the number of times that it rains or snows in the data. You may notice that there are days in which it does not rain or snow as shown by the row with No for both the rain and snow columns. There are also days in which it both rains and snows as shown in the row with Yes in both the rain and snow columns. Not surprisingly, a majority of the days it does not rain or snow, occurring about 46% of the time (<span class="math inline">\(1571 / (1571 + 728 + 821 + 280) = 46.2%\)</span>). Using similar logic, about 8% of the days in the data have both snow and rain.</p>
<!--
For the current classification, we will focus on days in which it either rains or snows instead of both or none. To do this, we will filter or restrict that data to those cases only, a data task that can be done with the `filter()` function. The `filter()` function works by selecting rows of data that match specific situations. This is similar to how a search engine, such as Google, works. In a search engine, you type is search criteria and the search engine gives you back matches that meet those criteria. The `filter()` function works similarly, we specify the criteria with which we hope to have data match to keep. 

In this example, we hope to keep the rows where it either rains or snows, but rows where it doesn't or both rains or snows. To do this, we will use the `|` and `&` operators which can be translated into "or" or "and" respectively. In English, the cases we want to keep are days in which it does not rain **and** it does snow *or* days in which it does rain **and** it does not snow. Turning this into data language that `filter()` can understand, we can substitute the `|` or `&` operators into places where or/and are in the English version along with the data attribute names. Therefore we could write in code, `(rain & snow) | (rain & snow)`. The last piece we need to add to the code version, is the values we want to retain for the attributes. For example, in the data, "No" means that the event did not occur and "Yes" means the event did occur. Therefore, translating the English version into code with data values would look like: `(rain == 'No' & snow == 'Yes') | (rain =='Yes' & snow == 'No')`. In R, the `==` means literal values, therefore the code `rain == 'No'` means the literal word "No" within the rain attribute. 


```r
us_weather_rs <- us_weather %>%
  filter((rain == 'No' & snow == 'Yes') | (rain == 'Yes' & snow == 'No'))

count(us_weather_rs, rain, snow)
```

```
## [90m# A tibble: 2 x 3[39m
##   rain  snow      n
##   [3m[90m<chr>[39m[23m [3m[90m<chr>[39m[23m [3m[90m<int>[39m[23m
## [90m1[39m No    Yes     728
## [90m2[39m Yes   No      821
```

We can further check if the filtering command worked by using the `count()` function. Using the `count()` function on the filtered data shows that we only retained rows of the data for the combinations that we wanted, namely days in which it there is some form of precipitation, but only if it snowed or rained, not both.
-->
<div id="fitting-a-classification-tree" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Fitting a Classification Tree</h3>
<p>Let‚Äôs class_tree our first classification tree to predict whether it snowed on a particular day. For this, we will use the <code>rpart()</code> function from the <em>rpart</em> package. The first argument to the <code>rpart()</code> function is a formula where the outcome of interest is specified to the left of the <code>~</code> and the attributes that are predictive of the outcome are specified to the right of the <code>~</code> separated with <code>+</code> signs. The second argument specifies the method for which we want to run the analysis, in this case we want to classify days based on the values in the data, therefore we specify <code>method = 'class'</code>. The final argument is the data element, in this case <code>us_weather</code>.</p>
<p>Before we fit the model, what attributes do you think would be predictive of whether it will rain or snow on a particular day during the winter months? Take a few minutes to brainstorm some ideas.</p>
<p>In this example, a handful of attributes to explore, including the average, minimum, and maximum temperature for the day. These happen to be all continuous attributes, meaning that these attributes can take many data values. The model is not limited to those types of data attributes, but that is where we will start the classification journey.</p>
<p>Notice that the fitted model is saved to the object, <code>class_tree</code>. This will allow for easier interaction with the model results later. Then after fitting the model, the model is visualized using the <code>rpart.plot()</code> function. The primary argument to the <code>rpart.plot()</code> function is the fitted model object from the <code>rpart()</code> function, here that would be <code>class_tree</code>. The additional arguments passed below adjust the appearance of the visualization.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="classification.html#cb96-1" aria-hidden="true"></a>class_tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(snow_factor <span class="op">~</span><span class="st"> </span>drybulbtemp_min <span class="op">+</span><span class="st"> </span>drybulbtemp_max, </span>
<span id="cb96-2"><a href="classification.html#cb96-2" aria-hidden="true"></a>   <span class="dt">method =</span> <span class="st">&#39;class&#39;</span>, <span class="dt">data =</span> us_weather)</span>
<span id="cb96-3"><a href="classification.html#cb96-3" aria-hidden="true"></a></span>
<span id="cb96-4"><a href="classification.html#cb96-4" aria-hidden="true"></a><span class="kw">rpart.plot</span>(class_tree, <span class="dt">roundint =</span> <span class="ot">FALSE</span>, <span class="dt">type =</span> <span class="dv">3</span>, <span class="dt">branch =</span> <span class="fl">.3</span>)</span></code></pre></div>
<div class="figure"><span id="fig:first-class-tree"></span>
<img src="05-classification-new_files/figure-html/first-class-tree-1.png" alt="Classification tree predicting whether it will snow or rain" width="672" />
<p class="caption">
Figure 5.1: Classification tree predicting whether it will snow or rain
</p>
</div>
<p>The visualization shown in Figure <a href="classification.html#fig:first-class-tree">5.1</a> produces the decision rules for the classification tree. The decision rules start from the top of the tree and proceed down the branches to the leaf nodes at the bottom that highlight the predictions. By default, the <code>rpart()</code> algorithm assumes that each split should go in two directions. For example, the first split occurs with the maximum temperature is less than 42 degrees Fahrenheit or greater than or equal to 42 degrees Fahrenheit. If the maximum temperature for the day is greater than or equal to 42 degrees Fahrenheit, the first split in the decision tree follows the left-most branch and proceeds to the left-most leaf node. This results in the prediction for those days as being days in which it does not snow (i.e., a category prediction of ‚ÄúNo‚Äù). The numbers below the ‚ÄúNo‚Äù label indicate that the probability of it snowing on a day where the maximum temperature was greater than or equal to 42 degrees Fahrenheit is 0.09 or about 9%. Furthermore, this category represents about 53% of the total number of data cases inputted.</p>
<p>Following the right-hand split of the first decision, which occurs for days when the maximum temperature is less than 42 degrees, we come to another split. This split is again for the maximum temperature, but now the split comes at 36 degrees Fahrenheit. In this case, if the temperature is greater than or equal to 36 degrees Fahrenheit, the decision leads to the next leaf node and a prediction that it will not snow that day. For this leaf node, there is more uncertainty in the prediction, where on average the probability of it snowing would be 0.42 or about 42%. This value is less than 50%, therefore the ‚ÄúNo‚Äù category is chosen. This occurs for about 16% of the data.</p>
<p>For days in which the maximum temperature is less than 36 degrees Fahrenheit, the decision tree moves to the right further and comes to another split. The third split in the decision tree is for the minimum daily temperature and occurs at 23 degrees Fahrenheit. For days where the minimum temperature is greater than 23 degrees Fahrenheit (but also had a maximum temperature less than 36 degree Fahrenheit), the right-most leaf node is predicted. For these data cases, about 8% of the total data, the prediction is that it will snow (i.e., ‚ÄúYes‚Äù category) and the probability of it snowing in those conditions is about 71%.</p>
<p>Finally, if the minimum temperature is less than 23 degrees Fahrenheit (but also had a maximum temperature less than 36 degree Fahrenheit), then one last split occurs on the maximum temperature at 29 degrees Fahrenheit. This leads to the last two leaf node in the middle of Figure <a href="classification.html#fig:first-class-tree">5.1</a>. One prediction states it will snow, for maximum temperature less than 29 degrees and one predicting it will not snow, for those greater than or equal to 29 degrees. Both of these leaf nodes have more uncertainty in the predictions, being close to 50% probability.</p>
<p>Note, that the average daily temperature was included in the model fitting procedure, but was not included in the results shown in Figure <a href="classification.html#fig:first-class-tree">5.1</a>. Why do you think this happened? The model results show the attributes that were helpful in making the prediction of whether it snowed or not. For this task, the model found that the maximum and minimum temperature attributes were more useful and adding the average daily temperature did not appreciably improve the predictions. For this reason, it did not show up in the decision tree. Furthermore, the attributes that are most informative in making the prediction are at the top of the decision tree. In the results shown in Figure <a href="classification.html#fig:first-class-tree">5.1</a>, the maximum daily temperature was the most helpful attribute in making the snow or not prediction.</p>
<p>The decision tree rules can also be requested in text form using the <code>rpart.rules()</code> function and are shown below. The rows in the output are the leaf nodes from <a href="classification.html#fig:first-class-tree">5.1</a> and the columns represent the probability of it snowing, the decision rules that are applicable, and the percentage of data found in each row. For example, for the first row, it is predicted to snow about 9% of the time when the maximum temperature for the day is greater than 42 and this occurred in 53% of the original data. Since the probability is less than 50%, the prediction would be that it would not snow on days with those characteristics. In rows where there are <code>&amp;</code> symbols, these separate different data attributes that are useful in the classification model.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="classification.html#cb97-1" aria-hidden="true"></a><span class="kw">rpart.rules</span>(class_tree, <span class="dt">cover =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>##  snow_factor                                                            cover
##         0.09 when drybulbtemp_max &gt;=       42                             53%
##         0.42 when drybulbtemp_max is 36 to 42                             16%
##         0.45 when drybulbtemp_max is 29 to 36 &amp; drybulbtemp_min &lt;  23      9%
##         0.63 when drybulbtemp_max &lt;  29       &amp; drybulbtemp_min &lt;  23     14%
##         0.71 when drybulbtemp_max &lt;  36       &amp; drybulbtemp_min &gt;= 23      8%</code></pre>
<div id="visualizing-results" class="section level4" number="5.1.1.1">
<h4><span class="header-section-number">5.1.1.1</span> Visualizing Results</h4>
<p>To get another view of what the classification model is doing in this scenario, we will visualize the study results. First, the <code>gf_point()</code> function is used to create a scatterplot where the maximum temperature is shown on the x-axis and the minimum temperature is shown on the y-axis, shown in Figure <a href="classification.html#fig:scatter-usweather">5.2</a>. There is a positive relationship between maximum and minimum temperatures and on days with lower maximum temperatures are where it tends to snow. However, there is not perfect separation, meaning that there are days that have similar minimum and maximum temperatures where it does snow and other where it does not snow.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="classification.html#cb99-1" aria-hidden="true"></a>temperature_scatter &lt;-<span class="st"> </span><span class="kw">gf_point</span>(drybulbtemp_min <span class="op">~</span><span class="st"> </span>drybulbtemp_max, </span>
<span id="cb99-2"><a href="classification.html#cb99-2" aria-hidden="true"></a>                                <span class="dt">color =</span> <span class="op">~</span><span class="st"> </span>snow_factor,</span>
<span id="cb99-3"><a href="classification.html#cb99-3" aria-hidden="true"></a>                                <span class="dt">alpha =</span> <span class="fl">.75</span>,</span>
<span id="cb99-4"><a href="classification.html#cb99-4" aria-hidden="true"></a>                                <span class="dt">data =</span> us_weather) <span class="op">%&gt;%</span></span>
<span id="cb99-5"><a href="classification.html#cb99-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">gf_labs</span>(<span class="dt">x =</span> <span class="st">&quot;Maximum Temperature (in F)&quot;</span>,</span>
<span id="cb99-6"><a href="classification.html#cb99-6" aria-hidden="true"></a>          <span class="dt">y =</span> <span class="st">&quot;Minimum Temperature (in F)&quot;</span>,</span>
<span id="cb99-7"><a href="classification.html#cb99-7" aria-hidden="true"></a>          <span class="dt">color =</span> <span class="st">&quot;Snow?&quot;</span>)</span>
<span id="cb99-8"><a href="classification.html#cb99-8" aria-hidden="true"></a></span>
<span id="cb99-9"><a href="classification.html#cb99-9" aria-hidden="true"></a>temperature_scatter</span></code></pre></div>
<div class="figure"><span id="fig:scatter-usweather"></span>
<img src="05-classification-new_files/figure-html/scatter-usweather-1.png" alt="Scatterplot of the minimum and maximum daily temperatures and if it snows or not" width="672" />
<p class="caption">
Figure 5.2: Scatterplot of the minimum and maximum daily temperatures and if it snows or not
</p>
</div>
<p>The next figure will make use of the <em>parttree</em> R package to visualize what the classification model is doing. The <code>geom_parttree()</code> function is used where the primary argument is the saved classification model object that was save earlier, named <code>class_tree</code>. The other two arguments to add are the fill aesthetic that is the outcome of the classification tree and to control how transparent the backgroud fill color is. In this example, this is set using <code>alpha = .25</code> where the transparency is set at 75% (i.e., 1 - 0.25 = 0.75). Setting a higher alpha value would reduce the amount of transparency, whereas setting a smaller value would increase the transparency.</p>
<p>Figure <a href="classification.html#fig:predict-usweather">5.3</a> gives a sense as to what the classification model is doing to the data. The classification model breaks the data into quadrants and makes a single uniform prediction for those quadrants. For example, the areas of the figure that are shaded as red are days in which the model predicts <strong>it will not snow</strong> whereas the blue/green color are days in which the model predicts <strong>it will snow</strong>. The data points are the real data cases, therefore there are instances inside each of the quadrants in which the model did not correctly predict or classify the case. Each of the quadrants in the figure represent different leaf nodes shown in <a href="classification.html#fig:first-class-tree">5.1</a> and each represent a different likelihood of it snowing.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="classification.html#cb100-1" aria-hidden="true"></a>temperature_scatter <span class="op">+</span><span class="st"> </span></span>
<span id="cb100-2"><a href="classification.html#cb100-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_parttree</span>(<span class="dt">data =</span> class_tree, <span class="kw">aes</span>(<span class="dt">fill =</span> snow_factor), <span class="dt">alpha =</span> <span class="fl">.25</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb100-3"><a href="classification.html#cb100-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">scale_fill_discrete</span>(<span class="st">&quot;Snow?&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:predict-usweather"></span>
<img src="05-classification-new_files/figure-html/predict-usweather-1.png" alt="Showing the predictions based on the classification tree with the raw data" width="672" />
<p class="caption">
Figure 5.3: Showing the predictions based on the classification tree with the raw data
</p>
</div>
</div>
</div>
<div id="accuracy" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Accuracy</h3>
<p>Evaluating the model accuracy helps to understand how well the model performed the classification. If you recall, the classification model is making a prediction about whether it is going to snow on a given day based on the observed data where it was recorded if it snowed that day or not. Therefore, the data has for each day if it snowed or not. With this information, how could we evaluate how well the model performed in classifying whether it snows on a given day?</p>
<p>To do this, the observation of whether it snowed or not can be compared to the model prediction of whether it snowed or not. Better classification accuracy would occur when the observed snow or no snow attribute is the same as the model prediction of snow or not. That is, when the same category is predicted as what is observed, this would result in better classification accuracy, a good thing. If there are cases where different categories between the observed and predicted categories or classes, this would be an example of poor classification accuracy.</p>
<p>In the data so far, there is the observed data value on whether it snowed or not, this is the attribute that was used to fit the classification model, named <code>snow_factor</code>. To add the predicted classes based on the classification model shown in Figure <a href="classification.html#fig:first-class-tree">5.1</a>, the <code>predict()</code> function can be used. To use the <code>predict()</code> function, the primary argument is a model object, in this case the classification model object named <code>class_tree</code>. To get the predicted classes, that is the leaf nodes at the bottom of Figure <a href="classification.html#fig:first-class-tree">5.1</a>, a second argument is needed, <code>type = 'class'</code> which tells the predict function to report the top line of the leaf nodes in Figure <a href="classification.html#fig:first-class-tree">5.1</a>. These predicted classes are saved into a new attribute named <code>snow_predict</code>. Another element is also added that represent the probability of a particular day not snowing or snowing, these are reported in the columns <code>No</code> and <code>Yes</code> in the resulting output.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="classification.html#cb101-1" aria-hidden="true"></a>us_weather_predict &lt;-<span class="st"> </span>us_weather <span class="op">%&gt;%</span></span>
<span id="cb101-2"><a href="classification.html#cb101-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">snow_predict =</span> <span class="kw">predict</span>(class_tree, <span class="dt">type =</span> <span class="st">&#39;class&#39;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb101-3"><a href="classification.html#cb101-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">cbind</span>(<span class="kw">predict</span>(class_tree, <span class="dt">type =</span> <span class="st">&#39;prob&#39;</span>))</span>
<span id="cb101-4"><a href="classification.html#cb101-4" aria-hidden="true"></a><span class="kw">head</span>(us_weather_predict, <span class="dt">n =</span> <span class="dv">20</span>)</span></code></pre></div>
<pre><code>##        station                date dewpoint_avg drybulbtemp_avg
## 1  72528014733 2018-10-01 23:59:00           51              52
## 2  72528014733 2018-10-02 23:59:00           59              60
## 3  72528014733 2018-10-03 23:59:00           55              62
## 4  72528014733 2018-10-04 23:59:00           56              60
## 5  72528014733 2018-10-05 23:59:00           43              51
## 6  72528014733 2018-10-06 23:59:00           62              63
## 7  72528014733 2018-10-07 23:59:00           58              60
## 8  72528014733 2018-10-08 23:59:00           61              68
## 9  72528014733 2018-10-09 23:59:00           66              77
## 10 72528014733 2018-10-10 23:59:00           64              74
## 11 72528014733 2018-10-11 23:59:00           56              62
## 12 72528014733 2018-10-12 23:59:00           36              47
## 13 72528014733 2018-10-13 23:59:00           36              46
## 14 72528014733 2018-10-14 23:59:00           39              51
## 15 72528014733 2018-10-15 23:59:00           43              49
## 16 72528014733 2018-10-16 23:59:00           32              45
## 17 72528014733 2018-10-17 23:59:00           34              45
## 18 72528014733 2018-10-18 23:59:00           30              40
## 19 72528014733 2018-10-19 23:59:00           38              50
## 20 72528014733 2018-10-20 23:59:00           42              48
##    relativehumidity_avg sealevelpressure_avg stationpressure_avg
## 1                    95                30.26               29.50
## 2                    96                30.01               29.26
## 3                    86                30.05               29.31
## 4                    77                29.97               29.18
## 5                    75                30.17               29.41
## 6                    90                30.03               29.28
## 7                    97                30.24               29.44
## 8                    84                30.23               29.49
## 9                    72                30.13               29.39
## 10                   70                29.89               29.18
## 11                   77                29.66               28.91
## 12                   66                29.82               29.05
## 13                   74                29.95               29.15
## 14                   69                30.12               29.34
## 15                   79                29.94               29.16
## 16                   61                30.06               29.31
## 17                   66                30.02               29.21
## 18                   68                30.37               29.59
## 19                   63                30.00               29.28
## 20                   86                29.68               28.90
##    wetbulbtemp_avg windspeed_avg cooling_degree_days
## 1               51          10.9                   0
## 2               60           8.5                   0
## 3               57           5.5                   0
## 4               59          12.5                   0
## 5               47           9.6                   0
## 6               63           8.1                   0
## 7               58           9.4                   0
## 8               63           7.9                   3
## 9               69          11.4                  12
## 10              68          10.6                   9
## 11              59          15.7                   0
## 12              42          12.5                   0
## 13              41           8.4                   0
## 14              45           6.5                   0
## 15              47          12.8                   0
## 16              39          15.8                   0
## 17              40          15.3                   0
## 18              36          11.2                   0
## 19              45          18.0                   0
## 20              45          12.3                   0
##    departure_from_normal_temperature heating_degree_days drybulbtemp_max
## 1                               -4.6                  13              54
## 2                                3.8                   5              69
## 3                                6.2                   3              70
## 4                                4.6                   5              74
## 5                               -4.0                  14              58
## 6                                8.4                   2              74
## 7                                5.7                   5              67
## 8                               14.1                   0              82
## 9                               23.5                   0              83
## 10                              20.9                   0              81
## 11                               9.2                   3              74
## 12                              -5.4                  18              51
## 13                              -6.1                  19              51
## 14                              -0.7                  14              60
## 15                              -2.4                  16              58
## 16                              -6.0                  20              52
## 17                              -5.7                  20              53
## 18                             -10.4                  25              47
## 19                               0.0                  15              57
## 20                              -1.7                  17              55
##    drybulbtemp_min peak_wind_direction peak_wind_speed precipitation snow_depth
## 1               50                  50              24         0.090          0
## 2               51                 320              24         1.000          0
## 3               53                 210              25         0.005          0
## 4               46                 220              39         0.450          0
## 5               44                 100              21         0.000          0
## 6               51                 250              26         0.730          0
## 7               53                  50              21         0.020          0
## 8               53                  70              20         0.010          0
## 9               70                 210              30         0.000          0
## 10              67                 190              25         0.005          0
## 11              50                 220              39         0.010          0
## 12              42                 280              27         0.010          0
## 13              40                 250              24         0.140          0
## 14              41                 250              17         0.000          0
## 15              40                 220              37         0.090          0
## 16              38                 210              40         0.005          0
## 17              36                 290              36         0.050          0
## 18              33                 250              28         0.030          0
## 19              43                 210              48         0.005          0
## 20              40                 220              49         0.470          0
##    snowfall wind_direction wind_speed weather_occurances sunrise sunset month
## 1     0.000             60         20           RA DZ BR     612   1757   Oct
## 2     0.000            320         21           RA DZ BR     613   1755   Oct
## 3     0.000            200         20              DZ BR     614   1753   Oct
## 4     0.000            220         32           TS RA BR     615   1751   Oct
## 5     0.000             70         16               &lt;NA&gt;     616   1750   Oct
## 6     0.000            200         20           TS RA BR     618   1748   Oct
## 7     0.000             60         16           RA DZ BR     619   1746   Oct
## 8     0.000             70         16                 RA     620   1744   Oct
## 9     0.000            210         23               &lt;NA&gt;     621   1743   Oct
## 10    0.000            190         21               &lt;NA&gt;     622   1741   Oct
## 11    0.000            240         29                 RA     623   1739   Oct
## 12    0.000            260         21                 RA     625   1738   Oct
## 13    0.000            240         18              RA BR     626   1736   Oct
## 14    0.000            250         14               &lt;NA&gt;     627   1734   Oct
## 15    0.000            290         28              RA BR     628   1733   Oct
## 16    0.000            220         30                 RA     629   1731   Oct
## 17    0.005            290         28           GR RA SN     631   1729   Oct
## 18    0.005            240         21           RA SN HZ     632   1728   Oct
## 19    0.000            240         35                 RA     633   1726   Oct
## 20    0.100            240         36     TS GR RA BR PL     634   1725   Oct
##    month_numeric year day winter_group    location fog mist drizzle rain snow
## 1             10 2018   1        18_19 Buffalo, NY  No  Yes     Yes  Yes   No
## 2             10 2018   2        18_19 Buffalo, NY  No  Yes     Yes  Yes   No
## 3             10 2018   3        18_19 Buffalo, NY  No  Yes     Yes   No   No
## 4             10 2018   4        18_19 Buffalo, NY  No  Yes      No  Yes   No
## 5             10 2018   5        18_19 Buffalo, NY  No   No      No   No   No
## 6             10 2018   6        18_19 Buffalo, NY  No  Yes      No  Yes   No
## 7             10 2018   7        18_19 Buffalo, NY  No  Yes     Yes  Yes   No
## 8             10 2018   8        18_19 Buffalo, NY  No   No      No  Yes   No
## 9             10 2018   9        18_19 Buffalo, NY  No   No      No   No   No
## 10            10 2018  10        18_19 Buffalo, NY  No   No      No   No   No
## 11            10 2018  11        18_19 Buffalo, NY  No   No      No  Yes   No
## 12            10 2018  12        18_19 Buffalo, NY  No   No      No  Yes   No
## 13            10 2018  13        18_19 Buffalo, NY  No  Yes      No  Yes   No
## 14            10 2018  14        18_19 Buffalo, NY  No   No      No   No   No
## 15            10 2018  15        18_19 Buffalo, NY  No  Yes      No  Yes   No
## 16            10 2018  16        18_19 Buffalo, NY  No   No      No  Yes   No
## 17            10 2018  17        18_19 Buffalo, NY  No   No      No  Yes  Yes
## 18            10 2018  18        18_19 Buffalo, NY  No   No      No  Yes  Yes
## 19            10 2018  19        18_19 Buffalo, NY  No   No      No  Yes   No
## 20            10 2018  20        18_19 Buffalo, NY  No  Yes      No  Yes   No
##    snow_factor snow_predict        No        Yes
## 1           No           No 0.9130676 0.08693245
## 2           No           No 0.9130676 0.08693245
## 3           No           No 0.9130676 0.08693245
## 4           No           No 0.9130676 0.08693245
## 5           No           No 0.9130676 0.08693245
## 6           No           No 0.9130676 0.08693245
## 7           No           No 0.9130676 0.08693245
## 8           No           No 0.9130676 0.08693245
## 9           No           No 0.9130676 0.08693245
## 10          No           No 0.9130676 0.08693245
## 11          No           No 0.9130676 0.08693245
## 12          No           No 0.9130676 0.08693245
## 13          No           No 0.9130676 0.08693245
## 14          No           No 0.9130676 0.08693245
## 15          No           No 0.9130676 0.08693245
## 16          No           No 0.9130676 0.08693245
## 17         Yes           No 0.9130676 0.08693245
## 18         Yes           No 0.9130676 0.08693245
## 19          No           No 0.9130676 0.08693245
## 20          No           No 0.9130676 0.08693245</code></pre>
<p>The first 20 rows of the resulting data are shown. Notice that for all of these 20 rows, the predicted class, shown in the attribute, <code>snow_predict</code>, are represented as ‚ÄúNo‚Äù indicating that these days it was not predicted to snow. Notice toward the bottom however, that there were two days in which it did in fact snow, shown in the column named, <code>snow_factor</code>. These would represent two cases of misclassification as the observed data is not the same as the model predicted class. Finally, the probabilities shown in the last two attribute columns are all the same here. These are all the same as the maximum dry bulb temperature was greater than 42 degrees Fahrenheit in all of these days. Therefore, all 20 of the cases shown in the data here represent the left-most leaf node shown in Figure <a href="classification.html#fig:first-class-tree">5.1</a>.</p>
<p>Now that the observed data and the model predicted classes are in the data, it is possible to produce a table that shows how many observations were correctly predicted (indicating better model accuracy). To do this, the <code>count()</code> function can be used where the observed and predicted class attributes are passed as arguments.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="classification.html#cb103-1" aria-hidden="true"></a>us_weather_predict <span class="op">%&gt;%</span></span>
<span id="cb103-2"><a href="classification.html#cb103-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">count</span>(snow_factor, snow_predict)</span></code></pre></div>
<pre><code>##   snow_factor snow_predict    n
## 1          No           No 2147
## 2          No          Yes  245
## 3         Yes           No  529
## 4         Yes          Yes  479</code></pre>
<p>The resulting table shows the observed data values in the left-most column (<code>snow_factor</code>) followed by the predicted class (<code>snow_predict</code>) in the middle column. The final column represents the number of rows or observations that were in each combination of the first two columns. For example, the first row shows that 2,147 observations were counted that had the combination where it was observed and predicted to have not snowed that day. These 2,147 observations would be instances of correct classification. The second row shows that 245 observations occurred where it was observed to not have snowed, but the model predicted it would snow that day. All of the 245 observations were misclassified based on the classification model.</p>
<p>From this table, the overall model accuracy can be calculated by summing up the cases that matched and dividing by the total number of observations. This computation would look like:
<span class="math display">\[
accuracy = \frac{\textrm{matching predictions}}{\textrm{total observations}} = \frac{(2147 + 479)}{(2147 + 245 + 529 + 479)} = .772 = 77.2%
\]</span>
This means that the overall classification accuracy for this example was just over 77%, meaning that about 77% of days the model was able to correctly classify whether it snowed or not. This computation can also be done programmatically. To do this, a new attribute named, <code>same_class</code>, can be added to the data that is given a value of 1 if the observed data matches the predicted class and a value of 0 otherwise. Descriptive statistics, such as the mean and sum, can be computed on this new vector to represent the accuracy as a proportion and the number of matching predictions (the numerator shown in the equation above).</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="classification.html#cb105-1" aria-hidden="true"></a>us_weather_predict <span class="op">%&gt;%</span></span>
<span id="cb105-2"><a href="classification.html#cb105-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">same_class =</span> <span class="kw">ifelse</span>(snow_factor <span class="op">==</span><span class="st"> </span>snow_predict, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></span>
<span id="cb105-3"><a href="classification.html#cb105-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">df_stats</span>(<span class="op">~</span><span class="st"> </span>same_class, mean, sum)</span></code></pre></div>
<pre><code>##     response      mean  sum
## 1 same_class 0.7723529 2626</code></pre>
<p>Notice that the same model accuracy was found, about 77.2%, and the number of observations (i.e., days) that the correct classification was found was 2,626 days. Is correctly predicting 77.2% of the days good? That is, would you say this model is doing a good job at accurately predicting if it will snow or not on that day?</p>
<div id="conditional-accuracy" class="section level4" number="5.1.2.1">
<h4><span class="header-section-number">5.1.2.1</span> Conditional Accuracy</h4>
<p>One potential misleading element of simply computing the overall model accuracy as done above, is that the accuracy will likely differ based on the which class. This could occur for a few reasons, one it could be more difficult to predict one of the classes due to similarities in data across the two classes. The two classes are also often unbalanced, therefore exploring the overall model accuracy will give more weight to the group/class that has more data. In addition, this group has more data so it could make it a bit easier for the model to predict, these issues could be exacerbated in small sample conditions.</p>
<p>Therefore, similar to earlier discussion in the book about multivariate distributions, it is often important to consider conditional or multivariate accuracy instead of the overall model accuracy. Let‚Äôs explore this a different way than simply computing a percentage, instead we could use a bar graph to explore the model accuracy. Figure <a href="classification.html#fig:bar-accuracy-count">5.4</a> shows the number of correct classifications for the two observed data classes (i.e., snow or did not snow) on the x-axis by the predicted classes shown with the fill color in the bars. The fill color are red for days that the model predicts it will not show and green/blue for days in which it will not snow. Therefore, accuracy would be represented in the left-bar by the red portion of the bar and the right-bar by the green/blue portion of the bar.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="classification.html#cb107-1" aria-hidden="true"></a><span class="kw">gf_bar</span>(<span class="op">~</span><span class="st"> </span>snow_factor, <span class="dt">fill =</span> <span class="op">~</span>snow_predict, <span class="dt">data =</span> us_weather_predict) <span class="op">%&gt;%</span></span>
<span id="cb107-2"><a href="classification.html#cb107-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">gf_labs</span>(<span class="dt">x =</span> <span class="st">&quot;Observed Snow Status&quot;</span>,</span>
<span id="cb107-3"><a href="classification.html#cb107-3" aria-hidden="true"></a>          <span class="dt">fill =</span> <span class="st">&quot;Predicted Snow Status&quot;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:bar-accuracy-count"></span>
<img src="05-classification-new_files/figure-html/bar-accuracy-count-1.png" alt="A bar graph showing the conditional prediction accuracy represented as counts." width="672" />
<p class="caption">
Figure 5.4: A bar graph showing the conditional prediction accuracy represented as counts.
</p>
</div>
<p>Figure <a href="classification.html#fig:bar-accuracy-count">5.4</a> is not a very good picture to depict accuracy as the two groups have different numbers of observations so comparisons between the bars is difficult. Secondly, the count metric makes it difficult to estimate how many are in each group, for example, it is difficult from the figure alone to know how many were incorrectly classified in the left-most bar represented by the blue/green color. These issues can be fixed by adding an additional argument, <code>position = 'fill'</code> which will scale each bar as a proportion, ranging from 0 to 1. The bar graph is now scaling each bar based on the sample size to normalize sample size differences.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="classification.html#cb108-1" aria-hidden="true"></a><span class="kw">gf_bar</span>(<span class="op">~</span><span class="st"> </span>snow_factor, <span class="dt">fill =</span> <span class="op">~</span>snow_predict, <span class="dt">data =</span> us_weather_predict, <span class="dt">position =</span> <span class="st">&quot;fill&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb108-2"><a href="classification.html#cb108-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">gf_labs</span>(<span class="dt">x =</span> <span class="st">&quot;Observed Snow Status&quot;</span>,</span>
<span id="cb108-3"><a href="classification.html#cb108-3" aria-hidden="true"></a>          <span class="dt">fill =</span> <span class="st">&quot;Predicted Snow Status&quot;</span>,</span>
<span id="cb108-4"><a href="classification.html#cb108-4" aria-hidden="true"></a>          <span class="dt">y =</span> <span class="st">&#39;Proportion&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb108-5"><a href="classification.html#cb108-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">gf_refine</span>(<span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">.1</span>)))</span></code></pre></div>
<div class="figure"><span id="fig:bar-accuracy-fill"></span>
<img src="05-classification-new_files/figure-html/bar-accuracy-fill-1.png" alt="A bar graph showing the conditional prediction accuracy represented as a proportion." width="672" />
<p class="caption">
Figure 5.5: A bar graph showing the conditional prediction accuracy represented as a proportion.
</p>
</div>
<p>From this new figure (Figure <a href="classification.html#fig:bar-accuracy-fill">5.5</a>), it is much easier to estimate the prediction accuracy from the figure. For example, the green/blue portion of the left-most bar is at about 0.10, meaning that about 10% of the cases are misclassified and 90% would be correctly classified. Therefore the classification accuracy for days in which it did not snow would be about 90%. Compare this to days in which it did not snow (the right bar), where the prediction accuracy represented by the green/blue color is about 48%, meaning that the misclassification rate is about 52%.</p>
<p>Let‚Äôs recalibrate how we think the model is doing? If you were just given the overall classification rate of about 77%, how did you feel about the model? Now that we know the model accurate predicts it won‚Äôt snow about 90% of the time, but can only identify that it will snow about 48% of the time, how well do you feel the model is performing now? Would you feel comfortable using this model in the real world?</p>
<p>One last note, we can also compute the conditional model accuracy more directly using the <code>df_stats()</code> function as was done for the overall model accuracy. The primary difference in the code is to specify the <code>same_class</code> attribute to the left of the <code>~</code>. This represent the attribute to compute the statistics of interest with. Another attribute is added to the right of the <code>~</code> to represent the attribute to condition on, in this case the observed data point of whether it snowed or not.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="classification.html#cb109-1" aria-hidden="true"></a>us_weather_predict <span class="op">%&gt;%</span></span>
<span id="cb109-2"><a href="classification.html#cb109-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">same_class =</span> <span class="kw">ifelse</span>(snow_factor <span class="op">==</span><span class="st"> </span>snow_predict, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></span>
<span id="cb109-3"><a href="classification.html#cb109-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">df_stats</span>(same_class <span class="op">~</span><span class="st"> </span>snow_factor, mean, sum, length)</span></code></pre></div>
<pre><code>##     response snow_factor      mean  sum length
## 1 same_class          No 0.8975753 2147   2392
## 2 same_class         Yes 0.4751984  479   1008</code></pre>
<p>The output returns the conditional model accuracy as a proportion, the number of correct classifications for each class/group, and the total number of observations (both correct and incorrect classifications) for each class/group. The estimated values we had from the figure were very close to the actual calculated values, but we find the figure to be more engaging than just the statistics.</p>
<!--
### Comparison to Baseline


```r
titanic_predict <- titanic_predict %>%
  mutate(tree_predict_full = predict(class_tree, type = 'class'))

titanic_predict %>%
  count(survived, tree_predict_full)
```


```r
gf_bar(~ survived, fill = ~tree_predict_full, data = titanic_predict, position = "fill") %>%
  gf_labs(y = "proportion") %>%
  gf_refine(scale_y_continuous(breaks = seq(0, 1, .1)))
```


```r
titanic_predict %>%
  mutate(same_class = ifelse(survived == tree_predict_full, 1, 0)) %>%
  df_stats(~ same_class, mean, sum)
```

#### Absolute vs Relative Comparison


### Training/Test Data

So far we have used the entire data to make our classification. This is not best practice and we will explore this is a bit more detail. First, take a minute to hypothesize why using the entire data to make our classification prediction may not be the best?

It is common to split the data prior to fitting a classification/prediction model into a training data set in which the model makes a series of predictions on the data, learns which data attributes are the most important, etc. Then, upon successfully identifying a useful model with the training data, test these model predictions on data that the model has not seen before. This is particularly important as the algorithms to make the predictions are very good at understanding and exploiting small differences in the data used to fit the model. Therefore, exploring the extent to which the model does a good job on data the model has not seen is a better test to the utility of the model. We will explore in more detail the impact of not using the training/test data split later, but first, let's refit the classification tree to the titanic data by splitting the data into 70% training and 30% test data. Why 70% training and 30% test? This is a number that is sometimes used as the splitting, an 80/20 split is also common. The main idea behind the making the test data smaller is so that the model has more data to train on initially to understand the attributes from the data. Secondly, the test data does not need to be quite as large, but we would like it to be representative. Here, the data are not too large, about 1000 passengers with available survival data, therefore, withholding more data helps to ensure the test data is representative of the 1000 total passengers.
Splitting the data into training/test

This is done with the rsample package utilizing three functions, initial_split(), training(), and test(). The initial_split() function helps to take the initial random sample and the proportion of data to use for the training data is initially identified. The random sample is done without replacement meaning that the data are randomly selected, but can not show up in the data more than once. Then, after using the initial_split() function, the training() and test() functions are used on the resulting output from initial_split() to obtain the training and test data respectively. It is good practice to use the set.seed() function to save the seed that was used as this is a random process. Without using the set.seed() function, the same split of data would likely not be able to be recreated in the code was ran again.

Let's do the data splitting.
 

```r
titanic <- bind_rows(titanic_train, titanic_test) %>% 
  mutate(survived = ifelse(Survived == 1, 'Survived', 'Died')) %>% 
  drop_na(survived)

set.seed(2019)
titanic_split <- initial_split(titanic, prop = .7)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```
 

```r
class_tree <- rpart(survived ~ Pclass + Sex + Age + Fare + Embarked + SibSp + Parch, 
   method = 'class', data = titanic_train)

rpart.plot(class_tree, roundint = FALSE, type = 3, branch = .3)
```


```r
prune_class_tree <- prune(class_tree, cp = .02)

rpart.plot(prune_class_tree, roundint = FALSE, type = 3, branch = .3)
```

This seems like a reasonable model. Let's check the model accuracy.


```r
titanic_predict <- titanic_train %>%
  mutate(tree_predict = predict(prune_class_tree, type = 'class'))
titanic_predict %>%
  mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %>%
  df_stats(~ same_class, mean, sum)
```

 This is actually slightly better accuracy compared to the model last time, about xxx compared to about xxx prediction accuracy. But, let's test the model out on the test data to see the prediction accuracy for the test data, the real test.



```r
titanic_predict_test <- titanic_test %>%
  mutate(tree_predict = predict(prune_class_tree, newdata = titanic_test, type = 'class'))
titanic_predict_test %>%
  mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %>%
  df_stats(~ same_class, mean, sum)
```

For the test data, prediction accuracy was quite a bit lower, about xxx.

### Introduction to resampling/bootstrap

To explore these ideas in more detail, it will be helpful to use a statistical technique called resampling or the bootstrap. We will use these ideas a lot going forward in this course. In very simple terminology, resampling or the bootstrap can help us understand uncertainty in our estimates and also allow us to be more flexible in the statistics that we run. The main drawback of resampling and bootstrap methods is that they can be computationally heavy, therefore depending on the situation, more time is needed to come to the conclusion desired.

Resampling and bootstrap methods use the sample data we have and perform the sampling procedure again treating the sample we have data for as the population. Generating the new samples is done with replacement (more on this later). This resampling is done many times (100, 500, 1000, etc.) with more in general being better. As an example with the titanic data, let's take the titanic data, assume this is the population of interest, and resample from this population 1000 times (with replacement) and each time we will calculate the proportion that survived the disaster in each sample. Before we write the code for this, a few questions to consider.

1. Would you expect the proportion that survived to be the same in each new sample? Why or why not?
2. Sampling with replacement keeps coming up, what do you think this means?
3. Hypothesize why sampling with replacement would be a good idea?

Let's now try the resampling with the calculation of the proportion that survived. We will then save these 1000 survival proportions and create a visualization.


```r
resample_titanic <- function(...) {
    titanic %>%
        sample_n(nrow(titanic), replace = TRUE) %>%
        df_stats(~ Survived, mean)
}

survival_prop <- map(1:1000, resample_titanic) %>% 
  bind_rows()

gf_density(~ mean_Survived, data = survival_prop)
```

#### Bootstrap variation in prediction accuracy

We can apply these same methods to evaluate the prediction accuracy based on the classification model above. When using the bootstrap, we can get an estimate for how much variation there is in the classification accuracy based on the sample that we have. In addition, we can explore how different the prediction accuracy would be for many samples when using all the data and by splitting the data into training and test sets.
Bootstrap full data.

Let's first explore the full data to see how much variation there is in the prediction accuracy using all of the data. Here we will again use the sample_n() function to sample with replacement, then fit the classification model to each of these samples, then calculate the prediction accuracy. First, I'm going to write a function to do all of these steps one time.


```r
calc_predict_acc <- function(data) {
  rsamp_titanic <- titanic %>%
    sample_n(nrow(titanic), replace = TRUE)

  class_model <- rpart(survived ~ Pclass + Sex + Age + Fare + SibSp + Parch, 
        method = 'class', data = rsamp_titanic, cp = .02)

  titanic_predict <- rsamp_titanic %>%
    mutate(tree_predict = predict(class_model, type = 'class'))
  titanic_predict %>%
    mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %>%
    df_stats(~ same_class, mean, sum)
}

calc_predict_acc()
```


 To do the bootstrap, this process can be replicated many times. In this case, I'm going to do 500. In practice, we would likely want to do a few more.



```r
predict_accuracy_fulldata <- map(1:2000, calc_predict_acc) %>%
  bind_rows()

gf_density(~ mean_same_class, data = predict_accuracy_fulldata)
```


```r
calc_predict_acc_split <- function(data) {
  titanic_split <- initial_split(titanic, prop = .7)
  titanic_train <- training(titanic_split)
  titanic_test <- testing(titanic_split)

  class_model <- rpart(survived ~ Pclass + Sex + Age + Fare + SibSp + Parch, 
        method = 'class', data = titanic_train, cp = .02)

  titanic_predict <- titanic_test %>%
    mutate(tree_predict = predict(class_model, newdata = titanic_test, type = 'class'))
  titanic_predict %>%
    mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %>%
    df_stats(~ same_class, mean, sum)
}

calc_predict_acc_split()
```


```r
predict_accuracy_traintest <- map(1:2000, calc_predict_acc_split) %>%
  bind_rows()

gf_density(~ mean_same_class, data = predict_accuracy_traintest)
```


```r
bind_rows(
  mutate(predict_accuracy_fulldata, type = "Full Data"),
  mutate(predict_accuracy_traintest, type = "Train/Test")
) %>%
  gf_density(~ mean_same_class, color = ~ type, fill = NA, size = 1.25)
```


### Cross-validation

-->

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multivariate-visualization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
